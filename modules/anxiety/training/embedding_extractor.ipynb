{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using primary device: cuda:0\n",
      "Attempting to use DATASET_DIRECTORY: /media/nas_mount/avinash_ocr/Dhruvkumar_Patel_MT24032/others/mental-health-meme-classification/dataset\n",
      "Expecting embedding/retrieval files in: /media/nas_mount/avinash_ocr/Dhruvkumar_Patel_MT24032/others/mental-health-meme-classification/anxiety_embeddings_output\n",
      "Loading data...\n",
      "Original train samples: 2608\n",
      "Original test samples: 652\n",
      "Removing classes: ['Irritatbily', 'Unknown']\n",
      "Filtered train samples: 2457\n",
      "Filtered test samples: 614\n",
      "Merging dataframes...\n",
      "Processing reasoning column: figurative_reasoning\n",
      "\n",
      "--- Checking Processed Reasoning (Example) ---\n",
      "Original Reasoning (TR-2483):\n",
      "Certainly! Here are the triples extracted from the anxiety meme image: 1. **Cause-effect:** - **Cause:** Lack of employment or financial stability. - **Effect:** Increased feelings of depression and hopelessness. 2. **Figurative Understanding:** - **Metaphor:** The image uses a stark contrast between the two faces to symbolize the gap between societal expectations and personal struggles. The first face represents societal advice (\"Just get a job and stop being lazy\"), while the second face represents the internal struggle (\"I'm literally so depressed that I just want to kill myself\"). - **Ironic undertone:** The advice given is often well-intentioned but can be perceived as dismissive or unrealistic by those who are already struggling with depression and lack of employment. 3. **Mental State:** - **Mental State:** Depression, hopelessness, and suicidal thoughts. These triples capture the essence of the meme, highlighting the disconnect between societal expectations and the personal struggles of individuals dealing with depression and lack of employment.\n",
      "\n",
      "Processed Reasoning (TR-2483):\n",
      "cause-effect: **cause:** lack of employment or financial stability. - **effect:** increased feelings of depression and hopelessness. 2.\n",
      "figurative understanding: **metaphor:** the image uses a stark contrast between the two faces to symbolize the gap between societal expectations and personal struggles. the first face represents societal advice (\"just get a job and stop being lazy\"), while the second face represents the internal struggle (\"i'm literally so depressed that i just want to kill myself\"). - **ironic undertone:** the advice given is often well-intentioned but can be perceived as dismissive or unrealistic by those who are already struggling with depression and lack of employment. 3. **mental state:** -\n",
      "mental state: depression, hopelessness, and suicidal thoughts. these triples capture the essence of the meme, highlighting the disconnect between societal expectations and the personal struggles of individuals dealing with depression and lack of employment.\n",
      "-------------------------------------------\n",
      "\n",
      "Processed Reasoning (Test Sample 0):\n",
      "cause-effect: **cause**: stressing about things one has no control over. - **effect**: increased anxiety and emotional distress. 2.\n",
      "figurative understanding: **metaphor**: the meme uses the visual metaphor of a person covering their face with their hands to symbolize the overwhelming feeling of helplessness and anxiety when faced with uncontrollable situations. - **symbolism**: the increasing size of the hands covering the face in the second panel symbolizes the intensification of the anxiety and emotional turmoil. 3.\n",
      "mental state: **emotional state**: anxiety, helplessness, and emotional distress.\n",
      "-------------------------------------------\n",
      "\n",
      "Encoding labels...\n",
      "Found 6 final classes after filtering: ['Difficulty Relaxing', 'Excessive Worry', 'Impending Doom', 'Lack of Worry Control', 'Nervousness', 'Restlessness']\n",
      "Generating prompts with 1 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Prompts: 100%|██████████| 2457/2457 [00:00<00:00, 4321.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts with 1 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Prompts: 100%|██████████| 614/614 [00:00<00:00, 4203.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- # Add this line for better encoding support\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm # Use tqdm directly\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import matplotlib.pyplot as plt\n",
    "# Import AutoConfig to potentially modify dropout, although it might conflict with pretrained settings\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import re\n",
    "import ast # To safely evaluate string representation of lists\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Set device (use GPU if available, otherwise CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# If using DataParallel, specify target devices\n",
    "DEVICE_IDS = [0] # Adjust GPU IDs if needed and using DataParallel\n",
    "PRIMARY_DEVICE = f'cuda:{DEVICE_IDS[0]}' if DEVICE == torch.device(\"cuda\") and DEVICE_IDS else DEVICE\n",
    "print(f\"Using primary device: {PRIMARY_DEVICE}\")\n",
    "\n",
    "# --- Hyperparameters from Paper ---\n",
    "LEARNING_RATE = 5e-5\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "WEIGHT_DECAY = 1e-2 # Paper specified 1*10^-2\n",
    "DROPOUT_RATE = 0.2 # From paper - Applying this via config might be complex/risky, using default for now\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512 # Capped at 512 as per paper\n",
    "GRADIENT_CLIP_THRESHOLD = 1.0\n",
    "\n",
    "# --- Other Configurations ---\n",
    "TOP_K = 3\n",
    "N_SHOT = 1 # Number of few-shot examples to include in the prompt (can be <= TOP_K)\n",
    "EPOCHS = 10 # You might need more epochs depending on convergence\n",
    "FREEZE_LAYERS_BELOW = 10 # Keep freezing strategy unless LoRA is explicitly implemented\n",
    "\n",
    "# Model Name\n",
    "MODEL_NAME = 'Tianlin668/MentalBART'\n",
    "\n",
    "# Classes to Remove\n",
    "CLASSES_TO_REMOVE = ['Irritatbily', 'Unknown'] # Adjusted 'Irritatbily' based on common spelling\n",
    "\n",
    "# DATASET DIRECTORY CONFIGURATION\n",
    "try:\n",
    "    script_dir = os.path.dirname(\"/media/nas_mount/avinash_ocr/Dhruvkumar_Patel_MT24032/others/mental-health-meme-classification/src/anxiety/training/embedding_extractor.ipynb\")\n",
    "    # Go up three levels from script dir to project root\n",
    "    project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\", \"..\")) # Adjust if needed\n",
    "    # Simple check if the detected root seems plausible\n",
    "    if project_root == '/' or not os.path.exists(os.path.join(project_root, 'README.md')): # Heuristic check\n",
    "         print(\"Warning: Project root auto-detection might be incorrect. Adjust path joining if necessary.\")\n",
    "         # Fallback or specific path might be needed depending on structure\n",
    "         project_root = os.path.abspath(\".\") # Assume script is run from near project root\n",
    "\n",
    "    DATASET_DIRECTORY = os.path.join(project_root, \"dataset\")\n",
    "    print(f\"Attempting to use DATASET_DIRECTORY: {DATASET_DIRECTORY}\")\n",
    "    if not os.path.exists(DATASET_DIRECTORY):\n",
    "         print(f\"ERROR: Cannot find dataset directory at {DATASET_DIRECTORY}.\")\n",
    "         # Provide guidance if path is wrong\n",
    "         print(\"Please ensure the 'dataset' directory exists at the root of your project.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "except NameError:\n",
    "    # This block runs if __file__ is not defined (e.g., in an interactive environment)\n",
    "    print(\"Warning: __file__ not defined. Using hardcoded relative paths from CWD.\")\n",
    "    # Assume CWD is project root for this fallback\n",
    "    project_root = os.path.abspath(\".\")\n",
    "    DATASET_DIRECTORY = os.path.join(project_root, \"dataset\")\n",
    "    if not os.path.exists(DATASET_DIRECTORY):\n",
    "        print(f\"ERROR: Cannot find dataset directory at {DATASET_DIRECTORY} relative to CWD.\")\n",
    "        print(\"Please run the script from the project root directory or adjust paths.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "ANXIETY_DATASET_DIRECTORY = os.path.join(DATASET_DIRECTORY, \"Anxiety_Data\")\n",
    "\n",
    "# --- Input File Paths ---\n",
    "# Original data files\n",
    "TARGET_TRAIN_FILE_PATH = os.path.join(ANXIETY_DATASET_DIRECTORY, \"final\", \"cleaned\", \"anxiety_train_combined_preprocessed.json\")\n",
    "TARGET_TEST_FILE_PATH = os.path.join(ANXIETY_DATASET_DIRECTORY, \"final\", \"cleaned\", \"anxiety_test_combined_preprocessed.json\")\n",
    "\n",
    "# Embedding directory and retrieval results\n",
    "EMBEDDING_DIR = os.path.join(project_root, \"anxiety_embeddings_output\") # Adjust if needed\n",
    "print(f\"Expecting embedding/retrieval files in: {EMBEDDING_DIR}\")\n",
    "RETRIEVED_TEST_INDICES_FILE = os.path.join(EMBEDDING_DIR, f\"test_top_{TOP_K}_similar_indices.csv\") # Using CSV\n",
    "RETRIEVED_TRAIN_INDICES_FILE = os.path.join(EMBEDDING_DIR, f\"train_top_{TOP_K}_similar_indices.csv\") # Using CSV\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def split_ocr(text, limit):\n",
    "    \"\"\"Splits text by words up to a specified limit.\"\"\"\n",
    "    if not isinstance(text, str): text = str(text)\n",
    "    words = text.split()\n",
    "    if len(words) > limit: return ' '.join(words[:limit]) + '...'\n",
    "    return text\n",
    "\n",
    "# Using the robust process_triples function from the previous iteration\n",
    "# def process_triples(triples):\n",
    "#     \"\"\"\n",
    "#     Robustly processes the figurative reasoning string to extract relevant sections.\n",
    "#     Specifically designed to handle structured reasoning with nested sections.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(triples, str):\n",
    "#         return \"Missing reasoning\"\n",
    "\n",
    "#     # Normalize the text to handle different formatting styles\n",
    "#     triples = triples.replace('**Cause-effect:**', '**Cause-effect**')\n",
    "#     triples = triples.replace('**Cause-effect :**', '**Cause-effect**')\n",
    "\n",
    "#     # Define regex patterns to extract sections\n",
    "#     section_patterns = [\n",
    "#         {\n",
    "#             'name': 'cause-effect',\n",
    "#             'pattern': r'\\*\\*Cause-effect\\*\\*\\s*:?\\s*(.+?)(?=\\*\\*Figurative|$)',\n",
    "#             'clean_pattern': r'\\s*-\\s*'\n",
    "#         },\n",
    "#         {\n",
    "#             'name': 'figurative understanding',\n",
    "#             'pattern': r'\\*\\*Figurative Understanding\\*\\*\\s*:?\\s*(.+?)(?=\\*\\*Mental|$)',\n",
    "#             'clean_pattern': r'\\s*-\\s*'\n",
    "#         },\n",
    "#         {\n",
    "#             'name': 'mental state',\n",
    "#             'pattern': r'\\*\\*Mental State\\*\\*\\s*:?\\s*(.+?)(?=$|These triples|$)',\n",
    "#             'clean_pattern': r'\\s*-\\s*'\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "#     # Function to clean and extract content\n",
    "#     def extract_section_content(section_text, clean_pattern):\n",
    "#         # Remove markdown formatting\n",
    "#         section_text = re.sub(r'\\*\\*[^*]+\\*\\*', '', section_text)\n",
    "        \n",
    "#         # Split into individual points\n",
    "#         points = [p.strip() for p in re.split(clean_pattern, section_text) if p.strip()]\n",
    "        \n",
    "#         # Clean each point\n",
    "#         cleaned_points = []\n",
    "#         for point in points:\n",
    "#             # Remove parentheses if present\n",
    "#             point = re.sub(r'^\\s*\\(|\\)\\s*$', '', point)\n",
    "#             # Remove extra whitespace\n",
    "#             point = re.sub(r'\\s+', ' ', point).strip()\n",
    "#             if point:\n",
    "#                 cleaned_points.append(point)\n",
    "        \n",
    "#         return ' '.join(cleaned_points) if cleaned_points else ''\n",
    "\n",
    "#     # Extract sections\n",
    "#     extracted_sections = {}\n",
    "#     for section in section_patterns:\n",
    "#         match = re.search(section['pattern'], triples, re.DOTALL | re.IGNORECASE)\n",
    "#         if match:\n",
    "#             content = extract_section_content(match.group(1), section['clean_pattern'])\n",
    "#             if content:\n",
    "#                 extracted_sections[section['name']] = content\n",
    "\n",
    "#     # Construct output\n",
    "#     if extracted_sections:\n",
    "#         output_lines = []\n",
    "#         for section, content in extracted_sections.items():\n",
    "#             output_lines.append(f\"{section.lower()}: {content}\")\n",
    "#         return '\\n'.join(output_lines)\n",
    "    \n",
    "#     # Fallback if no sections found\n",
    "#     return \"Reasoning sections not identified\"\n",
    "\n",
    "def process_triples(triples):\n",
    "    \"\"\"\n",
    "    Robustly processes the figurative reasoning string to extract relevant sections.\n",
    "    Prioritizes finding all three key sections ('cause-effect', 'figurative understanding', 'mental state').\n",
    "    If all three sections are reliably found and extracted, returns the structured, cleaned text.\n",
    "    Otherwise, returns the entire original reasoning string, lowercased, to avoid information loss.\n",
    "\n",
    "    Args:\n",
    "        triples (str): The raw figurative reasoning string.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed reasoning string or the lowercased original string.\n",
    "    \"\"\"\n",
    "    if not isinstance(triples, str) or not triples.strip():\n",
    "        return \"Missing or empty reasoning\" # Handle None, empty strings\n",
    "\n",
    "    text_lower = triples.lower() # Lowercase the entire input first\n",
    "\n",
    "    sections_to_find = {\n",
    "        'cause-effect': None,\n",
    "        'figurative understanding': None,\n",
    "        'mental state': None\n",
    "    }\n",
    "\n",
    "    # Pattern to find section markers like \"1. **title:**\", \"**title:**\", \"### title\" etc.\n",
    "    # Captures the title itself (group 1). Uses re.MULTILINE.\n",
    "    # Make it more robust to variations in spacing and markdown around the title.\n",
    "    pattern_section_start = r'(?:^\\s*\\d+\\.\\s*)?(?:[\\#\\*]*)\\s*(cause-effect|figurative understanding|mental state)\\b'\n",
    "\n",
    "    matches = list(re.finditer(pattern_section_start, text_lower, re.MULTILINE))\n",
    "\n",
    "    # If no potential section markers are found at all, fallback immediately\n",
    "    if not matches:\n",
    "        # print(f\"Debug: No section markers found for input: {text_lower[:100]}...\") # Optional debug\n",
    "        return text_lower # Fallback to original lowercased text\n",
    "\n",
    "    # Store found sections and their start/end points\n",
    "    found_sections_pos = {}\n",
    "    for match in matches:\n",
    "        title = match.group(1).strip()\n",
    "        # Find the end of the marker (e.g., after the colon and stars if present)\n",
    "        marker_end_match = re.search(r'[:\\*]+', text_lower[match.end():match.end()+10])\n",
    "        marker_end_pos = match.end() + marker_end_match.end() if marker_end_match else match.end()\n",
    "        found_sections_pos[title] = {'start': match.start(), 'marker_end': marker_end_pos}\n",
    "\n",
    "    # Determine content boundaries and extract raw content\n",
    "    sorted_titles = sorted(found_sections_pos.keys(), key=lambda k: found_sections_pos[k]['start'])\n",
    "\n",
    "    raw_content = {}\n",
    "    for i, title in enumerate(sorted_titles):\n",
    "        start_content = found_sections_pos[title]['marker_end']\n",
    "        # End content at the start of the *next* found section marker, or end of text\n",
    "        next_section_start = len(text_lower) # Default to end of string\n",
    "        if (i + 1) < len(sorted_titles):\n",
    "            next_title = sorted_titles[i+1]\n",
    "            next_section_start = found_sections_pos[next_title]['start']\n",
    "\n",
    "        content = text_lower[start_content:next_section_start].strip()\n",
    "        raw_content[title] = content\n",
    "\n",
    "\n",
    "    # Clean the extracted raw content for each required section\n",
    "    for title in sections_to_find.keys():\n",
    "        if title in raw_content:\n",
    "            content_to_clean = raw_content[title]\n",
    "            # Basic cleaning: split lines, remove leading markers/whitespace, join\n",
    "            lines = content_to_clean.splitlines()\n",
    "            cleaned_lines = []\n",
    "            for line in lines:\n",
    "                # Remove leading list markers, hyphens, stars, digits, colons, whitespace\n",
    "                cleaned_line = re.sub(r'^\\s*[-\\*\\u2022\\d\\.\\:]+\\s*', '', line).strip()\n",
    "                # Remove potential leftover sub-titles (simple version)\n",
    "                cleaned_line = re.sub(r'^\\s*\\w+\\s*:', '', cleaned_line).strip()\n",
    "\n",
    "                if cleaned_line:\n",
    "                    cleaned_lines.append(cleaned_line)\n",
    "            cleaned_content = ' '.join(cleaned_lines) # Join with spaces\n",
    "\n",
    "            if cleaned_content: # Only store if cleaning didn't result in empty string\n",
    "                 sections_to_find[title] = cleaned_content\n",
    "\n",
    "\n",
    "    # --- Validation and Fallback Logic ---\n",
    "    # Check if *all three* required sections were found and have content\n",
    "    all_sections_found = all(content is not None for content in sections_to_find.values())\n",
    "\n",
    "    if all_sections_found:\n",
    "        # Format the output string\n",
    "        output_lines = [f\"{title}: {content}\" for title, content in sections_to_find.items()]\n",
    "        return '\\n'.join(output_lines)\n",
    "    else:\n",
    "        # If any section is missing or empty after cleaning, return the original lowercased text\n",
    "        # print(f\"Debug: Fallback triggered. Found sections: { {k:v is not None for k,v in sections_to_find.items()} }\") # Optional Debug\n",
    "        return text_lower\n",
    "\n",
    "def prompt_it(current_df, original_train_df, label_encoder, n_shot=1):\n",
    "    \"\"\"Formats data into prompts including few-shot examples. Removes reasoning limit.\"\"\"\n",
    "    prompts = []\n",
    "    ocr_word_limit = 75\n",
    "    # REMOVED reasoning_word_limit\n",
    "\n",
    "    original_train_lookup = {idx: row for idx, row in original_train_df.iterrows()}\n",
    "    class_names_str = str(list(label_encoder.classes_)) # Get current class names\n",
    "\n",
    "    print(f\"Generating prompts with {n_shot} examples...\")\n",
    "    for idx, row in tqdm(current_df.iterrows(), total=len(current_df), desc=\"Generating Prompts\"):\n",
    "        current_ocr = row['ocr_text']\n",
    "        current_reasoning = row['processed_reasoning'] # Use the processed version\n",
    "        retrieved_indices_str = row[f'top_{TOP_K}_train_indices']\n",
    "\n",
    "        try:\n",
    "            retrieved_indices = ast.literal_eval(retrieved_indices_str)\n",
    "            if not isinstance(retrieved_indices, list): raise ValueError(\"Parsed indices not list\")\n",
    "        except (ValueError, SyntaxError, TypeError) as e:\n",
    "            retrieved_indices = []\n",
    "\n",
    "        # --- Build Few-Shot Examples ---\n",
    "        few_shot_examples_str = \"\"\n",
    "        indices_to_use = retrieved_indices[:n_shot]\n",
    "        example_count = 0\n",
    "        for i, train_index in enumerate(indices_to_use):\n",
    "            try:\n",
    "                if train_index in original_train_lookup:\n",
    "                    example_row = original_train_lookup[train_index]\n",
    "                    # Make sure this example isn't one of the removed classes\n",
    "                    if example_row['meme_anxiety_category'] in CLASSES_TO_REMOVE:\n",
    "                        continue # Skip this example if its class was removed\n",
    "\n",
    "                    shot_ocr = example_row['ocr_text']\n",
    "                    shot_reasoning = process_triples(example_row['figurative_reasoning'])\n",
    "                    shot_label = example_row['meme_anxiety_category']\n",
    "\n",
    "                    shot_ocr_limited = split_ocr(shot_ocr, ocr_word_limit)\n",
    "                    # REMOVED limit on shot_reasoning\n",
    "                    # shot_reasoning_limited = split_ocr(shot_reasoning, reasoning_word_limit)\n",
    "\n",
    "                    few_shot_examples_str += f\"##Example {example_count + 1}:\\n\"\n",
    "                    few_shot_examples_str += f\"<|ocr_text|>{shot_ocr_limited}\\n\"\n",
    "                    few_shot_examples_str += f\"<|commonsense figurative explanation|>{shot_reasoning}\\n\" # Use full reasoning\n",
    "                    few_shot_examples_str += f\"The mental health disorder of the person for this post is: {shot_label}\\n\\n\"\n",
    "                    example_count += 1\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning: Error processing few-shot example index {train_index} for {row['sample_id']}: {e}.\")\n",
    "\n",
    "\n",
    "        # --- Build Final Prompt ---\n",
    "        current_ocr_limited = split_ocr(current_ocr, ocr_word_limit)\n",
    "        # REMOVED limit on current_reasoning\n",
    "        # current_reasoning_limited = split_ocr(current_reasoning, reasoning_word_limit)\n",
    "\n",
    "        prompt = f\"#System: You specialize in analyzing mental health behaviors through social media posts. Your task is to classify the mental health issue depicted in a person's post from the following categories: {class_names_str}.\\n\\n\" # Use dynamic class names\n",
    "        prompt += few_shot_examples_str\n",
    "        prompt += \"###Your_turn:\\n\"\n",
    "        prompt += f\"<|ocr_text|>{current_ocr_limited}\\n\"\n",
    "        prompt += f\"<|commonsense figurative explanation|>{current_reasoning}\\n\" # Use full reasoning\n",
    "        prompt += \"The mental health disorder of the person for this post is: \"\n",
    "\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    current_df['prompt'] = prompts\n",
    "    return current_df\n",
    "\n",
    "\n",
    "# --- Dataset Class --- (Identical)\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx] if isinstance(self.texts[idx], str) else str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].squeeze(0)\n",
    "            attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids.to(torch.long),\n",
    "                'attention_mask': attention_mask.to(torch.long),\n",
    "                'targets': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "             print(f\"Error tokenizing text at index {idx}: {e}\")\n",
    "             dummy_encoding = self.tokenizer.encode_plus(\"Error\", max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "             return {\n",
    "                'input_ids': dummy_encoding['input_ids'].squeeze(0).to(torch.long),\n",
    "                'attention_mask': dummy_encoding['attention_mask'].squeeze(0).to(torch.long),\n",
    "                'targets': torch.tensor(-1, dtype=torch.long)\n",
    "             }\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_model(training_loader, model, optimizer, criterion, device, gradient_clip_val):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    loop = tqdm(training_loader, total=len(training_loader), leave=False, colour='cyan', desc=\"Training\")\n",
    "    for batch_idx, data in enumerate(loop):\n",
    "        valid_indices = data['targets'] != -1\n",
    "        if not valid_indices.any(): continue\n",
    "\n",
    "        ids = data['input_ids'][valid_indices].to(device)\n",
    "        mask = data['attention_mask'][valid_indices].to(device)\n",
    "        targets = data['targets'][valid_indices].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask).logits\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        if torch.isnan(loss): continue\n",
    "\n",
    "        loss.backward()\n",
    "        # Apply Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    return np.mean(train_losses) if train_losses else 0\n",
    "\n",
    "# Eval model function remains the same\n",
    "def eval_model(validation_loader, model, criterion, device, target_names):\n",
    "    model.eval()\n",
    "    val_targets = []\n",
    "    val_outputs = []\n",
    "    val_losses = []\n",
    "    loop = tqdm(validation_loader, total=len(validation_loader), leave=False, colour='magenta', desc=\"Evaluating\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(loop):\n",
    "            valid_indices = data['targets'] != -1\n",
    "            if not valid_indices.any(): continue\n",
    "\n",
    "            ids = data['input_ids'][valid_indices].to(device)\n",
    "            mask = data['attention_mask'][valid_indices].to(device)\n",
    "            targets = data['targets'][valid_indices].to(device)\n",
    "\n",
    "            outputs = model(ids, attention_mask=mask).logits\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            if torch.isnan(loss): continue\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_outputs.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    val_targets = np.array(val_targets)\n",
    "    val_outputs = np.array(val_outputs)\n",
    "\n",
    "    if len(val_targets) == 0:\n",
    "        print(\"Warning: No valid validation samples processed.\")\n",
    "        zero_report = {\n",
    "            'accuracy': 0.0,\n",
    "            'macro avg': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 0},\n",
    "            'weighted avg': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 0}\n",
    "        }\n",
    "        for name in target_names:\n",
    "             zero_report[name] = {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 0}\n",
    "        return zero_report, [], [], 0.0, 0.0\n",
    "\n",
    "    accuracy = accuracy_score(val_targets, val_outputs)\n",
    "    # Get unique labels actually present in targets/preds for report generation if needed\n",
    "    present_labels = np.unique(np.concatenate((val_targets, val_outputs)))\n",
    "    report = classification_report(\n",
    "        val_targets, val_outputs,\n",
    "        target_names=[target_names[i] for i in present_labels], # Only names for present labels\n",
    "        labels=present_labels, # Only score present labels\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    # Add overall metrics manually if needed for consistency, or recalculate full report\n",
    "    full_report = classification_report(\n",
    "        val_targets, val_outputs,\n",
    "        target_names=target_names, # All possible names\n",
    "        labels=np.arange(len(target_names)), # All possible label indices\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "\n",
    "    return full_report, val_targets, val_outputs, np.mean(val_losses) if val_losses else 0, accuracy\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        train_orig_df = pd.read_json(TARGET_TRAIN_FILE_PATH)\n",
    "        test_orig_df = pd.read_json(TARGET_TEST_FILE_PATH)\n",
    "        train_retrieval_df = pd.read_csv(RETRIEVED_TRAIN_INDICES_FILE)\n",
    "        test_retrieval_df = pd.read_csv(RETRIEVED_TEST_INDICES_FILE)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}. Please check paths.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Remove specified classes ---\n",
    "    label_col = 'meme_anxiety_category' # Define label column name\n",
    "    print(f\"Original train samples: {len(train_orig_df)}\")\n",
    "    print(f\"Original test samples: {len(test_orig_df)}\")\n",
    "    print(f\"Removing classes: {CLASSES_TO_REMOVE}\")\n",
    "    train_orig_df = train_orig_df[~train_orig_df[label_col].isin(CLASSES_TO_REMOVE)]\n",
    "    test_orig_df = test_orig_df[~test_orig_df[label_col].isin(CLASSES_TO_REMOVE)]\n",
    "    print(f\"Filtered train samples: {len(train_orig_df)}\")\n",
    "    print(f\"Filtered test samples: {len(test_orig_df)}\")\n",
    "    # Reset index after filtering to ensure iloc works correctly later\n",
    "    train_orig_df.reset_index(drop=True, inplace=True)\n",
    "    test_orig_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # 2. Merge DataFrames\n",
    "    print(\"Merging dataframes...\")\n",
    "    train_orig_df['sample_id'] = train_orig_df['sample_id'].astype(str)\n",
    "    test_orig_df['sample_id'] = test_orig_df['sample_id'].astype(str)\n",
    "    train_retrieval_df['sample_id'] = train_retrieval_df['sample_id'].astype(str)\n",
    "    test_retrieval_df['sample_id'] = test_retrieval_df['sample_id'].astype(str)\n",
    "\n",
    "    # Important: Merge filtered data with retrieval data\n",
    "    train_df = pd.merge(train_orig_df, train_retrieval_df, on='sample_id', how='left')\n",
    "    test_df = pd.merge(test_orig_df, test_retrieval_df, on='sample_id', how='left')\n",
    "\n",
    "    indices_col = f'top_{TOP_K}_train_indices'\n",
    "    if train_df[indices_col].isnull().any():\n",
    "        missing_count = train_df[indices_col].isnull().sum()\n",
    "        print(f\"Warning: {missing_count} training samples missing retrieval indices after merge. Filling with '[]'.\")\n",
    "        train_df[indices_col].fillna('[]', inplace=True)\n",
    "    if test_df[indices_col].isnull().any():\n",
    "        missing_count = test_df[indices_col].isnull().sum()\n",
    "        print(f\"Warning: {missing_count} test samples missing retrieval indices after merge. Filling with '[]'.\")\n",
    "        test_df[indices_col].fillna('[]', inplace=True)\n",
    "\n",
    "\n",
    "    # 3. Process Figurative Reasoning\n",
    "    reasoning_col = 'figurative_reasoning'\n",
    "    print(f\"Processing reasoning column: {reasoning_col}\")\n",
    "    train_df['processed_reasoning'] = train_df[reasoning_col].apply(process_triples)\n",
    "    test_df['processed_reasoning'] = test_df[reasoning_col].apply(process_triples)\n",
    "\n",
    "    # --- Example Check after processing ---\n",
    "    print(\"\\n--- Checking Processed Reasoning (Example) ---\")\n",
    "    example_idx_check = train_df[train_df['sample_id'] == 'TR-2483'].index[0]\n",
    "    print(f\"Original Reasoning (TR-2483):\\n{train_df.loc[example_idx_check, reasoning_col]}\\n\")\n",
    "    print(f\"Processed Reasoning (TR-2483):\\n{train_df.loc[example_idx_check, 'processed_reasoning']}\")\n",
    "    print(\"-------------------------------------------\\n\")\n",
    "    # --- Check another random one from test ---\n",
    "    if not test_df.empty:\n",
    "        print(f\"Processed Reasoning (Test Sample 0):\\n{test_df.loc[0, 'processed_reasoning']}\")\n",
    "        print(\"-------------------------------------------\\n\")\n",
    "\n",
    "    # 4. Label Encoding (AFTER filtering)\n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    # Fit on the filtered training data labels\n",
    "    train_df['labels'] = le.fit_transform(train_df[label_col])\n",
    "    # Transform the filtered test data labels\n",
    "    test_df['labels'] = le.transform(test_df[label_col])\n",
    "    num_labels = len(le.classes_)\n",
    "    class_names = list(le.classes_) # Get final class names\n",
    "    print(f\"Found {num_labels} final classes after filtering: {class_names}\")\n",
    "\n",
    "\n",
    "    # 5. Generate Prompts (pass the updated label encoder)\n",
    "    # We need the original train_df *before* filtering for the few-shot lookup,\n",
    "    # but we need to ensure the few-shot examples selected are not from the removed classes.\n",
    "    # Let's reload the original train df just for the lookup inside prompt_it\n",
    "    train_orig_df_for_lookup = pd.read_json(TARGET_TRAIN_FILE_PATH)\n",
    "    train_orig_df_for_lookup.reset_index(drop=True, inplace=True) # Ensure index consistency\n",
    "\n",
    "    train_df = prompt_it(train_df, train_orig_df_for_lookup, le, n_shot=N_SHOT)\n",
    "    test_df = prompt_it(test_df, train_orig_df_for_lookup, le, n_shot=N_SHOT)\n",
    "\n",
    "\n",
    "    # 6. Tokenizer and Model Loading\n",
    "    # print(f\"Loading tokenizer and model: {MODEL_NAME}\")\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # # Load config first if you want to try changing dropout (may require retraining from scratch)\n",
    "    # # config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    # # config.dropout = DROPOUT_RATE\n",
    "    # # config.attention_dropout = DROPOUT_RATE\n",
    "    # # model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, config=config)\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "\n",
    "    # # 7. Layer Freezing (remains the same)\n",
    "    # print(f\"Freezing layers below {FREEZE_LAYERS_BELOW}...\")\n",
    "    # layer_prefix = \"model.\"\n",
    "    # frozen_count = 0\n",
    "    # total_params = 0\n",
    "    # trainable_params = 0\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     total_params += 1\n",
    "    #     do_freeze = False\n",
    "    #     if name.startswith(layer_prefix + \"encoder.layers.\") or \\\n",
    "    #        name.startswith(layer_prefix + \"decoder.layers.\"):\n",
    "    #        try:\n",
    "    #            layer_num = int(name.split('.')[3])\n",
    "    #            if layer_num < FREEZE_LAYERS_BELOW:\n",
    "    #                param.requires_grad = False\n",
    "    #                do_freeze = True\n",
    "    #                frozen_count +=1\n",
    "    #        except (IndexError, ValueError): pass\n",
    "    #     if param.requires_grad: trainable_params += 1\n",
    "    # print(f\"Froze {frozen_count}/{total_params} param groups. Trainable: {trainable_params}\")\n",
    "\n",
    "\n",
    "    # # Move model to device(s)\n",
    "    # if torch.cuda.device_count() > 1 and len(DEVICE_IDS) > 1 :\n",
    "    #     print(f\"Using {len(DEVICE_IDS)} GPUs with DataParallel.\")\n",
    "    #     model = torch.nn.DataParallel(model, device_ids=DEVICE_IDS)\n",
    "    # model.to(PRIMARY_DEVICE)\n",
    "\n",
    "\n",
    "    # # 8. Create Datasets and DataLoaders\n",
    "    # print(\"Creating datasets and dataloaders...\")\n",
    "    # train_dataset = MemeDataset(train_df['prompt'].tolist(), train_df['labels'].tolist(), tokenizer, MAX_LENGTH)\n",
    "    # val_dataset = MemeDataset(test_df['prompt'].tolist(), test_df['labels'].tolist(), tokenizer, MAX_LENGTH)\n",
    "\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "    # # 9. Optimizer and Loss Function with Paper Hyperparameters\n",
    "    # print(\"Setting up optimizer with specified hyperparameters...\")\n",
    "    # optimizer = optim.AdamW(\n",
    "    #     filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    #     lr=LEARNING_RATE,\n",
    "    #     betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "    #     eps=ADAM_EPSILON,\n",
    "    #     weight_decay=WEIGHT_DECAY\n",
    "    # )\n",
    "    # criterion = nn.CrossEntropyLoss() # Correct for single-label\n",
    "\n",
    "\n",
    "    # # 10. Training Loop\n",
    "    # print(\"Starting training...\")\n",
    "    # history = defaultdict(list)\n",
    "    # best_f1_macro = 0.0\n",
    "\n",
    "    # for epoch in range(1, EPOCHS + 1):\n",
    "    #     print(f'\\n--- Epoch {epoch}/{EPOCHS} ---')\n",
    "    #     # Pass gradient clip value to training function\n",
    "    #     train_loss = train_model(train_loader, model, optimizer, criterion, PRIMARY_DEVICE, GRADIENT_CLIP_THRESHOLD)\n",
    "    #     # Pass final class names to eval function\n",
    "    #     report, val_targets, val_outputs, val_loss, accuracy = eval_model(val_loader, model, criterion, PRIMARY_DEVICE, class_names)\n",
    "\n",
    "    #     # Safely access metrics from report\n",
    "    #     val_f1_macro = report.get('macro avg', {}).get('f1-score', 0.0)\n",
    "    #     val_f1_weighted = report.get('weighted avg', {}).get('f1-score', 0.0)\n",
    "\n",
    "    #     print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    #     print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    #     print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    #     print(f\"  Val Accuracy: {accuracy:.4f}\")\n",
    "    #     print(f\"  Val F1 Macro: {val_f1_macro:.4f}\")\n",
    "    #     print(f\"  Val F1 Weighted: {val_f1_weighted:.4f}\")\n",
    "\n",
    "    #     history['train_loss'].append(train_loss)\n",
    "    #     history['val_loss'].append(val_loss)\n",
    "    #     history['val_accuracy'].append(accuracy)\n",
    "    #     history['val_f1_macro'].append(val_f1_macro)\n",
    "    #     history['val_f1_weighted'].append(val_f1_weighted)\n",
    "\n",
    "    #     if val_f1_macro > best_f1_macro:\n",
    "    #         best_f1_macro = val_f1_macro\n",
    "    #         model_to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "    #         save_dir = \"mental_bart_anxiety_finetuned_best_hyperparams\" # New dir name\n",
    "    #         os.makedirs(save_dir, exist_ok=True)\n",
    "    #         model_to_save.save_pretrained(save_dir)\n",
    "    #         tokenizer.save_pretrained(save_dir)\n",
    "    #         pd.Series(le.classes_).to_json(os.path.join(save_dir, \"label_classes.json\")) # Save final classes\n",
    "    #         print(f\"  -> New best model saved with Macro F1: {best_f1_macro:.4f} in '{save_dir}'\")\n",
    "\n",
    "    #         if val_targets is not None and val_outputs is not None and len(val_targets) > 0:\n",
    "    #              print(\"\\nClassification Report (Best Epoch):\\n\", classification_report(\n",
    "    #                  val_targets, val_outputs,\n",
    "    #                  target_names=class_names, zero_division=0, labels=np.arange(len(class_names))\n",
    "    #              ))\n",
    "\n",
    "    # # 11. Plotting Results\n",
    "    # print(\"\\nPlotting training history...\")\n",
    "    # # (Plotting code remains the same)\n",
    "    # fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "    # axs[0].plot(history['train_loss'], label='Train Loss')\n",
    "    # axs[0].plot(history['val_loss'], label='Validation Loss')\n",
    "    # axs[0].set_title('Training and Validation Losses')\n",
    "    # axs[0].set_ylabel('Loss')\n",
    "    # axs[0].set_xlabel('Epoch')\n",
    "    # axs[0].legend()\n",
    "    # axs[0].grid(True)\n",
    "\n",
    "    # axs[1].plot(history['val_accuracy'], label='Validation Accuracy', color='green')\n",
    "    # axs[1].set_title('Validation Accuracy')\n",
    "    # axs[1].set_ylabel('Accuracy')\n",
    "    # axs[1].set_xlabel('Epoch')\n",
    "    # axs[1].legend()\n",
    "    # axs[1].grid(True)\n",
    "\n",
    "    # axs[2].plot(history['val_f1_macro'], label='Validation F1 Macro', color='orange')\n",
    "    # axs[2].plot(history['val_f1_weighted'], label='Validation F1 Weighted', color='red')\n",
    "    # axs[2].set_title('Validation F1 Scores')\n",
    "    # axs[2].set_ylabel('F1 Score')\n",
    "    # axs[2].set_xlabel('Epoch')\n",
    "    # axs[2].legend()\n",
    "    # axs[2].grid(True)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plot_filename = os.path.join(EMBEDDING_DIR, \"training_history_plot_hyperparams.png\") # New plot name\n",
    "    # try:\n",
    "    #     plt.savefig(plot_filename)\n",
    "    #     print(f\"Training history plot saved to {plot_filename}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error saving plot: {e}\")\n",
    "\n",
    "    # print(\"\\n--- Training Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
