{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 1: Installs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip config set global.timeout 600\n",
    "# Using slightly newer compatible versions where possible\n",
    "!pip install -q \\\n",
    "    transformers==4.38.2 \\\n",
    "    sentence-transformers==2.7.0 \\\n",
    "    faiss-cpu==1.8.0 \\\n",
    "    torch==2.1.2 \\\n",
    "    accelerate==0.28.0 \\\n",
    "    scikit-learn==1.3.2 \\\n",
    "    pandas==2.1.4 \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    Pillow==10.2.0 \\\n",
    "    tqdm==4.66.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 2: Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BartForSequenceClassification,\n",
    "    BartTokenizer,\n",
    "    BartModel, # Base BART model\n",
    "    CLIPProcessor, # For vision features\n",
    "    CLIPVisionModel, # For vision features\n",
    "    get_linear_schedule_with_warmup,\n",
    "    CLIPVisionConfig # To get visual feature dimension\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, hamming_loss, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pickle\n",
    "import gc # Garbage collector\n",
    "import torch.nn.functional as F # For sigmoid and softmax\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 3: Configuration and Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 3: Configuration and Seed (Updated)\n",
    "\n",
    "# --- Basic Hyperparameters ---\n",
    "MAX_LEN = 512          # Max length for BART tokenizer\n",
    "BATCH_SIZE = 8         # Adjust based on GPU memory (T4 likely needs 4 or 8)\n",
    "NUM_EPOCHS = 10        # Number of training epochs PER ensemble member\n",
    "LEARNING_RATE = 3e-5\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "WEIGHT_DECAY = 0.01\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# --- RAG Hyperparameters ---\n",
    "TEXT_EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
    "RETRIEVAL_K = 3          # Number of examples to retrieve for prompts\n",
    "\n",
    "# --- Vision Hyperparameters ---\n",
    "VISION_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "try:\n",
    "    vision_config = CLIPVisionConfig.from_pretrained(VISION_MODEL_NAME)\n",
    "    VISUAL_FEATURE_DIM = vision_config.projection_dim\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not load vision config for {VISION_MODEL_NAME}: {e}. Defaulting VISUAL_FEATURE_DIM to 768.\")\n",
    "    VISUAL_FEATURE_DIM = 768\n",
    "\n",
    "# --- Ensemble Hyperparameters ---\n",
    "NUM_ENSEMBLE_MODELS = 3 # Number of models to train in the ensemble\n",
    "BASE_SEED = 42           # Base seed for reproducibility\n",
    "\n",
    "# --- Classification Model ---\n",
    "# <<< CHANGE HERE: Use the MentalBART model identifier >>>\n",
    "BASE_TEXT_MODEL_NAME = \"mental/mental-bart-base-cased\"\n",
    "# <<< END CHANGE >>>\n",
    "\n",
    "\n",
    "# --- Kaggle File Paths ---\n",
    "KAGGLE_INPUT_DIR = \"/kaggle/input/axiom-dataset\"\n",
    "KAGGLE_WORKING_DIR = \"/kaggle/working\"\n",
    "\n",
    "# --- Seed Function ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # torch.backends.cudnn.deterministic = True # Optional for reproducibility\n",
    "    # torch.backends.cudnn.benchmark = False   # Optional\n",
    "    logger.info(f\"Seed set to {seed}\")\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "logger.info(f\"Visual Feature Dimension set to: {VISUAL_FEATURE_DIM}\")\n",
    "# Log the base text model being used\n",
    "logger.info(f\"Base Text Model set to: {BASE_TEXT_MODEL_NAME}\")\n",
    "logger.info(f\"Text Embedding Model: {TEXT_EMBEDDING_MODEL}\")\n",
    "logger.info(f\"Vision Model: {VISION_MODEL_NAME}\")\n",
    "\n",
    "\n",
    "# Set initial seed\n",
    "set_seed(BASE_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 4: Visual Feature Extraction (Run Once)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === VISUAL FEATURE EXTRACTION ===\n",
    "# NOTE: This cell only needs to be run once if the features are saved.\n",
    "# If features (.pt files) already exist in /kaggle/working/visual_features,\n",
    "# you can skip running this cell on subsequent runs.\n",
    "\n",
    "def get_image_path(sample_id, dataset_base_path, json_path):\n",
    "    \"\"\"\n",
    "    Constructs the image path based on sample ID and base path.\n",
    "    Uses json_path to infer the split if prefix is ambiguous.\n",
    "    \"\"\"\n",
    "    split = None\n",
    "    # Try matching known prefixes first\n",
    "    if isinstance(sample_id, str):\n",
    "        if sample_id.startswith('TR-'): split = 'train'\n",
    "        elif sample_id.startswith('TE-'): split = 'test'\n",
    "        elif sample_id.startswith('VL-'): split = 'val' # Assuming VL- for validation if needed\n",
    "\n",
    "    # If split not determined by prefix, infer from the JSON file path\n",
    "    if not split:\n",
    "        if 'train.json' in json_path or 'anxiety_train.json' in json_path:\n",
    "            split = 'train'\n",
    "        elif 'test.json' in json_path or 'anxiety_test.json' in json_path:\n",
    "            split = 'test'\n",
    "        elif 'val.json' in json_path:\n",
    "             split = 'val'\n",
    "        else:\n",
    "            # Last resort - needs better logic if IDs aren't prefixed and file names don't help\n",
    "            logger.warning(f\"Cannot determine split (train/test/val) for sample_id: {sample_id} from path {json_path}. Assuming 'train'. Check logic.\")\n",
    "            split = 'train'\n",
    "\n",
    "    # Construct the image folder path based on dataset type and split\n",
    "    img_folder = None\n",
    "    parent_dir = os.path.dirname(dataset_base_path) # Go one level up from Anxiety_Data or Depressive_Data\n",
    "    if \"Anxiety_Data\" in dataset_base_path:\n",
    "        img_folder = os.path.join(parent_dir, f\"anxiety_{split}_image\")\n",
    "    elif \"Depressive_Data\" in dataset_base_path:\n",
    "        # Adjusted path structure for depression based on observed zip structure\n",
    "        # Check if Images/depressive_image/split exists\n",
    "        potential_path1 = os.path.join(parent_dir, \"Images\", \"depressive_image\", split)\n",
    "        if os.path.isdir(potential_path1):\n",
    "             img_folder = potential_path1\n",
    "        else:\n",
    "             # Fallback if the structure is different (e.g., directly under depressive_image)\n",
    "             potential_path2 = os.path.join(parent_dir, \"depressive_image\", split)\n",
    "             if os.path.isdir(potential_path2):\n",
    "                 img_folder = potential_path2\n",
    "             else:\n",
    "                 logger.error(f\"Cannot find depression image folder for split '{split}' near {parent_dir}\")\n",
    "                 return None\n",
    "    else:\n",
    "        logger.error(f\"Unknown dataset base path structure: {dataset_base_path}\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.isdir(img_folder):\n",
    "        logger.error(f\"Determined image folder does not exist: {img_folder}\")\n",
    "        return None\n",
    "\n",
    "    # Try common extensions\n",
    "    for ext in ['.jpeg', '.jpg', '.png']:\n",
    "        img_path = os.path.join(img_folder, f\"{sample_id}{ext}\")\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "\n",
    "    logger.debug(f\"Image file not found for {sample_id} in {img_folder} with common extensions (.jpeg, .jpg, .png).\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_and_save_features(dataset_name, split_name, json_path, dataset_base_path, processor, vision_model, device, output_file):\n",
    "    \"\"\"Extracts CLIP features for a given dataset split and saves them.\"\"\"\n",
    "    logger.info(f\"Extracting features for {dataset_name} - {split_name} from {json_path}...\")\n",
    "    features_map = {}\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"JSON file not found: {json_path}. Cannot extract features.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load JSON {json_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    missing_images = 0\n",
    "    vision_model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for sample in tqdm(data, desc=f\"Processing {split_name} images for {dataset_name}\"):\n",
    "            sample_id = sample.get('sample_id', sample.get('id')) # Handle both 'sample_id' and 'id' keys\n",
    "            if not sample_id:\n",
    "                logger.warning(\"Skipping sample with missing ID.\")\n",
    "                continue\n",
    "\n",
    "            # Pass the json_path to help get_image_path infer split if needed\n",
    "            image_path = get_image_path(sample_id, dataset_base_path, json_path)\n",
    "\n",
    "            if image_path and os.path.exists(image_path):\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "                    outputs = vision_model(**inputs)\n",
    "                    # Use pooler_output which is the CLS token's embedding after projection for CLIP\n",
    "                    # Squeeze to remove batch dimension (as we process one image at a time)\n",
    "                    features = outputs.pooler_output.squeeze().cpu() # (feature_dim,) -> Move to CPU before storing\n",
    "                    if features.shape[0] != VISUAL_FEATURE_DIM:\n",
    "                         logger.warning(f\"Extracted feature dim {features.shape[0]} != expected {VISUAL_FEATURE_DIM} for {sample_id}. Check CLIP model/config.\")\n",
    "                         # Handle dimension mismatch if necessary (e.g., skip, pad, project) - here we just warn\n",
    "                    features_map[sample_id] = features\n",
    "                except FileNotFoundError:\n",
    "                    # This case should be less likely now with the initial check\n",
    "                    logger.warning(f\"File not found during processing (should have been checked): {image_path}\")\n",
    "                    missing_images += 1\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to process image {image_path}: {e}\")\n",
    "                    missing_images += 1\n",
    "            else:\n",
    "                logger.debug(f\"Image path not found or invalid for sample_id: {sample_id}. Path sought: {image_path}\")\n",
    "                missing_images += 1\n",
    "\n",
    "    if missing_images > 0:\n",
    "        logger.warning(f\"Could not find or process {missing_images}/{len(data)} images for {dataset_name} - {split_name}.\")\n",
    "\n",
    "    if features_map:\n",
    "        logger.info(f\"Saving {len(features_map)} extracted features to {output_file}...\")\n",
    "        try:\n",
    "            torch.save(features_map, output_file)\n",
    "            logger.info(\"Features saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save features to {output_file}: {e}\")\n",
    "    else:\n",
    "        logger.warning(f\"No features were extracted for {dataset_name} - {split_name}. Output file not saved.\")\n",
    "\n",
    "# --- Load CLIP Model --- (Do this once)\n",
    "clip_processor = None\n",
    "clip_vision_model = None\n",
    "try:\n",
    "    logger.info(f\"Loading CLIP processor and vision model: {VISION_MODEL_NAME}\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(VISION_MODEL_NAME)\n",
    "    clip_vision_model = CLIPVisionModel.from_pretrained(VISION_MODEL_NAME).to(device)\n",
    "    clip_vision_model.eval() # Ensure model is in eval mode\n",
    "    logger.info(\"CLIP models loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load CLIP model or processor: {e}\", exc_info=True)\n",
    "    logger.error(\"Visual feature extraction cannot proceed.\")\n",
    "\n",
    "# --- Define Paths and Run Extraction (if models loaded) ---\n",
    "if clip_processor and clip_vision_model:\n",
    "    # Define base directories within the input dataset\n",
    "    anxiety_base_dir = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Anxiety_Data\")\n",
    "    depression_base_dir = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Depressive_Data\")\n",
    "\n",
    "    # Define the output directory for saving features\n",
    "    output_feature_dir = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\")\n",
    "    os.makedirs(output_feature_dir, exist_ok=True)\n",
    "    logger.info(f\"Visual features will be saved to: {output_feature_dir}\")\n",
    "\n",
    "    datasets_to_process = [\n",
    "        # Anxiety\n",
    "        (\"anxiety\", \"train\", os.path.join(anxiety_base_dir, \"anxiety_train.json\"), anxiety_base_dir, os.path.join(output_feature_dir, \"anxiety_train_features.pt\")),\n",
    "        (\"anxiety\", \"test\", os.path.join(anxiety_base_dir, \"anxiety_test.json\"), anxiety_base_dir, os.path.join(output_feature_dir, \"anxiety_test_features.pt\")),\n",
    "        # Depression\n",
    "        (\"depression\", \"train\", os.path.join(depression_base_dir, \"train.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_train_features.pt\")),\n",
    "        (\"depression\", \"test\", os.path.join(depression_base_dir, \"test.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_test_features.pt\")),\n",
    "        (\"depression\", \"val\", os.path.join(depression_base_dir, \"val.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_val_features.pt\")) # Add validation set\n",
    "    ]\n",
    "\n",
    "    for name, split, json_p, data_base_p, out_f in datasets_to_process:\n",
    "        # Only process if the corresponding JSON file exists\n",
    "        if os.path.exists(json_p):\n",
    "            # Check if features already exist before extracting\n",
    "            if not os.path.exists(out_f):\n",
    "                 logger.info(f\"Starting feature extraction for: {name} - {split}\")\n",
    "                 extract_and_save_features(name, split, json_p, data_base_p, clip_processor, clip_vision_model, device, out_f)\n",
    "                 gc.collect() # Collect garbage after processing each split\n",
    "                 if device == torch.device('cuda'):\n",
    "                     torch.cuda.empty_cache()\n",
    "            else:\n",
    "                 logger.info(f\"Visual features file already exists: {out_f}. Skipping extraction for {name} - {split}.\")\n",
    "        else:\n",
    "            logger.warning(f\"JSON file not found: {json_p}. Skipping feature extraction for {name} - {split}.\")\n",
    "\n",
    "    # Clean up vision model from memory after all extractions are done\n",
    "    logger.info(\"Visual feature extraction process finished. Cleaning up CLIP model...\")\n",
    "    del clip_vision_model\n",
    "    del clip_processor\n",
    "    gc.collect()\n",
    "    if device == torch.device('cuda'):\n",
    "        torch.cuda.empty_cache()\n",
    "    logger.info(\"CLIP model cleanup complete.\")\n",
    "else:\n",
    "    logger.error(\"CLIP model failed to load. Cannot extract visual features.\")\n",
    "    # Define dummy path so pipeline doesn't crash immediately, but loading will fail later\n",
    "    output_feature_dir = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\")\n",
    "    os.makedirs(output_feature_dir, exist_ok=True) # Still create the dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 5: Data Loading, Cleaning, Splitting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- load_data Function (handles image_id and label variations) ---\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads data from JSON, extracts relevant fields, handles label variations.\"\"\"\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "         logger.error(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "         # Optionally try to load line by line if it's jsonl\n",
    "         logger.info(f\"Attempting to load {file_path} as JSON Lines (.jsonl)\")\n",
    "         data = []\n",
    "         try:\n",
    "             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                 for line in f:\n",
    "                     try:\n",
    "                         data.append(json.loads(line))\n",
    "                     except json.JSONDecodeError:\n",
    "                         logger.warning(f\"Skipping invalid JSON line in {file_path}: {line.strip()}\")\n",
    "             if not data: return [] # If still empty after trying jsonl\n",
    "             logger.info(f\"Successfully loaded {len(data)} lines as JSON Lines.\")\n",
    "         except Exception as e_jsonl:\n",
    "             logger.error(f\"Failed to load as JSON Lines as well: {e_jsonl}\")\n",
    "             return []\n",
    "\n",
    "    is_anxiety = \"Anxiety_Data\" in file_path\n",
    "    filtered_data = []\n",
    "    processed_ids = set() # Keep track of processed IDs to avoid duplicates\n",
    "\n",
    "    for idx, sample in enumerate(data):\n",
    "        if not isinstance(sample, dict):\n",
    "            logger.warning(f\"Skipping non-dictionary item at index {idx} in {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Use 'sample_id' primarily, fallback to 'id', then None\n",
    "        sample_id = sample.get('sample_id', sample.get('id', None))\n",
    "\n",
    "        # Skip if ID is missing or already processed\n",
    "        if sample_id is None:\n",
    "            logger.warning(f\"Skipping sample at index {idx} due to missing ID.\")\n",
    "            continue\n",
    "        if sample_id in processed_ids:\n",
    "             logger.warning(f\"Skipping duplicate sample ID: {sample_id}\")\n",
    "             continue\n",
    "        processed_ids.add(sample_id)\n",
    "\n",
    "        # Ensure necessary fields exist\n",
    "        ocr_text = sample.get('ocr_text', None)\n",
    "        triples = sample.get('triples', \"\") # Default to empty string if missing\n",
    "\n",
    "        # Get image identifier (use sample_id if 'image_id' isn't explicitly present)\n",
    "        image_id = sample.get('image_id', sample_id)\n",
    "        if not image_id:\n",
    "             logger.warning(f\"Sample {sample_id} missing a valid image identifier. Using sample_id '{sample_id}'.\")\n",
    "             image_id = sample_id # Fallback for safety, though should have ID by now\n",
    "\n",
    "        # Add image_id to the sample dictionary\n",
    "        sample['image_id'] = image_id\n",
    "\n",
    "        # Process based on dataset type\n",
    "        if is_anxiety:\n",
    "            anxiety_label_key = 'meme_anxiety_category'\n",
    "            original_label = sample.get(anxiety_label_key, None)\n",
    "\n",
    "            if ocr_text is not None and original_label is not None:\n",
    "                 # Standardize known label variations\n",
    "                 if original_label == 'Irritatbily': original_label = 'Irritability'\n",
    "                 elif original_label == 'Unknown': original_label = 'Unknown Anxiety'\n",
    "\n",
    "                 sample['original_labels'] = original_label # Single label for anxiety\n",
    "                 sample['stratify_label'] = original_label # Use the single label for stratification\n",
    "                 sample['triples'] = triples # Ensure triples are included\n",
    "                 sample['ocr_text'] = ocr_text # Ensure ocr_text is included\n",
    "                 filtered_data.append(sample)\n",
    "            else:\n",
    "                 logger.warning(f\"Skipping anxiety sample {sample_id} due to missing 'ocr_text' or '{anxiety_label_key}'. OCR: {'Present' if ocr_text is not None else 'Missing'}, Label: {'Present' if original_label is not None else 'Missing'}\")\n",
    "\n",
    "        else: # Depression (multilabel)\n",
    "             depression_labels_key = 'meme_depressive_categories'\n",
    "             original_label_data = sample.get(depression_labels_key, None)\n",
    "             processed_labels = []\n",
    "\n",
    "             if ocr_text is not None and original_label_data is not None:\n",
    "                  # Handle different formats of labels (list, string, potentially comma-separated string)\n",
    "                  if isinstance(original_label_data, list):\n",
    "                      processed_labels = [str(lbl).strip() for lbl in original_label_data if str(lbl).strip()]\n",
    "                  elif isinstance(original_label_data, str):\n",
    "                      # Simple split by comma if it's a string, could be more robust\n",
    "                      processed_labels = [lbl.strip() for lbl in original_label_data.split(',') if lbl.strip()]\n",
    "                  else:\n",
    "                      logger.warning(f\"Unexpected label format for depression sample {sample_id}: {type(original_label_data)}. Treating as empty.\")\n",
    "                      processed_labels = []\n",
    "\n",
    "                  # Ensure 'Unknown Depression' is handled if needed, though seems less common\n",
    "                  processed_labels = [lbl if lbl != 'Unknown' else 'Unknown Depression' for lbl in processed_labels]\n",
    "\n",
    "                  if not processed_labels:\n",
    "                       logger.warning(f\"Depression sample {sample_id} resulted in empty label list after processing. Assigning 'Unknown Depression'. Original data: {original_label_data}\")\n",
    "                       processed_labels = [\"Unknown Depression\"] # Assign a default if empty\n",
    "\n",
    "                  sample['original_labels'] = processed_labels # List of labels for depression\n",
    "                  # Use the first label for stratification (or a default if list is somehow empty)\n",
    "                  sample['stratify_label'] = processed_labels[0] if processed_labels else \"Unknown Depression\"\n",
    "                  sample['triples'] = triples # Ensure triples are included\n",
    "                  sample['ocr_text'] = ocr_text # Ensure ocr_text is included\n",
    "                  filtered_data.append(sample)\n",
    "             else:\n",
    "                 logger.warning(f\"Skipping depression sample {sample_id} due to missing 'ocr_text' or '{depression_labels_key}'. OCR: {'Present' if ocr_text is not None else 'Missing'}, Label: {'Present' if original_label_data is not None else 'Missing'}\")\n",
    "\n",
    "    logger.info(f\"Loaded {len(filtered_data)} samples from {file_path} after filtering and processing.\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# --- clean_triples Function ---\n",
    "def clean_triples(triples_text):\n",
    "    \"\"\"Cleans the structured triples text, keeping section headers.\"\"\"\n",
    "    if pd.isna(triples_text) or not isinstance(triples_text, str) or not triples_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # Define the sections expected in the triples\n",
    "    sections = [\"Cause-Effect\", \"Figurative Understanding\", \"Mental State\"]\n",
    "    cleaned_parts = []\n",
    "\n",
    "    # Use regex to capture content for each section, handling potential missing sections\n",
    "    current_text = triples_text\n",
    "    found_any_section = False\n",
    "    for i, section in enumerate(sections):\n",
    "        # Regex to find section header and capture text until the next known header or end of string\n",
    "        # (?s) is equivalent to re.DOTALL flag\n",
    "        # Lazily match content with .*?\n",
    "        # Lookahead (?=...) ensures we stop before the next header or end of string ($)\n",
    "        next_headers_pattern = \"|\".join(f\"{re.escape(s)}\\\\s*:\" for s in sections[i+1:])\n",
    "        if next_headers_pattern:\n",
    "             pattern = rf\"(?s){re.escape(section)}\\s*:(.*?)(?=\\s*(?:{next_headers_pattern}|$))\"\n",
    "        else: # Last section\n",
    "             pattern = rf\"(?s){re.escape(section)}\\s*:(.*)\"\n",
    "\n",
    "        match = re.search(pattern, current_text, re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            content = match.group(1).strip()\n",
    "            # Further clean the content: remove excessive whitespace\n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            if content: # Only add if there's actual content\n",
    "                cleaned_parts.append(f\"{section}: {content}\")\n",
    "                found_any_section = True\n",
    "                # Reduce the search space for the next iteration (optional but can help)\n",
    "                # current_text = current_text[match.end():] # This might be too aggressive if order isn't guaranteed\n",
    "        else:\n",
    "             logger.debug(f\"Section '{section}' not found or empty in triples: {triples_text[:100]}...\") # Log if a section is missed\n",
    "\n",
    "\n",
    "    # If no standard sections were found, return the original text after basic whitespace cleaning\n",
    "    if not found_any_section:\n",
    "         logger.debug(f\"No standard sections found in triples. Returning cleaned original text: {triples_text[:100]}...\")\n",
    "         return re.sub(r'\\s+', ' ', triples_text).strip()\n",
    "\n",
    "    # Join the cleaned parts with newlines\n",
    "    return \"\\n\".join(cleaned_parts).strip()\n",
    "\n",
    "\n",
    "# --- split_data Function (Handles stratification carefully) ---\n",
    "def split_data(data: List[Dict], val_size: float = 0.1, test_size: float = 0.2, random_state: int = 42) -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "    \"\"\"Splits data into train, validation, and test sets with stratification.\"\"\"\n",
    "    if not data:\n",
    "        logger.error(\"Cannot split empty data list.\")\n",
    "        return [], [], []\n",
    "\n",
    "    n_samples = len(data)\n",
    "    logger.info(f\"Attempting to split {n_samples} samples. Val ratio: {val_size}, Test ratio: {test_size}\")\n",
    "\n",
    "    # Ensure ratios are valid\n",
    "    if val_size < 0 or val_size >= 1 or test_size < 0 or test_size >= 1 or (val_size + test_size >= 1):\n",
    "        logger.error(f\"Invalid split ratios: val={val_size}, test={test_size}. Ratios must be [0, 1) and sum < 1.\")\n",
    "        # Default behavior: return all data as train if ratios invalid\n",
    "        return data, [], []\n",
    "\n",
    "    try:\n",
    "        # Extract labels for stratification\n",
    "        stratify_labels = [d['stratify_label'] for d in data]\n",
    "    except KeyError:\n",
    "        logger.error(\"All samples must have a 'stratify_label' key for splitting.\")\n",
    "        return [], [], [] # Cannot proceed without labels\n",
    "\n",
    "    # Check for labels with only one sample, which breaks stratification\n",
    "    unique_labels, counts = np.unique(stratify_labels, return_counts=True)\n",
    "    labels_with_one_sample = unique_labels[counts == 1]\n",
    "\n",
    "    if len(labels_with_one_sample) > 0:\n",
    "        logger.warning(f\"Labels with only 1 sample found: {list(labels_with_one_sample)}. Stratification might be unstable or fail. Consider merging/removing.\")\n",
    "        # Proceeding anyway, but train_test_split might raise errors later if splits result in single-sample classes\n",
    "\n",
    "    train_data, val_data, test_data = [], [], []\n",
    "\n",
    "    # --- First Split: Separate Test Set (if test_size > 0) ---\n",
    "    if test_size > 0:\n",
    "        remaining_data = data\n",
    "        remaining_labels = stratify_labels\n",
    "        try:\n",
    "            logger.info(f\"Splitting off test set ({test_size * 100:.1f}%)...\")\n",
    "            train_val_indices, test_indices = train_test_split(\n",
    "                range(n_samples),\n",
    "                test_size=test_size,\n",
    "                random_state=random_state,\n",
    "                stratify=remaining_labels\n",
    "            )\n",
    "            train_val_data = [remaining_data[i] for i in train_val_indices]\n",
    "            test_data = [remaining_data[i] for i in test_indices]\n",
    "            train_val_labels = [remaining_labels[i] for i in train_val_indices]\n",
    "            logger.info(f\"Split complete: Train/Val pool = {len(train_val_data)}, Test = {len(test_data)}\")\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"Stratified split for test set failed: {e}. Falling back to non-stratified split for test set.\")\n",
    "            train_val_indices, test_indices = train_test_split(\n",
    "                range(n_samples),\n",
    "                test_size=test_size,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            train_val_data = [remaining_data[i] for i in train_val_indices]\n",
    "            test_data = [remaining_data[i] for i in test_indices]\n",
    "            # We don't need train_val_labels if the next split is non-stratified\n",
    "            train_val_labels = [stratify_labels[i] for i in train_val_indices] # Still get them in case next split works\n",
    "\n",
    "    else: # No test set needed, all data goes to train/val pool\n",
    "        logger.info(\"No test set requested (test_size=0). Using all data for train/val split.\")\n",
    "        train_val_data = data\n",
    "        train_val_labels = stratify_labels\n",
    "        test_data = []\n",
    "\n",
    "    # --- Second Split: Separate Validation Set from Train/Val Pool (if val_size > 0) ---\n",
    "    if val_size > 0 and len(train_val_data) > 0:\n",
    "        # Adjust val_size relative to the size of the train/val pool\n",
    "        if test_size > 0: # Need to adjust val_size because test set was removed\n",
    "             relative_val_size = val_size / (1.0 - test_size)\n",
    "        else: # No test set removed, val_size is already relative to the whole pool\n",
    "             relative_val_size = val_size\n",
    "\n",
    "        # Ensure relative_val_size is valid and meaningful\n",
    "        if relative_val_size <= 0 or relative_val_size >= 1:\n",
    "            logger.warning(f\"Calculated relative validation size ({relative_val_size:.3f}) is invalid or zero. Assigning all remaining data to train set.\")\n",
    "            train_data = train_val_data\n",
    "            val_data = []\n",
    "        elif len(train_val_data) < 2: # Cannot split if only 1 sample left\n",
    "             logger.warning(f\"Only {len(train_val_data)} sample(s) left for train/val split. Assigning all to train set.\")\n",
    "             train_data = train_val_data\n",
    "             val_data = []\n",
    "        else:\n",
    "            try:\n",
    "                logger.info(f\"Splitting off validation set ({relative_val_size * 100:.1f}% of remaining)...\")\n",
    "                train_indices, val_indices = train_test_split(\n",
    "                    range(len(train_val_data)),\n",
    "                    test_size=relative_val_size,\n",
    "                    random_state=random_state,\n",
    "                    stratify=train_val_labels\n",
    "                )\n",
    "                train_data = [train_val_data[i] for i in train_indices]\n",
    "                val_data = [train_val_data[i] for i in val_indices]\n",
    "                logger.info(f\"Split complete: Train = {len(train_data)}, Validation = {len(val_data)}\")\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"Stratified split for validation set failed: {e}. Falling back to non-stratified split for validation set.\")\n",
    "                # Check again if we have enough samples for non-stratified split\n",
    "                if len(train_val_data) >= 2:\n",
    "                    train_indices, val_indices = train_test_split(\n",
    "                        range(len(train_val_data)),\n",
    "                        test_size=relative_val_size,\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "                    train_data = [train_val_data[i] for i in train_indices]\n",
    "                    val_data = [train_val_data[i] for i in val_indices]\n",
    "                else: # Should not happen based on earlier check, but safeguard\n",
    "                    logger.warning(\"Cannot perform non-stratified split with < 2 samples. Assigning all to train.\")\n",
    "                    train_data = train_val_data\n",
    "                    val_data = []\n",
    "\n",
    "    else: # No validation set needed or train/val pool is empty\n",
    "        logger.info(\"No validation set requested (val_size=0) or train/val pool empty. Assigning remaining data to train set.\")\n",
    "        train_data = train_val_data # Assign whatever is left (could be empty)\n",
    "        val_data = []\n",
    "\n",
    "    # Final check on sizes\n",
    "    logger.info(f\"Final split sizes: Train={len(train_data)}, Validation={len(val_data)}, Test={len(test_data)}\")\n",
    "    if len(train_data) + len(val_data) + len(test_data) != n_samples:\n",
    "        logger.warning(\"Total samples after split do not match initial count. Check logic.\")\n",
    "\n",
    "    # Sanity check: Ensure no overlap between sets based on IDs\n",
    "    train_ids = {d['image_id'] for d in train_data}\n",
    "    val_ids = {d['image_id'] for d in val_data}\n",
    "    test_ids = {d['image_id'] for d in test_data}\n",
    "    if train_ids.intersection(val_ids): logger.error(\"Overlap detected between Train and Validation sets!\")\n",
    "    if train_ids.intersection(test_ids): logger.error(\"Overlap detected between Train and Test sets!\")\n",
    "    if val_ids.intersection(test_ids): logger.error(\"Overlap detected between Validation and Test sets!\")\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 6: RAG Components (Text-Based Embeddings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles generation of text embeddings using SentenceTransformer.\"\"\"\n",
    "    def __init__(self, model_name: str = TEXT_EMBEDDING_MODEL, device: Optional[str] = None):\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Initializing SentenceTransformer model: {model_name} on device: {self.device}\")\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name, device=self.device)\n",
    "            # Test encoding to get embedding dimension\n",
    "            test_emb = self.model.encode([\"test sentence\"])\n",
    "            self.embedding_dim = test_emb.shape[1]\n",
    "            logger.info(f\"SentenceTransformer model loaded successfully. Embedding dimension: {self.embedding_dim}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load SentenceTransformer model '{model_name}': {e}\", exc_info=True)\n",
    "            raise # Re-raise the exception to halt execution if model loading fails\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> Optional[np.ndarray]:\n",
    "        \"\"\"Generates embeddings for a list of texts.\"\"\"\n",
    "        if not texts:\n",
    "            logger.warning(\"Received empty list of texts for embedding generation.\")\n",
    "            return None\n",
    "        logger.info(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        try:\n",
    "            # Use tqdm for progress bar if list is large\n",
    "            show_progress = len(texts) > 1000\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=show_progress,\n",
    "                convert_to_numpy=True,\n",
    "                device=self.device\n",
    "            )\n",
    "            logger.info(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during embedding generation: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def generate_fused_embeddings(self, ocr_texts: List[str], triples_texts: List[str], batch_size: int = 32) -> Optional[np.ndarray]:\n",
    "        \"\"\"Generates embeddings for OCR and triples, then fuses them (e.g., concatenation).\"\"\"\n",
    "        logger.info(\"Generating fused text embeddings (OCR + Triples)...\")\n",
    "        if len(ocr_texts) != len(triples_texts):\n",
    "            logger.error(f\"Length mismatch between OCR texts ({len(ocr_texts)}) and Triples texts ({len(triples_texts)}). Cannot fuse.\")\n",
    "            return None\n",
    "\n",
    "        # Generate embeddings for OCR and Triples separately\n",
    "        # Handle cases where one or both might be empty lists correctly\n",
    "        ocr_embeddings = self.generate_embeddings(ocr_texts, batch_size=batch_size) if ocr_texts else None\n",
    "        triples_embeddings = self.generate_embeddings(triples_texts, batch_size=batch_size) if triples_texts else None\n",
    "\n",
    "        # Check if generation was successful\n",
    "        if ocr_embeddings is None and triples_embeddings is None:\n",
    "             logger.error(\"Both OCR and Triples embedding generation failed or produced None.\")\n",
    "             return None\n",
    "\n",
    "        # If one is missing, use zeros of the correct dimension\n",
    "        num_samples = len(ocr_texts) # Should be same as len(triples_texts)\n",
    "        if ocr_embeddings is None:\n",
    "             logger.warning(\"OCR embedding generation failed or list was empty. Using zero vectors.\")\n",
    "             ocr_embeddings = np.zeros((num_samples, self.embedding_dim), dtype=np.float32)\n",
    "        if triples_embeddings is None:\n",
    "             logger.warning(\"Triples embedding generation failed or list was empty. Using zero vectors.\")\n",
    "             triples_embeddings = np.zeros((num_samples, self.embedding_dim), dtype=np.float32)\n",
    "\n",
    "        # Normalize embeddings before concatenation (optional, but often good practice)\n",
    "        # Add small epsilon to avoid division by zero for zero vectors\n",
    "        epsilon = 1e-12\n",
    "        ocr_norm = np.linalg.norm(ocr_embeddings, axis=1, keepdims=True)\n",
    "        triples_norm = np.linalg.norm(triples_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "        ocr_normalized = np.divide(ocr_embeddings, ocr_norm + epsilon, out=np.zeros_like(ocr_embeddings), where=(ocr_norm + epsilon)!=0)\n",
    "        triples_normalized = np.divide(triples_embeddings, triples_norm + epsilon, out=np.zeros_like(triples_embeddings), where=(triples_norm + epsilon)!=0)\n",
    "\n",
    "\n",
    "        # Simple concatenation for fusion\n",
    "        fused_embeddings = np.concatenate([ocr_normalized, triples_normalized], axis=1)\n",
    "        logger.info(f\"Generated fused embeddings with shape: {fused_embeddings.shape}\")\n",
    "\n",
    "        # Clean up intermediate arrays\n",
    "        del ocr_embeddings, triples_embeddings, ocr_norm, triples_norm, ocr_normalized, triples_normalized\n",
    "        gc.collect()\n",
    "\n",
    "        return fused_embeddings\n",
    "\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles building and querying a FAISS index for text similarity retrieval.\"\"\"\n",
    "    def __init__(self, embeddings: Optional[np.ndarray], top_k: int = RETRIEVAL_K):\n",
    "        self.top_k = top_k\n",
    "        self.index = None\n",
    "        self.dimension = 0\n",
    "\n",
    "        if embeddings is not None and embeddings.size > 0:\n",
    "            # Ensure embeddings are float32 for FAISS\n",
    "            if embeddings.dtype != np.float32:\n",
    "                 logger.warning(f\"Embeddings dtype is {embeddings.dtype}, converting to float32 for FAISS.\")\n",
    "                 embeddings = embeddings.astype(np.float32)\n",
    "            self.build_index(embeddings)\n",
    "        else:\n",
    "            logger.warning(\"RAGRetriever initialized with no embeddings. Retrieval will not be possible.\")\n",
    "\n",
    "    def build_index(self, embeddings: np.ndarray):\n",
    "        \"\"\"Builds a FAISS index from the provided embeddings.\"\"\"\n",
    "        if embeddings is None or embeddings.shape[0] == 0:\n",
    "            logger.error(\"Cannot build FAISS index from empty or None embeddings.\")\n",
    "            return\n",
    "        if embeddings.ndim != 2:\n",
    "             logger.error(f\"Embeddings must be 2D (samples, dimension), but got shape {embeddings.shape}. Cannot build index.\")\n",
    "             return\n",
    "\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        n_samples = embeddings.shape[0]\n",
    "        logger.info(f\"Building FAISS IndexFlatL2 with dimension {self.dimension} for {n_samples} vectors.\")\n",
    "\n",
    "        try:\n",
    "            self.index = faiss.IndexFlatL2(self.dimension) # Using L2 distance (Euclidean)\n",
    "            self.index.add(embeddings)\n",
    "            logger.info(f\"FAISS index built successfully. Index size: {self.index.ntotal} vectors.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building FAISS index: {e}\", exc_info=True)\n",
    "            self.index = None # Ensure index is None if building failed\n",
    "\n",
    "    def retrieve_similar(self, query_embeddings: np.ndarray) -> Optional[np.ndarray]:\n",
    "        \"\"\"Retrieves indices of the top_k most similar items from the index.\"\"\"\n",
    "        if self.index is None:\n",
    "            logger.error(\"FAISS index is not built. Cannot retrieve similar items.\")\n",
    "            return None\n",
    "        if query_embeddings is None or query_embeddings.size == 0:\n",
    "            logger.warning(\"Received empty or None query embeddings for retrieval.\")\n",
    "            return None # Return None for empty query\n",
    "\n",
    "        # Ensure query is float32 and 2D\n",
    "        if query_embeddings.dtype != np.float32:\n",
    "             query_embeddings = query_embeddings.astype(np.float32)\n",
    "        if query_embeddings.ndim == 1:\n",
    "             query_embeddings = np.expand_dims(query_embeddings, axis=0) # Reshape (D,) to (1, D)\n",
    "\n",
    "        if query_embeddings.shape[1] != self.dimension:\n",
    "            logger.error(f\"Query embedding dimension ({query_embeddings.shape[1]}) does not match index dimension ({self.dimension}).\")\n",
    "            return None\n",
    "\n",
    "        # Determine number of neighbors to search for (k)\n",
    "        # We search for top_k + 1 because the query item itself might be in the index\n",
    "        # and we typically want to exclude self-retrieval for prompt construction.\n",
    "        k = min(self.top_k + 1, self.index.ntotal) # Cannot retrieve more neighbors than exist in index\n",
    "        if k == 0:\n",
    "            logger.warning(\"FAISS index is empty (ntotal=0). Cannot retrieve.\")\n",
    "            # Return an empty array structure consistent with multiple queries\n",
    "            return np.array([[] for _ in range(query_embeddings.shape[0])], dtype=int)\n",
    "\n",
    "\n",
    "        logger.info(f\"Searching for top {k} neighbors for {query_embeddings.shape[0]} query vectors...\")\n",
    "        try:\n",
    "            # search returns distances (D) and indices (I)\n",
    "            distances, indices = self.index.search(query_embeddings, k=k)\n",
    "            logger.info(f\"FAISS search completed. Found indices shape: {indices.shape}\")\n",
    "\n",
    "            # Optional: Exclude self-retrieval if necessary (depends on whether queries are from the indexed data)\n",
    "            # This requires knowing the original indices of the query embeddings if they are part of the training data.\n",
    "            # For simplicity here, we return the raw indices including potential self.\n",
    "            # The PromptConstructor might handle skipping the first result if needed.\n",
    "\n",
    "            return indices # Shape: (num_queries, k)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during FAISS search: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "\n",
    "class PromptConstructor:\n",
    "    \"\"\"Constructs prompts for the classification model, optionally including RAG examples.\"\"\"\n",
    "    def __init__(self, train_data: List[Dict], label_encoder: LabelEncoder):\n",
    "        self.train_data = train_data\n",
    "        self.label_encoder = label_encoder\n",
    "        try:\n",
    "             self.class_names = \", \".join(label_encoder.classes_)\n",
    "             logger.info(f\"PromptConstructor initialized with {len(train_data)} training examples. Target classes: {self.class_names}\")\n",
    "        except AttributeError:\n",
    "             logger.error(\"LabelEncoder does not seem to be fitted yet (no classes_ attribute).\")\n",
    "             self.class_names = \"ERROR_CLASSES_UNDEFINED\"\n",
    "\n",
    "\n",
    "    def construct_prompt(self, sample: Dict, retrieved_indices: Optional[List[int]] = None) -> str:\n",
    "        \"\"\"Constructs a classification prompt for a given sample, optionally adding similar examples.\"\"\"\n",
    "\n",
    "        # --- System/Task Instruction ---\n",
    "        system_instruction = (\n",
    "            f\"Perform { 'multilabel' if isinstance(sample.get('original_labels', None), list) else 'multiclass' } \"\n",
    "            f\"classification for the given meme's text and knowledge graph context.\\n\"\n",
    "            f\"Choose the most relevant category/categories from the following list: {self.class_names}.\\n\"\n",
    "            f\"{ 'Output all applicable labels separated by commas if multiple apply.' if isinstance(sample.get('original_labels', None), list) else 'Output only the single most likely label.' }\\n\\n\"\n",
    "        )\n",
    "        prompt = system_instruction\n",
    "\n",
    "        # --- Few-Shot Examples (RAG) ---\n",
    "        if retrieved_indices:\n",
    "            prompt += \"Here are some potentially similar examples:\\n\\n\"\n",
    "            num_added = 0\n",
    "            for idx in retrieved_indices:\n",
    "                # Ensure index is valid and avoid retrieving the sample itself (if applicable - assumes indices match train_data)\n",
    "                # Simple check: if the ID matches, skip. Assumes 'sample_id' or 'id' exists.\n",
    "                current_id = sample.get('sample_id', sample.get('id'))\n",
    "                try:\n",
    "                    retrieved_sample = self.train_data[idx]\n",
    "                    retrieved_id = retrieved_sample.get('sample_id', retrieved_sample.get('id'))\n",
    "\n",
    "                    if current_id is not None and current_id == retrieved_id:\n",
    "                        logger.debug(f\"Skipping self-retrieval for index {idx} (ID: {current_id})\")\n",
    "                        continue # Skip self\n",
    "\n",
    "                    ex_text = retrieved_sample.get(\"ocr_text\", \"N/A\")\n",
    "                    ex_triples = retrieved_sample.get(\"triples\", \"\") # Cleaned triples should be here\n",
    "                    ex_labels = retrieved_sample.get(\"original_labels\", \"N/A\")\n",
    "\n",
    "                    # Format labels nicely\n",
    "                    if isinstance(ex_labels, list):\n",
    "                        ex_label_str = \", \".join(ex_labels) if ex_labels else \"None\"\n",
    "                    else: # Should be single string for anxiety\n",
    "                        ex_label_str = str(ex_labels) if ex_labels is not None else \"N/A\"\n",
    "\n",
    "                    prompt += f\"--- Example {num_added + 1} ---\\n\"\n",
    "                    prompt += f\"Example Text: {ex_text}\\n\"\n",
    "                    if ex_triples:\n",
    "                        prompt += f\"Example Knowledge:\\n{ex_triples}\\n\" # Keep newline for readability\n",
    "                    prompt += f\"Example Category: {ex_label_str}\\n\\n\"\n",
    "                    num_added += 1\n",
    "                    if num_added >= RETRIEVAL_K: # Limit to K examples even if more retrieved\n",
    "                         break\n",
    "\n",
    "                except IndexError:\n",
    "                    logger.warning(f\"Retrieved index {idx} is out of bounds for train_data (size {len(self.train_data)}).\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing retrieved example at index {idx}: {e}\")\n",
    "\n",
    "            if num_added > 0:\n",
    "                 prompt += \"--- End of Examples ---\\n\\n\"\n",
    "            else:\n",
    "                 prompt += \"No similar examples found or provided.\\n\\n\"\n",
    "\n",
    "\n",
    "        # --- Current Sample to Classify ---\n",
    "        prompt += \"Now, classify the following meme:\\n\\n\"\n",
    "        current_text = sample.get('ocr_text', 'N/A')\n",
    "        current_triples = sample.get('triples', '') # Assumes triples are already cleaned\n",
    "\n",
    "        prompt += f\"Text: {current_text}\\n\"\n",
    "        if current_triples:\n",
    "            prompt += f\"Knowledge:\\n{current_triples}\\n\" # Keep newline\n",
    "        prompt += \"Category:\" # Model should predict what comes after this\n",
    "\n",
    "        return prompt.strip() # Remove any trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 7: Dataset Classes (Handles Image Features)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AnxietyDataset(Dataset):\n",
    "    \"\"\"Dataset for single-label Anxiety classification (multimodal).\"\"\"\n",
    "    def __init__(self,\n",
    "                 samples: List[Dict],\n",
    "                 prompts: List[str],\n",
    "                 tokenizer: BartTokenizer,\n",
    "                 max_len: int,\n",
    "                 label_encoder: LabelEncoder,\n",
    "                 image_features_map: Dict[str, torch.Tensor],\n",
    "                 visual_feature_dim: int = VISUAL_FEATURE_DIM):\n",
    "        self.samples = samples\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_encoder = label_encoder\n",
    "        self.image_features_map = image_features_map\n",
    "        self.visual_feature_dim = visual_feature_dim # Store expected dim\n",
    "        self.default_image_feature = torch.zeros(self.visual_feature_dim, dtype=torch.float) # Pre-create zero tensor\n",
    "\n",
    "        logger.info(f\"AnxietyDataset created with {len(samples)} samples.\")\n",
    "        if len(samples) != len(prompts):\n",
    "            logger.warning(f\"Mismatch between number of samples ({len(samples)}) and prompts ({len(prompts)}).\")\n",
    "\n",
    "        # Pre-verify image features for a few samples (optional)\n",
    "        missing_count = 0\n",
    "        for i in range(min(5, len(samples))):\n",
    "            sample_id = samples[i].get('image_id')\n",
    "            if sample_id not in image_features_map:\n",
    "                 missing_count += 1\n",
    "        if missing_count > 0:\n",
    "             logger.warning(f\"In first 5 samples, {missing_count} are missing pre-computed image features.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        prompt = self.prompts[idx]\n",
    "        label_str = sample.get(\"original_labels\", None) # Expecting single string label\n",
    "        image_id = sample.get(\"image_id\", None) # Get the image identifier\n",
    "\n",
    "        # Encode label\n",
    "        label_idx = -1 # Use -1 as placeholder for missing/unknown\n",
    "        if label_str is not None:\n",
    "            try:\n",
    "                label_idx = self.label_encoder.transform([label_str])[0]\n",
    "            except ValueError:\n",
    "                logger.error(f\"Label '{label_str}' in sample {idx} (ID: {sample.get('id', 'N/A')}) not found in LabelEncoder classes: {self.label_encoder.classes_}. Assigning index 0 (or handle differently).\")\n",
    "                # Decide how to handle unknown labels during training/eval\n",
    "                # Option 1: Assign a default index (e.g., 0)\n",
    "                label_idx = 0 # Or find an 'unknown' class index if available\n",
    "                # Option 2: Raise an error\n",
    "                # raise ValueError(f\"Unknown label encountered: {label_str}\")\n",
    "                # Option 3: Skip the sample (would require changes in DataLoader/training loop)\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Error encoding label '{label_str}' for sample {idx}: {e}\")\n",
    "                 label_idx = 0 # Fallback to 0\n",
    "\n",
    "        # Get image features\n",
    "        if image_id and image_id in self.image_features_map:\n",
    "            img_features = self.image_features_map[image_id]\n",
    "            # Ensure the loaded feature has the correct dimension\n",
    "            if img_features.shape[0] != self.visual_feature_dim:\n",
    "                 logger.warning(f\"Image feature for ID {image_id} has incorrect dimension {img_features.shape[0]}, expected {self.visual_feature_dim}. Using zeros.\")\n",
    "                 img_features = self.default_image_feature\n",
    "            # Ensure dtype is float\n",
    "            if img_features.dtype != torch.float:\n",
    "                 img_features = img_features.float()\n",
    "        else:\n",
    "            logger.debug(f\"Image features missing for image_id '{image_id}' in sample {idx}. Using default zero vector.\")\n",
    "            img_features = self.default_image_feature\n",
    "\n",
    "\n",
    "        # Tokenize prompt\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                prompt,\n",
    "                max_length=self.max_len,\n",
    "                padding=\"max_length\", # Pad to max_len\n",
    "                truncation=True,      # Truncate if longer\n",
    "                return_tensors=\"pt\"   # Return PyTorch tensors\n",
    "            )\n",
    "            # Squeeze to remove the batch dimension added by tokenizer\n",
    "            input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error tokenizing prompt for sample {idx} (ID: {sample.get('id', 'N/A')}): {e}\", exc_info=True)\n",
    "             # Return dummy data or raise error\n",
    "             input_ids = torch.zeros(self.max_len, dtype=torch.long)\n",
    "             attention_mask = torch.zeros(self.max_len, dtype=torch.long)\n",
    "             # Keep label_idx and img_features as potentially valid\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"image_features\": img_features, # Return the (potentially zero) image features\n",
    "            \"label\": torch.tensor(label_idx, dtype=torch.long) # CrossEntropyLoss expects long indices\n",
    "        }\n",
    "\n",
    "\n",
    "class DepressionDataset(Dataset):\n",
    "    \"\"\"Dataset for multi-label Depression classification (multimodal).\"\"\"\n",
    "    def __init__(self,\n",
    "                 samples: List[Dict],\n",
    "                 prompts: List[str],\n",
    "                 tokenizer: BartTokenizer,\n",
    "                 max_len: int,\n",
    "                 label_encoder: LabelEncoder,\n",
    "                 image_features_map: Dict[str, torch.Tensor],\n",
    "                 visual_feature_dim: int = VISUAL_FEATURE_DIM):\n",
    "        self.samples = samples\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_encoder = label_encoder\n",
    "        self.num_labels = len(label_encoder.classes_)\n",
    "        self.image_features_map = image_features_map\n",
    "        self.visual_feature_dim = visual_feature_dim\n",
    "        self.default_image_feature = torch.zeros(self.visual_feature_dim, dtype=torch.float)\n",
    "\n",
    "        logger.info(f\"DepressionDataset created with {len(samples)} samples and {self.num_labels} labels.\")\n",
    "        if len(samples) != len(prompts):\n",
    "            logger.warning(f\"Mismatch between number of samples ({len(samples)}) and prompts ({len(prompts)}).\")\n",
    "\n",
    "        # Pre-verify image features for a few samples (optional)\n",
    "        missing_count = 0\n",
    "        for i in range(min(5, len(samples))):\n",
    "            sample_id = samples[i].get('image_id')\n",
    "            if sample_id not in image_features_map:\n",
    "                 missing_count += 1\n",
    "        if missing_count > 0:\n",
    "             logger.warning(f\"In first 5 samples, {missing_count} are missing pre-computed image features.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        prompt = self.prompts[idx]\n",
    "        label_str_list = sample.get(\"original_labels\", []) # Expecting list of strings\n",
    "        image_id = sample.get(\"image_id\", None)\n",
    "\n",
    "        # Create multi-hot encoded label vector\n",
    "        multi_hot_label = torch.zeros(self.num_labels, dtype=torch.float) # Use float for BCEWithLogitsLoss\n",
    "        if not isinstance(label_str_list, list):\n",
    "             logger.warning(f\"Expected label_str_list to be a list for sample {idx}, but got {type(label_str_list)}. Treating as empty.\")\n",
    "             label_str_list = []\n",
    "\n",
    "        if not label_str_list:\n",
    "            logger.warning(f\"Sample {idx} (ID: {sample.get('id', 'N/A')}) has an empty label list.\")\n",
    "            # Decide how to handle samples with no labels (e.g., skip, assign 'Unknown')\n",
    "            # Here, we just leave the multi_hot_label as all zeros.\n",
    "        else:\n",
    "            for label_str in label_str_list:\n",
    "                try:\n",
    "                    label_idx = self.label_encoder.transform([label_str])[0]\n",
    "                    if 0 <= label_idx < self.num_labels:\n",
    "                        multi_hot_label[label_idx] = 1.0\n",
    "                    else:\n",
    "                         logger.error(f\"Label index {label_idx} out of bounds for label '{label_str}' in sample {idx}.\")\n",
    "                except ValueError:\n",
    "                    logger.error(f\"Label '{label_str}' in sample {idx} (ID: {sample.get('id', 'N/A')}) not found in LabelEncoder classes: {self.label_encoder.classes_}. Skipping this label.\")\n",
    "                except Exception as e:\n",
    "                     logger.error(f\"Error encoding label '{label_str}' for sample {idx}: {e}\")\n",
    "\n",
    "        # Get image features\n",
    "        if image_id and image_id in self.image_features_map:\n",
    "            img_features = self.image_features_map[image_id]\n",
    "            if img_features.shape[0] != self.visual_feature_dim:\n",
    "                 logger.warning(f\"Image feature for ID {image_id} has incorrect dimension {img_features.shape[0]}, expected {self.visual_feature_dim}. Using zeros.\")\n",
    "                 img_features = self.default_image_feature\n",
    "            if img_features.dtype != torch.float:\n",
    "                 img_features = img_features.float()\n",
    "        else:\n",
    "            logger.debug(f\"Image features missing for image_id '{image_id}' in sample {idx}. Using default zero vector.\")\n",
    "            img_features = self.default_image_feature\n",
    "\n",
    "        # Tokenize prompt\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                prompt,\n",
    "                max_length=self.max_len,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error tokenizing prompt for sample {idx} (ID: {sample.get('id', 'N/A')}): {e}\", exc_info=True)\n",
    "             input_ids = torch.zeros(self.max_len, dtype=torch.long)\n",
    "             attention_mask = torch.zeros(self.max_len, dtype=torch.long)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"image_features\": img_features, # Return the image features\n",
    "            \"label\": multi_hot_label # Return the multi-hot vector\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 8: Model Definition (Fusion Classifier)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 8: Model Definition (Fusion Classifier)\n",
    "\n",
    "class M3H_FusionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal Classifier fusing BART text features and CLIP visual features.\n",
    "\n",
    "    Uses the BART encoder's output (pooled) and projects visual features\n",
    "    to the same dimension. A gating mechanism then combines these features\n",
    "    before a final classification layer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_labels: int,\n",
    "                 pretrained_model_name: str = BASE_TEXT_MODEL_NAME,\n",
    "                 is_multilabel: bool = False,\n",
    "                 visual_feature_dim: int = VISUAL_FEATURE_DIM,\n",
    "                 dropout_prob: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.visual_feature_dim = visual_feature_dim\n",
    "\n",
    "        logger.info(f\"Initializing M3H_FusionClassifier:\")\n",
    "        logger.info(f\"  - Base Text Model: {pretrained_model_name}\")\n",
    "        logger.info(f\"  - Task Type: {'Multi-Label' if is_multilabel else 'Single-Label (Multiclass)'}\")\n",
    "        logger.info(f\"  - Number of Labels: {num_labels}\")\n",
    "        logger.info(f\"  - Visual Feature Dimension (Input): {visual_feature_dim}\")\n",
    "        logger.info(f\"  - Dropout Probability: {dropout_prob}\")\n",
    "\n",
    "        try:\n",
    "            # Load the base BART model (we only need the encoder usually, but BartModel gives easy access)\n",
    "            self.bart = BartModel.from_pretrained(pretrained_model_name)\n",
    "            self.text_feature_dim = self.bart.config.hidden_size # Dimension of BART's output embeddings\n",
    "            logger.info(f\"  - BART Hidden Dimension (Text Feature Dim): {self.text_feature_dim}\")\n",
    "\n",
    "            # Projection layer for visual features if dimensions don't match BART's hidden size\n",
    "            if self.visual_feature_dim != self.text_feature_dim:\n",
    "                logger.info(f\"  - Adding projection layer for visual features: {self.visual_feature_dim} -> {self.text_feature_dim}\")\n",
    "                self.visual_projection = nn.Linear(self.visual_feature_dim, self.text_feature_dim)\n",
    "            else:\n",
    "                logger.info(\"  - Visual feature dimension matches BART hidden size. Using Identity projection.\")\n",
    "                self.visual_projection = nn.Identity() # No projection needed if dims match\n",
    "\n",
    "            # Gating mechanism: Learn a weight for combining text and visual features\n",
    "            # Input dimension is doubled because we concatenate text and projected visual features\n",
    "            self.gate_layer = nn.Linear(self.text_feature_dim * 2, self.text_feature_dim)\n",
    "\n",
    "            # Fusion dropout\n",
    "            self.fusion_dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "            # Final classification layer\n",
    "            self.classifier = nn.Linear(self.text_feature_dim, num_labels)\n",
    "\n",
    "        except OSError as e:\n",
    "             logger.error(f\"Could not find/load pretrained BART model '{pretrained_model_name}'. Check model name and internet connection. Error: {e}\", exc_info=True)\n",
    "             raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing model layers: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        # Define the loss function based on the task type\n",
    "        if self.is_multilabel:\n",
    "            self.loss_fct = nn.BCEWithLogitsLoss() # Suitable for multi-label classification\n",
    "            logger.info(\"  - Using BCEWithLogitsLoss for multi-label task.\")\n",
    "        else:\n",
    "            self.loss_fct = nn.CrossEntropyLoss() # Suitable for single-label (multiclass) classification\n",
    "            logger.info(\"  - Using CrossEntropyLoss for single-label task.\")\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                image_features: torch.Tensor, # Add image_features as input\n",
    "                labels: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        Forward pass of the fusion model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Batch of input token IDs. Shape: (batch_size, seq_len)\n",
    "            attention_mask (torch.Tensor): Batch of attention masks. Shape: (batch_size, seq_len)\n",
    "            image_features (torch.Tensor): Batch of image features. Shape: (batch_size, visual_feature_dim)\n",
    "            labels (Optional[torch.Tensor]): Batch of labels.\n",
    "                - Shape: (batch_size,) for single-label (long)\n",
    "                - Shape: (batch_size, num_labels) for multi-label (float)\n",
    "\n",
    "        Returns:\n",
    "            A simple object with 'loss' (if labels provided) and 'logits'.\n",
    "        \"\"\"\n",
    "        # 1. Get Text Features from BART Encoder\n",
    "        # We only need the encoder's output. `last_hidden_state` has shape (batch_size, seq_len, hidden_size)\n",
    "        # We typically use the output of the first token ([CLS] equivalent for BART) as the pooled representation.\n",
    "        encoder_outputs = self.bart.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # Use the hidden state of the first token (index 0)\n",
    "        # Shape: (batch_size, hidden_size)\n",
    "        bart_pooled_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # 2. Project Visual Features\n",
    "        # Shape: (batch_size, visual_feature_dim) -> (batch_size, text_feature_dim)\n",
    "        projected_visual_features = self.visual_projection(image_features)\n",
    "\n",
    "        # 3. Fuse Features using Gating\n",
    "        # Concatenate text and visual features along the feature dimension\n",
    "        combined_features = torch.cat((bart_pooled_output, projected_visual_features), dim=1) # Shape: (batch_size, text_feature_dim * 2)\n",
    "\n",
    "        # Calculate the gate weights (sigmoid ensures values between 0 and 1)\n",
    "        gate_values = torch.sigmoid(self.gate_layer(combined_features)) # Shape: (batch_size, text_feature_dim)\n",
    "\n",
    "        # Apply the gate: weighted sum of text and visual features\n",
    "        # fused = gate * text + (1 - gate) * visual\n",
    "        fused_features = gate_values * bart_pooled_output + (1 - gate_values) * projected_visual_features # Shape: (batch_size, text_feature_dim)\n",
    "\n",
    "        # Apply dropout to the fused representation\n",
    "        fused_features_dropout = self.fusion_dropout(fused_features)\n",
    "\n",
    "        # 4. Classification\n",
    "        # Get logits from the classifier\n",
    "        logits = self.classifier(fused_features_dropout) # Shape: (batch_size, num_labels)\n",
    "\n",
    "        # 5. Calculate Loss (if labels are provided)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.is_multilabel:\n",
    "                # BCEWithLogitsLoss expects logits and float labels (0.0 or 1.0)\n",
    "                loss = self.loss_fct(logits, labels.float())\n",
    "            else:\n",
    "                # CrossEntropyLoss expects logits and long labels (class indices)\n",
    "                loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # Return results in a simple structure similar to Hugging Face model outputs\n",
    "        # Avoid returning complex objects if not necessary for compatibility\n",
    "        class SimpleOutput:\n",
    "             def __init__(self):\n",
    "                 self.loss = None\n",
    "                 self.logits = None\n",
    "\n",
    "        output = SimpleOutput()\n",
    "        output.loss = loss\n",
    "        output.logits = logits\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 9: Training Function (Trains ONE Model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 9: Training Function (Trains ONE Model)\n",
    "\n",
    "def train_and_evaluate(\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    output_dir: str, # Directory for saving models and logs for this specific run\n",
    "    label_encoder: LabelEncoder,\n",
    "    is_multilabel: bool\n",
    ") -> Tuple[Dict[str, List], str]:\n",
    "    \"\"\"Trains and evaluates a single model instance.\"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Starting training run. Output directory: {output_dir}\")\n",
    "    logger.info(f\"Task Type: {'Multi-Label' if is_multilabel else 'Single-Label (Multiclass)'}\")\n",
    "\n",
    "    best_val_metric = 0.0 # Use Macro F1 for single-label, Micro F1 for multi-label? Or choose one consistently.\n",
    "    best_val_f1_macro = 0.0 # Track best macro F1 specifically for model saving\n",
    "    best_model_path = os.path.join(output_dir, \"best_model_macro_f1.pt\")\n",
    "    last_model_path = os.path.join(output_dir, \"last_model.pt\")\n",
    "    log_file = os.path.join(output_dir, \"training_log.csv\")\n",
    "    training_logs = [] # Store logs for saving to CSV\n",
    "    history = defaultdict(list) # Store metrics history\n",
    "\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    logger.info(f\"Total training steps: {total_steps} ({len(train_dataloader)} steps/epoch * {num_epochs} epochs)\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_num = epoch + 1\n",
    "            logger.info(f\"--- Epoch {epoch_num}/{num_epochs} ---\")\n",
    "\n",
    "            # --- Training Phase ---\n",
    "            model.train() # Set model to training mode\n",
    "            total_train_loss = 0.0\n",
    "            all_train_logits = []\n",
    "            all_train_labels_raw = [] # Store raw labels from dataloader\n",
    "\n",
    "            train_progress = tqdm(train_dataloader, desc=f\"Train Epoch {epoch_num}\", leave=False)\n",
    "            for batch_idx, batch in enumerate(train_progress):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Move batch to device\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                image_features = batch[\"image_features\"].to(device) # Get image features\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                try:\n",
    "                    # Forward pass\n",
    "                    outputs = model(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    image_features=image_features, # Pass image features\n",
    "                                    labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    if loss is None:\n",
    "                        logger.error(f\"Epoch {epoch_num}, Batch {batch_idx}: Loss is None. Check model forward pass and loss calculation.\")\n",
    "                        continue # Skip batch if loss is None\n",
    "\n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    # Gradient clipping (optional but recommended)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step() # Update learning rate\n",
    "\n",
    "                    total_train_loss += loss.item()\n",
    "\n",
    "                    # Store logits and labels for epoch metrics calculation\n",
    "                    all_train_logits.append(logits.detach().cpu())\n",
    "                    all_train_labels_raw.append(labels.cpu())\n",
    "\n",
    "                    # Update progress bar\n",
    "                    train_progress.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during training batch {batch_idx} in epoch {epoch_num}: {e}\", exc_info=True)\n",
    "                    # Decide whether to continue or stop training\n",
    "                    # For now, just log and continue to the next batch\n",
    "                    continue\n",
    "\n",
    "            # Calculate average training loss for the epoch\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0.0\n",
    "\n",
    "            # Calculate training metrics for the epoch\n",
    "            if not all_train_logits or not all_train_labels_raw:\n",
    "                 logger.warning(f\"Epoch {epoch_num}: No training logits or labels collected. Skipping training metrics calculation.\")\n",
    "                 train_f1_micro, train_f1_macro, train_f1_weighted = 0.0, 0.0, 0.0\n",
    "            else:\n",
    "                all_train_logits_cat = torch.cat(all_train_logits, dim=0)\n",
    "                all_train_labels_raw_cat = torch.cat(all_train_labels_raw, dim=0)\n",
    "\n",
    "                if is_multilabel:\n",
    "                    train_probs = torch.sigmoid(all_train_logits_cat).numpy()\n",
    "                    train_preds = (train_probs > 0.5).astype(int)\n",
    "                    train_labels = all_train_labels_raw_cat.numpy().astype(int)\n",
    "                    train_f1_micro = f1_score(train_labels, train_preds, average=\"micro\", zero_division=0)\n",
    "                    train_f1_macro = f1_score(train_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "                    train_f1_weighted = f1_score(train_labels, train_preds, average=\"weighted\", zero_division=0)\n",
    "                    train_accuracy = accuracy_score(train_labels, train_preds) # Subset accuracy\n",
    "                    train_hamming = hamming_loss(train_labels, train_preds)\n",
    "                    logger.info(f\"Epoch {epoch_num} Train - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}, Hamming: {train_hamming:.4f}, MicroF1: {train_f1_micro:.4f}, MacroF1: {train_f1_macro:.4f}\")\n",
    "                else: # Single-label\n",
    "                    train_preds = torch.argmax(all_train_logits_cat, dim=1).numpy()\n",
    "                    train_labels = all_train_labels_raw_cat.numpy()\n",
    "                    train_f1_micro = f1_score(train_labels, train_preds, average=\"micro\", zero_division=0)\n",
    "                    train_f1_macro = f1_score(train_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "                    train_f1_weighted = f1_score(train_labels, train_preds, average=\"weighted\", zero_division=0)\n",
    "                    train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "                    train_hamming = 0.0 # Not typically used for single-label multiclass\n",
    "                    logger.info(f\"Epoch {epoch_num} Train - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}, MacroF1: {train_f1_macro:.4f}, WeightedF1: {train_f1_weighted:.4f}\")\n",
    "\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            total_val_loss = 0.0\n",
    "            all_val_logits = []\n",
    "            all_val_labels_raw = []\n",
    "\n",
    "            val_progress = tqdm(val_dataloader, desc=f\"Val Epoch {epoch_num}\", leave=False)\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for batch in val_progress:\n",
    "                    # Move batch to device\n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                    image_features = batch[\"image_features\"].to(device) # Get image features\n",
    "                    labels = batch[\"label\"].to(device)\n",
    "\n",
    "                    try:\n",
    "                        # Forward pass\n",
    "                        outputs = model(input_ids=input_ids,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        image_features=image_features, # Pass image features\n",
    "                                        labels=labels)\n",
    "                        loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "\n",
    "                        if loss is not None:\n",
    "                            total_val_loss += loss.item()\n",
    "\n",
    "                        # Store logits and labels for epoch metrics calculation\n",
    "                        all_val_logits.append(logits.cpu())\n",
    "                        all_val_labels_raw.append(labels.cpu())\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error during validation batch in epoch {epoch_num}: {e}\", exc_info=True)\n",
    "                        continue\n",
    "\n",
    "            # Calculate average validation loss for the epoch\n",
    "            avg_val_loss = total_val_loss / len(val_dataloader) if len(val_dataloader) > 0 else 0.0\n",
    "\n",
    "            # Calculate validation metrics for the epoch\n",
    "            if not all_val_logits or not all_val_labels_raw:\n",
    "                 logger.warning(f\"Epoch {epoch_num}: No validation logits or labels collected. Skipping validation metrics calculation.\")\n",
    "                 val_f1_micro, val_f1_macro, val_f1_weighted, val_accuracy, val_hamming = 0.0, 0.0, 0.0, 0.0, 1.0\n",
    "            else:\n",
    "                all_val_logits_cat = torch.cat(all_val_logits, dim=0)\n",
    "                all_val_labels_raw_cat = torch.cat(all_val_labels_raw, dim=0)\n",
    "\n",
    "                if is_multilabel:\n",
    "                    val_probs = torch.sigmoid(all_val_logits_cat).numpy()\n",
    "                    val_preds = (val_probs > 0.5).astype(int)\n",
    "                    val_labels = all_val_labels_raw_cat.numpy().astype(int)\n",
    "                    val_f1_micro = f1_score(val_labels, val_preds, average=\"micro\", zero_division=0)\n",
    "                    val_f1_macro = f1_score(val_labels, val_preds, average=\"macro\", zero_division=0)\n",
    "                    val_f1_weighted = f1_score(val_labels, val_preds, average=\"weighted\", zero_division=0)\n",
    "                    val_accuracy = accuracy_score(val_labels, val_preds) # Subset accuracy\n",
    "                    val_hamming = hamming_loss(val_labels, val_preds)\n",
    "                    logger.info(f\"Epoch {epoch_num} Val   - Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.4f}, Hamming: {val_hamming:.4f}, MicroF1: {val_f1_micro:.4f}, MacroF1: {val_f1_macro:.4f}\")\n",
    "                else: # Single-label\n",
    "                    val_preds = torch.argmax(all_val_logits_cat, dim=1).numpy()\n",
    "                    val_labels = all_val_labels_raw_cat.numpy()\n",
    "                    val_f1_micro = f1_score(val_labels, val_preds, average=\"micro\", zero_division=0)\n",
    "                    val_f1_macro = f1_score(val_labels, val_preds, average=\"macro\", zero_division=0)\n",
    "                    val_f1_weighted = f1_score(val_labels, val_preds, average=\"weighted\", zero_division=0)\n",
    "                    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "                    val_hamming = 0.0 # Not applicable\n",
    "                    logger.info(f\"Epoch {epoch_num} Val   - Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.4f}, MacroF1: {val_f1_macro:.4f}, WeightedF1: {val_f1_weighted:.4f}\")\n",
    "\n",
    "\n",
    "            # --- Logging and Saving ---\n",
    "            # Append metrics to history\n",
    "            history['epoch'].append(epoch_num)\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['train_f1_macro'].append(train_f1_macro)\n",
    "            history['val_f1_macro'].append(val_f1_macro)\n",
    "            history['train_f1_micro'].append(train_f1_micro)\n",
    "            history['val_f1_micro'].append(val_f1_micro)\n",
    "            history['train_f1_weighted'].append(train_f1_weighted)\n",
    "            history['val_f1_weighted'].append(val_f1_weighted)\n",
    "            history['train_accuracy'].append(train_accuracy)\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            history['train_hamming'].append(train_hamming)\n",
    "            history['val_hamming'].append(val_hamming)\n",
    "            history['learning_rate'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "            # Store log entry for CSV\n",
    "            current_log = {\n",
    "                'epoch': epoch_num,\n",
    "                'train_loss': avg_train_loss, 'val_loss': avg_val_loss,\n",
    "                'train_f1_macro': train_f1_macro, 'val_f1_macro': val_f1_macro,\n",
    "                'train_f1_micro': train_f1_micro, 'val_f1_micro': val_f1_micro,\n",
    "                'train_f1_weighted': train_f1_weighted, 'val_f1_weighted': val_f1_weighted,\n",
    "                'train_accuracy': train_accuracy, 'val_accuracy': val_accuracy,\n",
    "                'train_hamming': train_hamming, 'val_hamming': val_hamming,\n",
    "                'learning_rate': scheduler.get_last_lr()[0]\n",
    "            }\n",
    "            training_logs.append(current_log)\n",
    "\n",
    "            # Save the best model based on validation macro F1\n",
    "            if val_f1_macro > best_val_f1_macro:\n",
    "                best_val_f1_macro = val_f1_macro\n",
    "                logger.info(f\"Epoch {epoch_num}: New best validation Macro F1: {best_val_f1_macro:.4f}. Saving model to {best_model_path}\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                 logger.info(f\"Epoch {epoch_num}: Validation Macro F1 ({val_f1_macro:.4f}) did not improve from best ({best_val_f1_macro:.4f}).\")\n",
    "\n",
    "\n",
    "            # Save the model from the last epoch\n",
    "            torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "            # Save logs to CSV\n",
    "            pd.DataFrame(training_logs).to_csv(log_file, index=False)\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del all_train_logits, all_train_labels_raw, all_val_logits, all_val_labels_raw\n",
    "            del all_train_logits_cat, all_train_labels_raw_cat # Explicitly delete concatenated tensors\n",
    "            if 'all_val_logits_cat' in locals(): del all_val_logits_cat\n",
    "            if 'all_val_labels_raw_cat' in locals(): del all_val_labels_raw_cat\n",
    "            gc.collect()\n",
    "            if device == torch.device('cuda'):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        logger.info(f\"Training finished after {num_epochs} epochs.\")\n",
    "        logger.info(f\"Best Validation Macro F1 achieved: {best_val_f1_macro:.4f}\")\n",
    "        logger.info(f\"Best model saved to: {best_model_path}\")\n",
    "        logger.info(f\"Last model saved to: {last_model_path}\")\n",
    "        logger.info(f\"Training logs saved to: {log_file}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "         logger.warning(\"Training interrupted by user (KeyboardInterrupt).\")\n",
    "         # Save current state if interrupted\n",
    "         torch.save(model.state_dict(), last_model_path)\n",
    "         pd.DataFrame(training_logs).to_csv(log_file, index=False)\n",
    "         logger.info(f\"Saved last model state to {last_model_path} and logs to {log_file}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during the training loop: {e}\", exc_info=True)\n",
    "        # Save logs even if error occurs\n",
    "        pd.DataFrame(training_logs).to_csv(log_file, index=False)\n",
    "        raise # Re-raise the exception after logging\n",
    "\n",
    "    return history, best_model_path # Return history and path to the best model found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 10: Plotting Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history: Dict[str, List], output_dir: str, suffix: str = \"\"):\n",
    "    \"\"\"Plots training and validation metrics stored in the history dictionary.\"\"\"\n",
    "    if not history or not isinstance(history, dict):\n",
    "        logger.warning(\"Cannot plot history: Invalid or empty history dictionary provided.\")\n",
    "        return\n",
    "\n",
    "    # Check if essential keys exist and have data\n",
    "    required_keys = ['train_loss', 'val_loss', 'train_f1_macro', 'val_f1_macro']\n",
    "    if not all(key in history and history[key] for key in required_keys):\n",
    "         logger.warning(f\"History dict missing essential data ({required_keys}). Cannot generate plots.\")\n",
    "         # Log available keys for debugging:\n",
    "         logger.debug(f\"Available keys in history: {list(history.keys())}\")\n",
    "         # Try plotting available metrics anyway\n",
    "         # return # Or uncomment to stop if essential plots can't be made\n",
    "\n",
    "    num_epochs = len(history.get('epoch', history.get('train_loss', []))) # Use epoch key if available, else infer from loss\n",
    "    if num_epochs == 0:\n",
    "        logger.warning(\"History contains no epochs to plot.\")\n",
    "        return\n",
    "\n",
    "    epochs_range = history.get('epoch', range(1, num_epochs + 1)) # Use actual epoch numbers if available\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid') # Use a clean style\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12)) # Create 2x2 grid of subplots\n",
    "    fig.suptitle(f'Training History {suffix}'.strip(), fontsize=16)\n",
    "\n",
    "    # --- Plot 1: Loss ---\n",
    "    ax = axs[0, 0]\n",
    "    if 'train_loss' in history and history['train_loss']:\n",
    "        ax.plot(epochs_range, history['train_loss'], 'o-', label='Train Loss', color='royalblue')\n",
    "    if 'val_loss' in history and history['val_loss']:\n",
    "        ax.plot(epochs_range, history['val_loss'], 'o-', label='Validation Loss', color='darkorange')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    if ax.has_data(): ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Plot 2: Macro F1 Score ---\n",
    "    ax = axs[0, 1]\n",
    "    if 'train_f1_macro' in history and history['train_f1_macro']:\n",
    "        ax.plot(epochs_range, history['train_f1_macro'], 'o-', label='Train Macro F1', color='royalblue')\n",
    "    if 'val_f1_macro' in history and history['val_f1_macro']:\n",
    "        ax.plot(epochs_range, history['val_f1_macro'], 'o-', label='Validation Macro F1', color='darkorange')\n",
    "    ax.set_title('Macro F1 Score')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_ylim(bottom=0, top=1.05) # F1 score between 0 and 1\n",
    "    if ax.has_data(): ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Plot 3: Weighted F1 Score ---\n",
    "    ax = axs[1, 0]\n",
    "    if 'train_f1_weighted' in history and history['train_f1_weighted']:\n",
    "        ax.plot(epochs_range, history['train_f1_weighted'], 'o-', label='Train Weighted F1', color='royalblue')\n",
    "    if 'val_f1_weighted' in history and history['val_f1_weighted']:\n",
    "        ax.plot(epochs_range, history['val_f1_weighted'], 'o-', label='Validation Weighted F1', color='darkorange')\n",
    "    ax.set_title('Weighted F1 Score')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_ylim(bottom=0, top=1.05)\n",
    "    if ax.has_data(): ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Plot 4: Validation Accuracy & Hamming Loss ---\n",
    "    ax = axs[1, 1]\n",
    "    plot4_has_data = False\n",
    "    if 'val_accuracy' in history and history['val_accuracy']:\n",
    "         ax.plot(epochs_range, history['val_accuracy'], 'o-', label='Validation Accuracy', color='forestgreen')\n",
    "         plot4_has_data = True\n",
    "    if 'val_hamming' in history and history['val_hamming']:\n",
    "         # Only plot Hamming if it's meaningful (e.g., > 0 for multilabel)\n",
    "         if any(h > 0 for h in history['val_hamming']):\n",
    "             ax.plot(epochs_range, history['val_hamming'], 'o-', label='Validation Hamming Loss', color='crimson')\n",
    "             plot4_has_data = True\n",
    "         else:\n",
    "              logger.info(\"Skipping Hamming Loss plot as values seem to be zero (likely single-label).\")\n",
    "\n",
    "    ax.set_title('Validation Accuracy / Hamming Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_ylim(bottom=-0.05, top=1.05) # Metrics typically between 0 and 1\n",
    "    if plot4_has_data: ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    # --- Save and Show Plot ---\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust layout to prevent title overlap\n",
    "    plot_filename = f'training_history{suffix}.png'\n",
    "    plot_path = os.path.join(output_dir, plot_filename)\n",
    "    try:\n",
    "        plt.savefig(plot_path, dpi=300) # Save with higher resolution\n",
    "        logger.info(f\"Training history plot saved to: {plot_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save training plot to {plot_path}: {e}\")\n",
    "\n",
    "    plt.show() # Display the plot in the notebook\n",
    "    plt.close(fig) # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 11: Evaluation Function (Ensemble)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_ensemble(\n",
    "    model_paths: List[str],\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    label_encoder: LabelEncoder,\n",
    "    is_multilabel: bool,\n",
    "    num_labels: int,\n",
    "    output_dir: str, # Base output directory for saving reports\n",
    "    report_suffix: str = \"eval_ensemble\", # e.g., \"validation_ensemble\", \"test_ensemble\"\n",
    "    base_model_name: str = BASE_TEXT_MODEL_NAME,\n",
    "    visual_feature_dim: int = VISUAL_FEATURE_DIM\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluates an ensemble of models on a given dataloader.\"\"\"\n",
    "\n",
    "    if not dataloader:\n",
    "        logger.warning(f\"Dataloader for '{report_suffix}' is None or empty. Skipping evaluation.\")\n",
    "        return {'error': 'No data provided'}\n",
    "    if not model_paths:\n",
    "        logger.error(\"No model paths provided for ensemble evaluation.\")\n",
    "        return {'error': 'No models provided'}\n",
    "\n",
    "    logger.info(f\"--- Starting ENSEMBLE Evaluation ({report_suffix}) ---\")\n",
    "    logger.info(f\"Evaluating {len(model_paths)} models on {len(dataloader.dataset)} samples.\")\n",
    "    logger.info(f\"Task Type: {'Multi-Label' if is_multilabel else 'Single-Label (Multiclass)'}\")\n",
    "\n",
    "    all_individual_model_logits = [] # Store logits from each model [model1_logits, model2_logits, ...]\n",
    "    all_labels_raw = None # Store true labels once (from the first model's pass)\n",
    "\n",
    "    # --- Get predictions from each model ---\n",
    "    for model_idx, model_path in enumerate(model_paths):\n",
    "        logger.info(f\"Loading and evaluating model {model_idx + 1}/{len(model_paths)}: {os.path.basename(model_path)}\")\n",
    "\n",
    "        try:\n",
    "            # Instantiate the correct model architecture\n",
    "            model = M3H_FusionClassifier(\n",
    "                num_labels=num_labels,\n",
    "                pretrained_model_name=base_model_name,\n",
    "                is_multilabel=is_multilabel,\n",
    "                visual_feature_dim=visual_feature_dim\n",
    "            )\n",
    "            # Load state dict - try strict first, then fallback\n",
    "            try:\n",
    "                # Use weights_only=False if the saved state includes more than just weights (safer default)\n",
    "                # If you are SURE only weights were saved, weights_only=True can be slightly faster/safer.\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=True)\n",
    "                logger.info(\"Model state loaded successfully (strict=True).\")\n",
    "            except RuntimeError as e:\n",
    "                logger.warning(f\"Strict state dict loading failed for model {model_idx+1}: {e}. Attempting non-strict loading (strict=False).\")\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=False)\n",
    "                logger.info(\"Model state loaded with strict=False.\")\n",
    "\n",
    "            model.to(device)\n",
    "            model.eval() # Set to evaluation mode\n",
    "        except FileNotFoundError:\n",
    "             logger.error(f\"Model file not found: {model_path}. Skipping this model.\")\n",
    "             continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model {model_idx + 1} from {model_path}: {e}\", exc_info=True)\n",
    "            continue # Skip this model if loading fails\n",
    "\n",
    "        # --- Collect logits and labels for this model ---\n",
    "        current_model_logits = []\n",
    "        batch_labels_list = [] # Collect labels only for the first model pass\n",
    "\n",
    "        eval_progress = tqdm(dataloader, desc=f\"Eval Model {model_idx + 1}\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(eval_progress):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                image_features = batch['image_features'].to(device) # Get image features\n",
    "                labels = batch['label'].to(device) # Get labels for storage (if first model)\n",
    "\n",
    "                try:\n",
    "                    # Forward pass (only need logits)\n",
    "                    outputs = model(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    image_features=image_features, # Pass image features\n",
    "                                    labels=None) # No need to calculate loss here\n",
    "\n",
    "                    current_model_logits.append(outputs.logits.cpu())\n",
    "\n",
    "                    # Store labels only during the first model's pass\n",
    "                    if model_idx == 0:\n",
    "                        batch_labels_list.append(labels.cpu())\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during evaluation batch {batch_idx} for model {model_idx + 1}: {e}\", exc_info=True)\n",
    "                    # Continue to next batch if one fails\n",
    "\n",
    "        # Concatenate logits for the current model\n",
    "        if current_model_logits:\n",
    "            all_individual_model_logits.append(torch.cat(current_model_logits, dim=0))\n",
    "        else:\n",
    "             logger.warning(f\"No logits collected for model {model_idx + 1}. It might be skipped in the ensemble average.\")\n",
    "\n",
    "\n",
    "        # Concatenate all labels after the first model's pass\n",
    "        if model_idx == 0 and batch_labels_list:\n",
    "            all_labels_raw = torch.cat(batch_labels_list, dim=0)\n",
    "        elif model_idx == 0 and not batch_labels_list:\n",
    "             logger.error(\"Failed to collect any labels during evaluation. Cannot calculate metrics.\")\n",
    "             return {'error': 'Label collection failed'}\n",
    "\n",
    "\n",
    "        # Clean up memory for the current model\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device == torch.device('cuda'):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # --- Aggregate predictions and calculate metrics ---\n",
    "    if not all_individual_model_logits or all_labels_raw is None:\n",
    "        logger.error(\"Evaluation failed: No valid logits collected from any model or labels are missing.\")\n",
    "        return {'error': 'Logit/Label collection failed'}\n",
    "\n",
    "    # Stack logits from all models: shape (num_models, num_samples, num_labels)\n",
    "    stacked_logits = torch.stack(all_individual_model_logits, dim=0)\n",
    "    logger.info(f\"Logits collected from {stacked_logits.shape[0]} models. Stacked shape: {stacked_logits.shape}\")\n",
    "\n",
    "    # Average logits across models: shape (num_samples, num_labels)\n",
    "    avg_logits = torch.mean(stacked_logits, dim=0)\n",
    "\n",
    "    # --- Calculate Metrics based on Task Type ---\n",
    "    metrics_dict = {}\n",
    "    classification_rep = \"Classification report generation failed.\"\n",
    "    confusion_matrix_report = \"Confusion matrix generation failed.\"\n",
    "\n",
    "    try:\n",
    "        if is_multilabel:\n",
    "            logger.info(f\"Calculating multi-label metrics ({report_suffix})...\")\n",
    "            # Convert averaged logits to probabilities and then predictions\n",
    "            probs = torch.sigmoid(avg_logits).numpy()\n",
    "            preds = (probs > 0.5).astype(int) # Use 0.5 threshold\n",
    "            labels = all_labels_raw.numpy().astype(int)\n",
    "\n",
    "            # Calculate standard multi-label metrics\n",
    "            accuracy = accuracy_score(labels, preds) # Subset accuracy\n",
    "            hamming = hamming_loss(labels, preds)\n",
    "            f1_micro = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "            f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "            f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "            f1_samples = f1_score(labels, preds, average='samples', zero_division=0)\n",
    "\n",
    "            metrics_dict = {\n",
    "                'accuracy_subset': accuracy, 'hamming_loss': hamming,\n",
    "                'micro_f1': f1_micro, 'macro_f1': f1_macro,\n",
    "                'weighted_f1': f1_weighted, 'samples_f1': f1_samples\n",
    "            }\n",
    "\n",
    "            # Generate classification report\n",
    "            try:\n",
    "                 classification_rep = classification_report(\n",
    "                     labels, preds,\n",
    "                     target_names=label_encoder.classes_,\n",
    "                     digits=4, zero_division=0\n",
    "                 )\n",
    "            except Exception as cr_e:\n",
    "                 logger.error(f\"Error generating classification report: {cr_e}\")\n",
    "                 classification_rep = f\"Error generating classification report: {cr_e}\"\n",
    "\n",
    "\n",
    "            # Generate multi-label confusion matrix report (one matrix per label)\n",
    "            try:\n",
    "                 cm = multilabel_confusion_matrix(labels, preds)\n",
    "                 cm_lines = [\"--- Multilabel Confusion Matrices ---\"]\n",
    "                 for i, label_name in enumerate(label_encoder.classes_):\n",
    "                     cm_lines.append(f\"\\nLabel: {label_name} (Index {i})\")\n",
    "                     cm_lines.append(f\"{cm[i]}\") # TN, FP / FN, TP\n",
    "                     cm_lines.append(f\"  TN={cm[i][0,0]}, FP={cm[i][0,1]}\")\n",
    "                     cm_lines.append(f\"  FN={cm[i][1,0]}, TP={cm[i][1,1]}\")\n",
    "                 confusion_matrix_report = \"\\n\".join(cm_lines)\n",
    "                 cm_report_path = os.path.join(output_dir, f\"confusion_matrix_{report_suffix}.txt\")\n",
    "                 with open(cm_report_path, \"w\") as f:\n",
    "                     f.write(confusion_matrix_report)\n",
    "                 logger.info(f\"Multi-label confusion matrix report saved to: {cm_report_path}\")\n",
    "            except Exception as cm_e:\n",
    "                 logger.error(f\"Error generating multilabel confusion matrix: {cm_e}\")\n",
    "                 confusion_matrix_report = f\"Error generating multilabel confusion matrix: {cm_e}\"\n",
    "\n",
    "\n",
    "        else: # Single-label (Multiclass)\n",
    "            logger.info(f\"Calculating single-label metrics ({report_suffix})...\")\n",
    "            # Convert averaged logits to predictions\n",
    "            preds = torch.argmax(avg_logits, dim=1).numpy()\n",
    "            labels = all_labels_raw.numpy()\n",
    "\n",
    "            # Calculate standard classification metrics\n",
    "            accuracy = accuracy_score(labels, preds)\n",
    "            # Hamming loss is less common but can be calculated (is 1 - accuracy here)\n",
    "            hamming = hamming_loss(labels, preds)\n",
    "            f1_micro = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "            f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "            f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "            # f1_samples is not applicable here\n",
    "\n",
    "            metrics_dict = {\n",
    "                'accuracy': accuracy, 'hamming_loss': hamming, # Note: hamming = 1-accuracy for single-label\n",
    "                'micro_f1': f1_micro, 'macro_f1': f1_macro,\n",
    "                'weighted_f1': f1_weighted\n",
    "            }\n",
    "\n",
    "            # Generate classification report (handle potentially missing classes in preds)\n",
    "            try:\n",
    "                # Get unique labels present in either true labels or predictions\n",
    "                present_labels = sorted(list(set(labels) | set(preds)))\n",
    "                # Get corresponding names from encoder, checking bounds\n",
    "                target_names = [label_encoder.classes_[i] for i in present_labels if 0 <= i < len(label_encoder.classes_)]\n",
    "                valid_present_labels = [i for i in present_labels if 0 <= i < len(label_encoder.classes_)]\n",
    "\n",
    "                if valid_present_labels: # Only generate report if there are valid labels\n",
    "                    classification_rep = classification_report(\n",
    "                        labels, preds,\n",
    "                        labels=valid_present_labels, # Report only on present labels\n",
    "                        target_names=target_names,\n",
    "                        digits=4, zero_division=0\n",
    "                    )\n",
    "                else:\n",
    "                    classification_rep = \"No valid labels found in predictions or ground truth.\"\n",
    "            except Exception as cr_e:\n",
    "                 logger.error(f\"Error generating classification report: {cr_e}\")\n",
    "                 classification_rep = f\"Error generating classification report: {cr_e}\"\n",
    "\n",
    "            # Confusion matrix for single-label (can be plotted or saved)\n",
    "            # Not generating detailed text report here, focus on classification report\n",
    "            confusion_matrix_report = \"Standard confusion matrix can be generated if needed (not included in this text report).\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating metrics for {report_suffix}: {e}\", exc_info=True)\n",
    "        metrics_dict = {'error': f\"Metric calculation failed: {e}\"}\n",
    "        classification_rep = f\"Report generation failed due to metric error: {e}\"\n",
    "\n",
    "    # --- Save Report ---\n",
    "    metrics_dict['report'] = classification_rep # Add the report string to the dictionary\n",
    "    report_path = os.path.join(output_dir, f\"classification_report_{report_suffix}.txt\")\n",
    "    try:\n",
    "        with open(report_path, \"w\", encoding='utf-8') as f:\n",
    "            f.write(f\"--- Ensemble Evaluation Metrics ({report_suffix}) ---\\n\\n\")\n",
    "            for key, value in metrics_dict.items():\n",
    "                if key != 'report':\n",
    "                    # Format floats nicely, handle other types gracefully\n",
    "                    if isinstance(value, float):\n",
    "                         f.write(f\"{key.replace('_', ' ').title()}: {value:.4f}\\n\")\n",
    "                    else:\n",
    "                         f.write(f\"{key.replace('_', ' ').title()}: {value}\\n\")\n",
    "\n",
    "            f.write(\"\\n--- Classification Report ---\\n\\n\")\n",
    "            f.write(classification_rep)\n",
    "            f.write(\"\\n\\n\")\n",
    "            # Optionally include CM report here too if desired\n",
    "            # f.write(confusion_matrix_report)\n",
    "\n",
    "        logger.info(f\"Evaluation report saved to: {report_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save evaluation report to {report_path}: {e}\")\n",
    "\n",
    "    logger.info(f\"--- Ensemble Evaluation ({report_suffix}) Complete ---\")\n",
    "    # Log key metrics directly\n",
    "    log_key = 'accuracy_subset' if is_multilabel else 'accuracy'\n",
    "    logger.info(f\"Final {report_suffix} {log_key}: {metrics_dict.get(log_key, 'N/A'):.4f}, Macro F1: {metrics_dict.get('macro_f1', 'N/A'):.4f}\")\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 12: Prediction Function (Ensemble)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_ensemble(\n",
    "    model_paths: List[str],\n",
    "    tokenizer: BartTokenizer,\n",
    "    label_encoder: LabelEncoder,\n",
    "    text: str,\n",
    "    triples: str = \"\", # Optional triples input\n",
    "    image_features: Optional[torch.Tensor] = None, # Input image features\n",
    "    device: Optional[torch.device] = None,\n",
    "    max_length: int = MAX_LEN,\n",
    "    is_multilabel: bool = False,\n",
    "    threshold: float = 0.5, # Threshold for multi-label prediction\n",
    "    base_model_name: str = BASE_TEXT_MODEL_NAME,\n",
    "    visual_feature_dim: int = VISUAL_FEATURE_DIM,\n",
    "    num_labels: Optional[int] = None # Can be inferred from encoder if not provided\n",
    ") -> Dict:\n",
    "    \"\"\"Generates predictions for a single instance using an ensemble of models.\"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if not model_paths:\n",
    "        logger.error(\"No model paths provided for prediction.\")\n",
    "        return {'error': 'No models provided'}\n",
    "\n",
    "    if num_labels is None:\n",
    "         try:\n",
    "             num_labels = len(label_encoder.classes_)\n",
    "         except AttributeError:\n",
    "             logger.error(\"Cannot infer num_labels: LabelEncoder not fitted or invalid.\")\n",
    "             return {'error': 'Cannot infer number of labels'}\n",
    "\n",
    "\n",
    "    # --- Prepare Input ---\n",
    "    # 1. Clean triples and construct prompt\n",
    "    cleaned_triples = clean_triples(triples)\n",
    "    # Use PromptConstructor logic (simplified here for single instance)\n",
    "    class_names_str = \", \".join(label_encoder.classes_)\n",
    "    prompt = (\n",
    "        f\"Perform {'multilabel' if is_multilabel else 'multiclass'} classification.\\n\"\n",
    "        f\"Choose from: {class_names_str}.\\n\\n\"\n",
    "        f\"Text: {text}\\n\"\n",
    "    )\n",
    "    if cleaned_triples:\n",
    "        prompt += f\"Knowledge:\\n{cleaned_triples}\\n\"\n",
    "    prompt += \"Category:\"\n",
    "\n",
    "    # 2. Tokenize prompt\n",
    "    try:\n",
    "        encoding = tokenizer(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            padding='max_length', # Ensure consistent length\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(device)       # Shape: (1, seq_len)\n",
    "        attention_mask = encoding['attention_mask'].to(device) # Shape: (1, seq_len)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during tokenization: {e}\", exc_info=True)\n",
    "        return {'error': f\"Tokenization failed: {e}\"}\n",
    "\n",
    "    # 3. Prepare image features\n",
    "    if image_features is None:\n",
    "        logger.warning(\"No image features provided for prediction. Using default zero vector.\")\n",
    "        image_features = torch.zeros(1, visual_feature_dim, dtype=torch.float) # Add batch dim\n",
    "    elif not isinstance(image_features, torch.Tensor):\n",
    "        logger.error(f\"image_features must be a torch.Tensor, but got {type(image_features)}.\")\n",
    "        return {'error': 'Invalid image feature type'}\n",
    "    elif image_features.shape != (visual_feature_dim,): # Expecting single sample feature (dim,)\n",
    "         # If shape is (1, dim), it's already batched, otherwise add batch dim if it's (dim,)\n",
    "         if image_features.shape == (1, visual_feature_dim):\n",
    "             pass # Already has batch dimension\n",
    "         elif len(image_features.shape) == 1 and image_features.shape[0] == visual_feature_dim:\n",
    "              image_features = image_features.unsqueeze(0) # Add batch dimension: (dim,) -> (1, dim)\n",
    "         else:\n",
    "              logger.error(f\"Provided image_features have incorrect shape {image_features.shape}. Expected ({visual_feature_dim},) or (1, {visual_feature_dim}).\")\n",
    "              return {'error': 'Incorrect image feature dimensions'}\n",
    "\n",
    "    # Ensure image features are float and on the correct device\n",
    "    image_features = image_features.float().to(device) # Shape: (1, visual_feature_dim)\n",
    "\n",
    "\n",
    "    # --- Get Predictions from Ensemble ---\n",
    "    all_logits = [] # Store logits from each model for this instance\n",
    "    for model_idx, model_path in enumerate(model_paths):\n",
    "        try:\n",
    "            # Instantiate model\n",
    "            model = M3H_FusionClassifier(\n",
    "                num_labels=num_labels,\n",
    "                pretrained_model_name=base_model_name,\n",
    "                is_multilabel=is_multilabel,\n",
    "                visual_feature_dim=visual_feature_dim\n",
    "            )\n",
    "            # Load state dict\n",
    "            try:\n",
    "                 model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=True)\n",
    "            except RuntimeError:\n",
    "                 logger.warning(f\"Strict loading failed for model {model_idx+1}. Trying strict=False.\")\n",
    "                 model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=False)\n",
    "\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            # Get logits\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                image_features=image_features, # Pass image features\n",
    "                                labels=None)\n",
    "                all_logits.append(outputs.logits.cpu()) # Move logits to CPU\n",
    "\n",
    "            # Clean up model memory\n",
    "            del model\n",
    "            gc.collect()\n",
    "            if device == torch.device('cuda'):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             logger.error(f\"Prediction failed: Model file not found: {model_path}. Skipping this model.\")\n",
    "             continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading or predicting with model {model_idx + 1} ({os.path.basename(model_path)}): {e}\", exc_info=True)\n",
    "            # Continue to try other models in the ensemble\n",
    "\n",
    "    if not all_logits:\n",
    "        logger.error(\"Prediction failed: Could not get logits from any model in the ensemble.\")\n",
    "        return {'error': 'No models produced valid logits'}\n",
    "\n",
    "    # --- Aggregate Logits and Determine Final Prediction ---\n",
    "    # Stack logits: shape (num_valid_models, 1, num_labels) -> squeeze -> (num_valid_models, num_labels)\n",
    "    stacked_logits = torch.stack(all_logits, dim=0).squeeze(1)\n",
    "    # Average logits across models\n",
    "    avg_logits = torch.mean(stacked_logits, dim=0) # Shape: (num_labels,)\n",
    "\n",
    "    prediction_result = {'probabilities': {}}\n",
    "    try:\n",
    "        if is_multilabel:\n",
    "            # Calculate probabilities using sigmoid\n",
    "            probs = torch.sigmoid(avg_logits).numpy()\n",
    "            # Determine predicted labels based on threshold\n",
    "            predicted_indices = np.where(probs > threshold)[0]\n",
    "\n",
    "            if len(predicted_indices) > 0:\n",
    "                 # Inverse transform indices to label names\n",
    "                 predicted_labels = label_encoder.inverse_transform(predicted_indices).tolist()\n",
    "            else:\n",
    "                 predicted_labels = [\"None\"] # Or return empty list: []\n",
    "\n",
    "            prediction_result['predicted_labels'] = predicted_labels # List of predicted labels\n",
    "            # Store all probabilities\n",
    "            prediction_result['probabilities'] = {label_encoder.classes_[i]: float(probs[i]) for i in range(len(probs))}\n",
    "\n",
    "        else: # Single-label (Multiclass)\n",
    "            # Calculate probabilities using softmax\n",
    "            probs = torch.softmax(avg_logits, dim=0)\n",
    "            # Get the index of the highest probability\n",
    "            pred_class_idx = torch.argmax(avg_logits).item()\n",
    "            predicted_label = \"Error: Index out of bounds\"\n",
    "            try:\n",
    "                if 0 <= pred_class_idx < len(label_encoder.classes_):\n",
    "                     predicted_label = label_encoder.inverse_transform([pred_class_idx])[0]\n",
    "                else:\n",
    "                     logger.error(f\"Predicted class index {pred_class_idx} is out of bounds for label encoder classes (size {len(label_encoder.classes_)}).\")\n",
    "            except Exception as le_error:\n",
    "                 logger.error(f\"Error during label encoder inverse transform for index {pred_class_idx}: {le_error}\")\n",
    "\n",
    "            prediction_result['predicted_class'] = predicted_label # Single predicted class name\n",
    "            # Store all probabilities\n",
    "            class_probs_numpy = probs.cpu().numpy()\n",
    "            prediction_result['probabilities'] = {label_encoder.classes_[i]: float(class_probs_numpy[i]) for i in range(len(class_probs_numpy))}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error aggregating predictions or converting labels: {e}\", exc_info=True)\n",
    "        prediction_result = {'error': f\"Prediction aggregation failed: {e}\"}\n",
    "\n",
    "    return prediction_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 13: Main Pipeline Function (Orchestrates Ensemble Training)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_ensemble_pipeline(\n",
    "    dataset_type: str = 'anxiety', # 'anxiety' or 'depression'\n",
    "    num_ensemble_models: int = NUM_ENSEMBLE_MODELS,\n",
    "    base_seed: int = BASE_SEED,\n",
    "    use_test_set: bool = True, # If True, loads separate test file. If False, splits train -> train/val/test\n",
    "    val_split_ratio: float = 0.1,  # Fraction of *original* train data for validation\n",
    "    test_split_ratio: float = 0.2, # Fraction of *original* train data for test (used only if use_test_set=False)\n",
    "    batch_size_override: int = BATCH_SIZE,\n",
    "    num_epochs_per_model: int = NUM_EPOCHS,\n",
    "    embedding_model_name: str = TEXT_EMBEDDING_MODEL, # For RAG text embeddings\n",
    "    base_text_model_name: str = BASE_TEXT_MODEL_NAME, # For classifier\n",
    "    visual_feature_dim: int = VISUAL_FEATURE_DIM,\n",
    "    visual_feature_dir: str = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\") # Dir where features are saved/loaded\n",
    ") -> Tuple[Optional[Dict], Optional[BartTokenizer], Optional[LabelEncoder], List[str]]:\n",
    "    \"\"\"\n",
    "    Runs the full ensemble training and evaluation pipeline.\n",
    "\n",
    "    Args:\n",
    "        dataset_type: Type of dataset ('anxiety' or 'depression').\n",
    "        num_ensemble_models: Number of models to train in the ensemble.\n",
    "        base_seed: Starting seed for reproducibility across ensemble runs.\n",
    "        use_test_set: Whether to load a separate test JSON file.\n",
    "        val_split_ratio: Ratio for the validation set.\n",
    "        test_split_ratio: Ratio for the test set (only if use_test_set=False).\n",
    "        batch_size_override: Batch size for DataLoaders.\n",
    "        num_epochs_per_model: Epochs to train each individual model.\n",
    "        embedding_model_name: Name of the SentenceTransformer model for RAG.\n",
    "        base_text_model_name: Name of the BART model for the classifier.\n",
    "        visual_feature_dim: Dimension of the visual features.\n",
    "        visual_feature_dir: Directory containing pre-computed visual features.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - Final evaluation metrics dictionary (or None if failed).\n",
    "        - Trained BartTokenizer (or None if failed).\n",
    "        - Fitted LabelEncoder (or None if failed).\n",
    "        - List of paths to the best trained models in the ensemble.\n",
    "    \"\"\"\n",
    "\n",
    "    is_multilabel = (dataset_type == 'depression')\n",
    "    logger.info(f\"--- Starting ENSEMBLE Pipeline ({num_ensemble_models} models) for Dataset: {dataset_type} ---\")\n",
    "    logger.info(f\"Task Type: {'Multi-Label' if is_multilabel else 'Single-Label (Multiclass)'}\")\n",
    "\n",
    "    # --- 1. Define Paths and Load Data ---\n",
    "    if dataset_type == \"anxiety\":\n",
    "        train_file_path = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Anxiety_Data\", \"anxiety_train.json\")\n",
    "        test_file_path = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Anxiety_Data\", \"anxiety_test.json\")\n",
    "        img_feature_train_path = os.path.join(visual_feature_dir, \"anxiety_train_features.pt\")\n",
    "        img_feature_test_path = os.path.join(visual_feature_dir, \"anxiety_test_features.pt\")\n",
    "        img_feature_val_path = None # No separate val features assumed for anxiety\n",
    "        output_basedir_name = \"anxiety\"\n",
    "    elif dataset_type == \"depression\":\n",
    "        train_file_path = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Depressive_Data\", \"train.json\")\n",
    "        test_file_path = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Depressive_Data\", \"test.json\")\n",
    "        val_file_path = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Depressive_Data\", \"val.json\") # Separate val file\n",
    "        img_feature_train_path = os.path.join(visual_feature_dir, \"depression_train_features.pt\")\n",
    "        img_feature_test_path = os.path.join(visual_feature_dir, \"depression_test_features.pt\")\n",
    "        img_feature_val_path = os.path.join(visual_feature_dir, \"depression_val_features.pt\") # Separate val features\n",
    "        output_basedir_name = \"depression\"\n",
    "    else:\n",
    "        logger.error(f\"Invalid dataset_type specified: {dataset_type}. Choose 'anxiety' or 'depression'.\")\n",
    "        return None, None, None, []\n",
    "\n",
    "    # --- Create Output Directory ---\n",
    "    pipeline_base_output_dir = os.path.join(KAGGLE_WORKING_DIR, output_basedir_name, \"output\", \"ensemble_fusion\")\n",
    "    os.makedirs(pipeline_base_output_dir, exist_ok=True)\n",
    "    logger.info(f\"Pipeline base output directory: {pipeline_base_output_dir}\")\n",
    "\n",
    "    # --- Load Text Data ---\n",
    "    logger.info(\"Step 1a: Loading text data...\")\n",
    "    full_train_data = load_data(train_file_path)\n",
    "    if not full_train_data:\n",
    "        logger.error(f\"Failed to load training data from {train_file_path}. Aborting pipeline.\")\n",
    "        return None, None, None, []\n",
    "\n",
    "    train_data, val_data, test_data = [], [], []\n",
    "    separate_val_file_loaded = False\n",
    "\n",
    "    # Handle Validation/Test data loading/splitting\n",
    "    if dataset_type == 'depression' and os.path.exists(val_file_path):\n",
    "         logger.info(f\"Loading separate validation data from {val_file_path}\")\n",
    "         val_data = load_data(val_file_path)\n",
    "         if not val_data:\n",
    "              logger.warning(f\"Separate validation file {val_file_path} loaded empty. Will split from train set.\")\n",
    "              # Proceed to split train_data into train/val below\n",
    "         else:\n",
    "              train_data = full_train_data # Use all loaded train data as train\n",
    "              separate_val_file_loaded = True\n",
    "              logger.info(f\"Loaded {len(val_data)} samples for validation.\")\n",
    "\n",
    "    if use_test_set and os.path.exists(test_file_path):\n",
    "        logger.info(f\"Loading separate test data from {test_file_path}\")\n",
    "        test_data = load_data(test_file_path)\n",
    "        if not test_data:\n",
    "             logger.warning(f\"Separate test file {test_file_path} loaded empty. Test set will be empty.\")\n",
    "        else:\n",
    "             logger.info(f\"Loaded {len(test_data)} samples for testing.\")\n",
    "        # If validation data wasn't loaded separately, split the training data\n",
    "        if not separate_val_file_loaded:\n",
    "             logger.info(\"Splitting loaded train data into Train and Validation sets (no test split as test file loaded).\")\n",
    "             train_data, val_data, _ = split_data(full_train_data, val_size=val_split_ratio, test_size=0, random_state=base_seed) # test_size=0 means no test split from train\n",
    "    elif use_test_set and not os.path.exists(test_file_path):\n",
    "        logger.warning(f\"use_test_set=True, but test file not found at {test_file_path}.\")\n",
    "        if not separate_val_file_loaded:\n",
    "             logger.info(\"Splitting loaded train data into Train and Validation sets (no test split).\")\n",
    "             train_data, val_data, _ = split_data(full_train_data, val_size=val_split_ratio, test_size=0, random_state=base_seed)\n",
    "        test_data = [] # Ensure test_data is empty\n",
    "    else: # use_test_set is False - split train data into train, val, test\n",
    "        if separate_val_file_loaded:\n",
    "             logger.warning(\"use_test_set=False but separate validation data was loaded. Test set will be empty.\")\n",
    "             test_data = []\n",
    "             # Train and Val already set\n",
    "        else:\n",
    "             logger.info(\"Splitting loaded train data into Train, Validation, and Test sets.\")\n",
    "             train_data, val_data, test_data = split_data(full_train_data, val_size=val_split_ratio, test_size=test_split_ratio, random_state=base_seed)\n",
    "\n",
    "\n",
    "    logger.info(f\"Data split sizes: Train={len(train_data)}, Validation={len(val_data)}, Test={len(test_data)}\")\n",
    "    if not train_data or not val_data:\n",
    "        logger.error(\"Training or Validation data is empty after loading/splitting. Aborting.\")\n",
    "        return None, None, None, []\n",
    "\n",
    "    # --- Load Visual Features ---\n",
    "    logger.info(\"Step 1b: Loading pre-computed visual features...\")\n",
    "    image_features_train = {}\n",
    "    image_features_val = {}\n",
    "    image_features_test = {}\n",
    "\n",
    "    def load_feature_file(path, description):\n",
    "        features = {}\n",
    "        if path and os.path.exists(path):\n",
    "            try:\n",
    "                features = torch.load(path, map_location='cpu') # Load to CPU first\n",
    "                logger.info(f\"Loaded {len(features)} {description} features from {os.path.basename(path)}.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {description} features from {path}: {e}\", exc_info=True)\n",
    "        elif path:\n",
    "             logger.warning(f\"{description.capitalize()} feature file not found: {path}\")\n",
    "        return features\n",
    "\n",
    "    image_features_train = load_feature_file(img_feature_train_path, \"train\")\n",
    "    if img_feature_val_path: # Only load if path exists (e.g., for depression)\n",
    "         image_features_val = load_feature_file(img_feature_val_path, \"validation\")\n",
    "    if use_test_set or not use_test_set: # Load test features if path exists regardless of split method\n",
    "        image_features_test = load_feature_file(img_feature_test_path, \"test\")\n",
    "\n",
    "\n",
    "    # Combine all loaded features into a single map for easy access by Datasets\n",
    "    # Prioritize train, then val, then test if there are ID overlaps (shouldn't happen with good IDs)\n",
    "    image_features_map = {**image_features_test, **image_features_val, **image_features_train}\n",
    "    logger.info(f\"Combined visual feature map contains features for {len(image_features_map)} unique image IDs.\")\n",
    "    # Optional: Check coverage\n",
    "    train_ids_with_features = sum(1 for s in train_data if s.get('image_id') in image_features_map)\n",
    "    val_ids_with_features = sum(1 for s in val_data if s.get('image_id') in image_features_map)\n",
    "    test_ids_with_features = sum(1 for s in test_data if s.get('image_id') in image_features_map)\n",
    "    logger.info(f\"Feature coverage: Train={train_ids_with_features}/{len(train_data)}, Val={val_ids_with_features}/{len(val_data)}, Test={test_ids_with_features}/{len(test_data)}\")\n",
    "    if train_ids_with_features < len(train_data) or val_ids_with_features < len(val_data):\n",
    "         logger.warning(\"Missing some visual features for train/validation data. Models will use zero vectors for these.\")\n",
    "\n",
    "    # --- Clean Triples ---\n",
    "    logger.info(\"Step 1c: Cleaning triples text in all datasets...\")\n",
    "    for dataset in [train_data, val_data, test_data]:\n",
    "        if dataset: # Check if dataset is not empty\n",
    "            for sample in dataset:\n",
    "                sample['triples'] = clean_triples(sample.get('triples', ''))\n",
    "\n",
    "    # --- Encode Labels ---\n",
    "    logger.info(\"Step 1d: Encoding labels...\")\n",
    "    all_possible_labels = set()\n",
    "    for dataset in [train_data, val_data, test_data]:\n",
    "        if dataset:\n",
    "            for sample in dataset:\n",
    "                labels_in_sample = sample.get('original_labels')\n",
    "                if isinstance(labels_in_sample, list):\n",
    "                    all_possible_labels.update(label for label in labels_in_sample if label) # Add non-empty labels\n",
    "                elif isinstance(labels_in_sample, str) and labels_in_sample: # Check if string and non-empty\n",
    "                    all_possible_labels.add(labels_in_sample)\n",
    "                # else: ignore None or empty lists/strings\n",
    "\n",
    "    if not all_possible_labels:\n",
    "        logger.error(\"No valid labels found in any dataset split. Cannot proceed.\")\n",
    "        return None, None, None, []\n",
    "\n",
    "    # Sort labels for consistent encoding\n",
    "    sorted_labels = sorted(list(all_possible_labels))\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(sorted_labels)\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    logger.info(f\"Label encoding complete. Found {num_labels} unique labels: {label_encoder.classes_.tolist()}\")\n",
    "\n",
    "    # --- RAG Setup (Text Embeddings) ---\n",
    "    # Only generate embeddings for training data to build the RAG index\n",
    "    train_fused_embeddings = None\n",
    "    retriever = None\n",
    "    embedding_generator = None # Define scope outside try block\n",
    "\n",
    "    logger.info(\"Step 2: Generating text embeddings for RAG index (Train data only)...\")\n",
    "    train_ocr = [s.get('ocr_text', '') for s in train_data]\n",
    "    train_triples = [s.get('triples', '') for s in train_data]\n",
    "\n",
    "    try:\n",
    "        # Initialize embedding generator (ensure model is loaded to device)\n",
    "        embedding_generator = EmbeddingGenerator(model_name=embedding_model_name, device=device)\n",
    "        # Generate fused embeddings (OCR + Triples)\n",
    "        train_fused_embeddings = embedding_generator.generate_fused_embeddings(train_ocr, train_triples)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during text embedding generation: {e}\", exc_info=True)\n",
    "        logger.warning(\"Proceeding without RAG features due to embedding error.\")\n",
    "    finally:\n",
    "        # Clean up embedding model from memory\n",
    "        del embedding_generator\n",
    "        if 'embedding_generator' in locals() and hasattr(locals()['embedding_generator'], 'model'):\n",
    "             del locals()['embedding_generator'].model # Try deleting model explicitly\n",
    "        gc.collect()\n",
    "        if device == torch.device('cuda'):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    logger.info(\"Step 3: Building RAG retriever index (if embeddings generated)...\")\n",
    "    if train_fused_embeddings is not None:\n",
    "        retriever = RAGRetriever(train_fused_embeddings, top_k=RETRIEVAL_K)\n",
    "        if retriever.index is None:\n",
    "            logger.warning(\"FAISS index building failed. RAG retrieval will be disabled.\")\n",
    "            retriever = None # Disable retriever if index failed\n",
    "    else:\n",
    "        logger.warning(\"No text embeddings generated for training data. RAG retrieval disabled.\")\n",
    "        retriever = None\n",
    "\n",
    "\n",
    "    # --- Prompt Construction ---\n",
    "    logger.info(\"Step 4: Preparing prompt constructor...\")\n",
    "    prompt_constructor = PromptConstructor(train_data, label_encoder)\n",
    "\n",
    "    # Helper function to generate prompts (with or without RAG)\n",
    "    # RAG is currently disabled for simplicity and focus on fusion. Enable if needed.\n",
    "    def get_prompts_for_dataset(data_split: List[Dict], split_name: str) -> List[str]:\n",
    "        logger.info(f\"Generating prompts for {split_name} data ({len(data_split)} samples)...\")\n",
    "        # RAG Disabled Version:\n",
    "        prompts = [prompt_constructor.construct_prompt(s, None) for s in tqdm(data_split, desc=f\"Generating {split_name} Prompts\")]\n",
    "\n",
    "        # RAG Enabled Version (Example - requires query embeddings):\n",
    "        # logger.info(f\"Generating query embeddings for {split_name} data...\")\n",
    "        # split_ocr = [s.get('ocr_text', '') for s in data_split]\n",
    "        # split_triples = [s.get('triples', '') for s in data_split]\n",
    "        # query_embeddings = None\n",
    "        # try:\n",
    "        #     temp_embed_gen = EmbeddingGenerator(model_name=embedding_model_name, device=device)\n",
    "        #     query_embeddings = temp_embed_gen.generate_fused_embeddings(split_ocr, split_triples)\n",
    "        # finally:\n",
    "        #     del temp_embed_gen # Clean up temporary generator\n",
    "        #     gc.collect()\n",
    "        #     if device == torch.device('cuda'): torch.cuda.empty_cache()\n",
    "\n",
    "        # prompts = []\n",
    "        # retrieved_indices_batch = None\n",
    "        # if retriever and query_embeddings is not None:\n",
    "        #     logger.info(f\"Retrieving similar examples for {split_name}...\")\n",
    "        #     retrieved_indices_batch = retriever.retrieve_similar(query_embeddings)\n",
    "\n",
    "        # for i, sample in enumerate(tqdm(data_split, desc=f\"Constructing {split_name} Prompts\")):\n",
    "        #     retrieved_ids = None\n",
    "        #     if retrieved_indices_batch is not None and i < len(retrieved_indices_batch):\n",
    "        #          # Exclude self-retrieval (assuming index i corresponds to sample i)\n",
    "        #          retrieved_ids = [idx for idx in retrieved_indices_batch[i] if idx != i][:RETRIEVAL_K]\n",
    "        #     prompts.append(prompt_constructor.construct_prompt(sample, retrieved_ids))\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    # Generate prompts for each split (RAG disabled here)\n",
    "    train_prompts = get_prompts_for_dataset(train_data, \"Train\")\n",
    "    val_prompts = get_prompts_for_dataset(val_data, \"Validation\")\n",
    "    test_prompts = get_prompts_for_dataset(test_data, \"Test\") if test_data else []\n",
    "\n",
    "\n",
    "    # --- Tokenizer, Datasets, DataLoaders ---\n",
    "    logger.info(\"Step 5: Loading tokenizer...\")\n",
    "    try:\n",
    "        tokenizer = BartTokenizer.from_pretrained(base_text_model_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load tokenizer '{base_text_model_name}': {e}\", exc_info=True)\n",
    "        return None, None, None, []\n",
    "\n",
    "    logger.info(\"Step 6: Creating Datasets...\")\n",
    "    try:\n",
    "        DatasetClass = DepressionDataset if is_multilabel else AnxietyDataset\n",
    "        train_dataset = DatasetClass(train_data, train_prompts, tokenizer, MAX_LEN, label_encoder, image_features_map, visual_feature_dim)\n",
    "        val_dataset = DatasetClass(val_data, val_prompts, tokenizer, MAX_LEN, label_encoder, image_features_map, visual_feature_dim)\n",
    "        test_dataset = DatasetClass(test_data, test_prompts, tokenizer, MAX_LEN, label_encoder, image_features_map, visual_feature_dim) if test_data else None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Datasets: {e}\", exc_info=True)\n",
    "        return None, tokenizer, label_encoder, [] # Return tokenizer/encoder if they loaded\n",
    "\n",
    "    logger.info(\"Step 7: Creating DataLoaders...\")\n",
    "    try:\n",
    "        # Consider num_workers based on system capabilities (e.g., 2 or 4 on Kaggle)\n",
    "        num_workers = 2 if torch.cuda.is_available() else 0 # Use workers only if GPU available\n",
    "        pin_memory = True if device == torch.device('cuda') else False # Pin memory only for CUDA\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size_override, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size_override, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size_override, shuffle=False, num_workers=num_workers, pin_memory=pin_memory) if test_dataset else None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create DataLoaders: {e}\", exc_info=True)\n",
    "        return None, tokenizer, label_encoder, []\n",
    "\n",
    "    # --- Ensemble Training Loop ---\n",
    "    logger.info(f\"Step 8: Starting Ensemble Training ({num_ensemble_models} models)...\")\n",
    "    trained_model_paths = []\n",
    "    all_histories = [] # Store history from each run\n",
    "\n",
    "    for i in range(num_ensemble_models):\n",
    "        run_seed = base_seed + i\n",
    "        set_seed(run_seed) # Set seed for this specific run\n",
    "        model_run_output_dir = os.path.join(pipeline_base_output_dir, f\"run_{run_seed}\")\n",
    "        os.makedirs(model_run_output_dir, exist_ok=True)\n",
    "        logger.info(f\"--- Training Model {i + 1}/{num_ensemble_models} (Seed: {run_seed}) ---\")\n",
    "        logger.info(f\"Output for this run: {model_run_output_dir}\")\n",
    "\n",
    "        # Instantiate model for this run\n",
    "        try:\n",
    "            model = M3H_FusionClassifier(\n",
    "                num_labels=num_labels,\n",
    "                pretrained_model_name=base_text_model_name,\n",
    "                is_multilabel=is_multilabel,\n",
    "                visual_feature_dim=visual_feature_dim,\n",
    "                dropout_prob=DROPOUT # Use configured dropout\n",
    "            ).to(device)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model initialization failed for run {i + 1} (Seed: {run_seed}): {e}\", exc_info=True)\n",
    "            continue # Skip to the next run\n",
    "\n",
    "        # Optimizer and Scheduler for this run\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, betas=(ADAM_BETA1, ADAM_BETA2), eps=ADAM_EPSILON, weight_decay=WEIGHT_DECAY)\n",
    "        total_training_steps = len(train_dataloader) * num_epochs_per_model\n",
    "        num_warmup_steps = int(0.1 * total_training_steps) # 10% warmup\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_training_steps)\n",
    "\n",
    "        # Train and evaluate this model instance\n",
    "        try:\n",
    "            history, best_model_path_run = train_and_evaluate(\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=num_epochs_per_model,\n",
    "                output_dir=model_run_output_dir,\n",
    "                label_encoder=label_encoder,\n",
    "                is_multilabel=is_multilabel\n",
    "            )\n",
    "            all_histories.append(history)\n",
    "            trained_model_paths.append(best_model_path_run)\n",
    "            logger.info(f\"Successfully completed training for model {i + 1} (Seed: {run_seed}). Best model saved to: {best_model_path_run}\")\n",
    "            # Plot history for this specific run\n",
    "            plot_training_history(history, model_run_output_dir, suffix=f\"_run_{run_seed}\")\n",
    "\n",
    "        except Exception as train_e:\n",
    "            logger.error(f\"Training failed for model {i + 1} (Seed: {run_seed}): {train_e}\", exc_info=True)\n",
    "            # Optionally save the traceback\n",
    "            tb_path = os.path.join(model_run_output_dir, \"error_traceback.txt\")\n",
    "            with open(tb_path, \"w\") as f:\n",
    "                 traceback.print_exc(file=f)\n",
    "            logger.info(f\"Error traceback saved to: {tb_path}\")\n",
    "            # Continue to the next model in the ensemble\n",
    "\n",
    "        finally:\n",
    "            # Clean up GPU memory after each run\n",
    "            del model, optimizer, scheduler\n",
    "            gc.collect()\n",
    "            if device == torch.device('cuda'):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    logger.info(f\"--- Ensemble Training Finished. Successfully trained {len(trained_model_paths)}/{num_ensemble_models} models. ---\")\n",
    "\n",
    "    if not trained_model_paths:\n",
    "        logger.error(\"No models were successfully trained in the ensemble. Aborting further steps.\")\n",
    "        return None, tokenizer, label_encoder, []\n",
    "\n",
    "    # --- Final Ensemble Evaluation ---\n",
    "    logger.info(\"Step 9: Evaluating the trained ensemble...\")\n",
    "    final_metrics = {} # Store metrics from val/test\n",
    "\n",
    "    # Evaluate on Validation Set\n",
    "    logger.info(\"--- Final Validation Set Evaluation (Ensemble) ---\")\n",
    "    val_metrics = evaluate_ensemble(\n",
    "        model_paths=trained_model_paths,\n",
    "        dataloader=val_dataloader,\n",
    "        device=device,\n",
    "        label_encoder=label_encoder,\n",
    "        is_multilabel=is_multilabel,\n",
    "        num_labels=num_labels,\n",
    "        output_dir=pipeline_base_output_dir, # Save reports in the main ensemble output dir\n",
    "        report_suffix=\"validation_ensemble\",\n",
    "        base_model_name=base_text_model_name,\n",
    "        visual_feature_dim=visual_feature_dim\n",
    "    )\n",
    "    final_metrics['validation'] = val_metrics\n",
    "    logger.info(\"Validation Ensemble Metrics:\")\n",
    "    for k, v in val_metrics.items():\n",
    "        if k != 'report': logger.info(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "\n",
    "    # Evaluate on Test Set (if available)\n",
    "    if test_dataloader:\n",
    "        logger.info(\"--- Final Test Set Evaluation (Ensemble) ---\")\n",
    "        test_metrics = evaluate_ensemble(\n",
    "            model_paths=trained_model_paths,\n",
    "            dataloader=test_dataloader,\n",
    "            device=device,\n",
    "            label_encoder=label_encoder,\n",
    "            is_multilabel=is_multilabel,\n",
    "            num_labels=num_labels,\n",
    "            output_dir=pipeline_base_output_dir,\n",
    "            report_suffix=\"test_ensemble\",\n",
    "            base_model_name=base_text_model_name,\n",
    "            visual_feature_dim=visual_feature_dim\n",
    "        )\n",
    "        final_metrics['test'] = test_metrics\n",
    "        logger.info(\"Test Ensemble Metrics:\")\n",
    "        for k, v in test_metrics.items():\n",
    "             if k != 'report': logger.info(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "    else:\n",
    "        logger.info(\"No test dataloader available. Skipping final test set evaluation.\")\n",
    "        final_metrics['test'] = \"Skipped (No Test Data)\"\n",
    "\n",
    "\n",
    "    # --- Save Label Encoder ---\n",
    "    logger.info(\"Step 10: Saving Label Encoder...\")\n",
    "    label_encoder_path = os.path.join(pipeline_base_output_dir, \"label_encoder.pkl\")\n",
    "    try:\n",
    "        with open(label_encoder_path, 'wb') as f:\n",
    "            pickle.dump(label_encoder, f)\n",
    "        logger.info(f\"Label encoder saved successfully to: {label_encoder_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save label encoder to {label_encoder_path}: {e}\")\n",
    "\n",
    "    logger.info(f\"--- ENSEMBLE Pipeline finished for dataset '{dataset_type}' ---\")\n",
    "    logger.info(f\"Best models saved in respective run_* directories within: {pipeline_base_output_dir}\")\n",
    "    logger.info(f\"List of best model paths: {trained_model_paths}\")\n",
    "    logger.info(f\"Final evaluation reports and label encoder saved in: {pipeline_base_output_dir}\")\n",
    "\n",
    "    # Return final metrics, tokenizer, label encoder, and paths to best models\n",
    "    return final_metrics, tokenizer, label_encoder, trained_model_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 14: Execution Cell (Runs Ensemble Pipeline)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration for the Run ---\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#       <<<<< CHOOSE DATASET TYPE HERE >>>>>\n",
    "DATASET_CHOICE = 'depression'   # Options: 'anxiety' or 'depression'\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# --- Other Ensemble & Training Settings ---\n",
    "NUM_ENSEMBLE_RUNS = 3       # How many models in the ensemble (e.g., 3 or 5)\n",
    "EPOCHS_PER_MODEL = 10       # Epochs for EACH model in the ensemble\n",
    "BATCH_SIZE_TO_USE = 8       # Adjust based on GPU memory (T4 usually handles 8 well)\n",
    "\n",
    "# --- Data Splitting Strategy ---\n",
    "# If True, expects 'anxiety_test.json' or 'test.json'/'val.json' in dataset dirs\n",
    "# If False, splits the main training file into train/val/test sets\n",
    "USE_SEPARATE_TEST_FILE = True\n",
    "# Ratios used ONLY if USE_SEPARATE_TEST_FILE = False (or if separate files are missing)\n",
    "# These ratios are relative to the *original* training data size.\n",
    "VALIDATION_SPLIT = 0.1      # e.g., 10% of original train data for validation\n",
    "TEST_SPLIT = 0.2            # e.g., 20% of original train data for test\n",
    "\n",
    "# Override hyperparameters from Cell 3 if needed for this specific run\n",
    "# LEARNING_RATE_OVERRIDE = 5e-5\n",
    "# DROPOUT_OVERRIDE = 0.15\n",
    "\n",
    "\n",
    "# --- Run the Ensemble Pipeline ---\n",
    "logger.info(f\"===== Starting Ensemble Pipeline Execution for: {DATASET_CHOICE} =====\")\n",
    "logger.info(f\"Number of ensemble models: {NUM_ENSEMBLE_RUNS}\")\n",
    "logger.info(f\"Epochs per model: {EPOCHS_PER_MODEL}\")\n",
    "logger.info(f\"Batch size: {BATCH_SIZE_TO_USE}\")\n",
    "logger.info(f\"Using separate test file: {USE_SEPARATE_TEST_FILE}\")\n",
    "\n",
    "# Clean up memory before starting\n",
    "gc.collect()\n",
    "if device == torch.device('cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(f\"CUDA Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    logger.info(f\"CUDA Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "# Initialize variables to store results\n",
    "final_eval_metrics = None\n",
    "trained_tokenizer = None\n",
    "trained_label_encoder = None\n",
    "trained_model_paths = []\n",
    "\n",
    "try:\n",
    "    final_eval_metrics, trained_tokenizer, trained_label_encoder, trained_model_paths = run_ensemble_pipeline(\n",
    "        dataset_type=DATASET_CHOICE,\n",
    "        num_ensemble_models=NUM_ENSEMBLE_RUNS,\n",
    "        base_seed=BASE_SEED, # Use the global base seed\n",
    "        use_test_set=USE_SEPARATE_TEST_FILE,\n",
    "        val_split_ratio=VALIDATION_SPLIT,\n",
    "        test_split_ratio=TEST_SPLIT,\n",
    "        batch_size_override=BATCH_SIZE_TO_USE,\n",
    "        num_epochs_per_model=EPOCHS_PER_MODEL,\n",
    "        embedding_model_name=TEXT_EMBEDDING_MODEL, # From config\n",
    "        base_text_model_name=BASE_TEXT_MODEL_NAME, # From config\n",
    "        visual_feature_dim=VISUAL_FEATURE_DIM,     # From config\n",
    "        visual_feature_dir = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\") # Pass dir where features are stored\n",
    "        # Pass overrides if defined:\n",
    "        # learning_rate_override = LEARNING_RATE_OVERRIDE,\n",
    "        # dropout_override = DROPOUT_OVERRIDE,\n",
    "    )\n",
    "    logger.info(\"<<<<< run_ensemble_pipeline function returned >>>>>\")\n",
    "\n",
    "    # --- Post-Training Analysis ---\n",
    "    if trained_model_paths and trained_tokenizer and trained_label_encoder:\n",
    "        logger.info(f\"Pipeline completed successfully for '{DATASET_CHOICE}'.\")\n",
    "        logger.info(f\"Trained {len(trained_model_paths)} models.\")\n",
    "        logger.info(f\"Best model paths: {trained_model_paths}\")\n",
    "        logger.info(\"Final Evaluation Metrics:\")\n",
    "        print(json.dumps(final_eval_metrics, indent=2)) # Pretty print the metrics dict\n",
    "\n",
    "        # --- Example Prediction Call (Requires loading features for the sample) ---\n",
    "        # You would need to get the 'image_features' tensor for a specific sample ID\n",
    "        # example_sample_id = \"some_image_id_from_your_data\"\n",
    "        # try:\n",
    "        #     # Load the combined feature map again (or pass it)\n",
    "        #     combined_features_path = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\", f\"{DATASET_CHOICE}_combined_features.pt\") # Assuming you saved a combined one\n",
    "        #     if os.path.exists(combined_features_path):\n",
    "        #          all_features = torch.load(combined_features_path, map_location='cpu')\n",
    "        #          example_image_feature = all_features.get(example_sample_id)\n",
    "        #          if example_image_feature is not None:\n",
    "        #              logger.info(f\"\\n--- Running Example Prediction for ID: {example_sample_id} ---\")\n",
    "        #              prediction = predict_ensemble(\n",
    "        #                  model_paths=trained_model_paths,\n",
    "        #                  tokenizer=trained_tokenizer,\n",
    "        #                  label_encoder=trained_label_encoder,\n",
    "        #                  text=\"This is some example meme text.\",\n",
    "        #                  triples=\"Cause-Effect: text causes laughter\", # Optional example triples\n",
    "        #                  image_features=example_image_feature, # Provide the loaded features\n",
    "        #                  device=device,\n",
    "        #                  is_multilabel=(DATASET_CHOICE == 'depression'),\n",
    "        #                  base_model_name=BASE_TEXT_MODEL_NAME,\n",
    "        #                  visual_feature_dim=VISUAL_FEATURE_DIM,\n",
    "        #                  num_labels=len(trained_label_encoder.classes_)\n",
    "        #              )\n",
    "        #              logger.info(\"Example Prediction Result:\")\n",
    "        #              print(json.dumps(prediction, indent=2))\n",
    "        #          else:\n",
    "        #              logger.warning(f\"Could not find image features for example ID: {example_sample_id}\")\n",
    "        #     else:\n",
    "        #         logger.warning(\"Could not load combined features map for example prediction.\")\n",
    "\n",
    "        # except Exception as pred_e:\n",
    "        #     logger.error(f\"Error running example prediction: {pred_e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        logger.error(f\"Pipeline for '{DATASET_CHOICE}' did not complete successfully. Check logs for errors.\")\n",
    "        if not trained_model_paths: logger.error(\"  - No models were successfully trained.\")\n",
    "        if not trained_tokenizer: logger.error(\"  - Tokenizer was not loaded/returned.\")\n",
    "        if not trained_label_encoder: logger.error(\"  - Label encoder was not fitted/returned.\")\n",
    "\n",
    "\n",
    "except Exception as pipeline_e:\n",
    "    logger.error(f\"!!!!!! CATASTROPHIC ERROR in pipeline execution cell !!!!!!\")\n",
    "    logger.error(f\"Error Type: {type(pipeline_e).__name__}\")\n",
    "    logger.error(f\"Error Message: {pipeline_e}\")\n",
    "    logger.error(\"Traceback:\", exc_info=True)\n",
    "    # Save traceback to file\n",
    "    tb_main_path = os.path.join(KAGGLE_WORKING_DIR, f\"{DATASET_CHOICE}_pipeline_crash_traceback.txt\")\n",
    "    with open(tb_main_path, \"w\") as f:\n",
    "        traceback.print_exc(file=f)\n",
    "    logger.info(f\"Crash traceback saved to: {tb_main_path}\")\n",
    "\n",
    "\n",
    "# --- Final Cleanup ---\n",
    "logger.info(\"Performing final cleanup...\")\n",
    "# Explicitly delete large objects if they exist\n",
    "if 'final_eval_metrics' in locals(): del final_eval_metrics\n",
    "if 'trained_tokenizer' in locals(): del trained_tokenizer\n",
    "if 'trained_label_encoder' in locals(): del trained_label_encoder\n",
    "if 'trained_model_paths' in locals(): del trained_model_paths\n",
    "# Delete potentially large data splits if no longer needed\n",
    "if 'train_data' in locals(): del train_data\n",
    "if 'val_data' in locals(): del val_data\n",
    "if 'test_data' in locals(): del test_data\n",
    "if 'full_train_data' in locals(): del full_train_data\n",
    "if 'image_features_map' in locals(): del image_features_map\n",
    "\n",
    "gc.collect() # Run garbage collection\n",
    "if device == torch.device('cuda'):\n",
    "    torch.cuda.empty_cache() # Clear PyTorch CUDA cache\n",
    "    logger.info(\"CUDA cache cleared.\")\n",
    "    logger.info(f\"Final CUDA Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    logger.info(f\"Final CUDA Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "logger.info(f\"===== Execution cell finished for: {DATASET_CHOICE} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 15: Zip Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os # Ensure os is imported if running this cell independently\n",
    "\n",
    "# --- Determine paths based on the DATASET_CHOICE from the previous execution cell ---\n",
    "# This relies on DATASET_CHOICE being set correctly in Cell 14\n",
    "try:\n",
    "    # Check if DATASET_CHOICE exists and is one of the expected values\n",
    "    if 'DATASET_CHOICE' in locals() and DATASET_CHOICE in ['anxiety', 'depression']:\n",
    "        output_subdir = DATASET_CHOICE\n",
    "        logger.info(f\"Determined output subdirectory for zipping: '{output_subdir}'\")\n",
    "    else:\n",
    "        # Attempt to infer from directory structure if variable is missing\n",
    "        anxiety_path = \"/kaggle/working/anxiety/output/ensemble_fusion/\"\n",
    "        depression_path = \"/kaggle/working/depression/output/ensemble_fusion/\"\n",
    "        if os.path.exists(anxiety_path):\n",
    "             output_subdir = \"anxiety\"\n",
    "             logger.warning(\"DATASET_CHOICE variable not found or invalid. Inferred '{output_subdir}' from directory structure.\")\n",
    "        elif os.path.exists(depression_path):\n",
    "             output_subdir = \"depression\"\n",
    "             logger.warning(\"DATASET_CHOICE variable not found or invalid. Inferred '{output_subdir}' from directory structure.\")\n",
    "        else:\n",
    "             output_subdir = \"anxiety\" # Default if inference fails\n",
    "             logger.error(\"DATASET_CHOICE variable not found and cannot infer directory. Defaulting to '{output_subdir}'. Check Cell 14 execution and output paths.\")\n",
    "\n",
    "except NameError:\n",
    "    output_subdir = \"anxiety\" # Fallback if DATASET_CHOICE is not defined at all\n",
    "    logger.error(\"NameError: DATASET_CHOICE variable not defined. Defaulting to '{output_subdir}'. Please run Cell 14 first.\")\n",
    "\n",
    "\n",
    "# Define the source directory to zip (the base ensemble output dir)\n",
    "# This directory contains the run_* subfolders, reports, and label encoder\n",
    "source_path = f\"/kaggle/working/{output_subdir}/output/ensemble_fusion/\"\n",
    "zip_filename = f\"/kaggle/working/{output_subdir}_ensemble_fusion_output.zip\" # Changed filename for clarity\n",
    "\n",
    "print(f\"\\nAttempting to zip contents of: {source_path}\")\n",
    "print(f\"Creating archive named: {zip_filename}\")\n",
    "\n",
    "# --- Create the zip archive ---\n",
    "if os.path.exists(source_path) and os.path.isdir(source_path):\n",
    "    # Check if the directory is empty\n",
    "    if not os.listdir(source_path):\n",
    "        print(f\"\\nWARNING: Source directory '{source_path}' is empty. Zip file will be created but empty.\")\n",
    "        # Still create an empty zip or skip? Let's create it for consistency.\n",
    "        # The command might fail on an empty dir depending on zip version, let's try anyway.\n",
    "        !zip -r -q {zip_filename} {source_path}* # Using * inside might handle empty dir better sometimes\n",
    "        # Check if zip was created\n",
    "        if os.path.exists(zip_filename):\n",
    "            print(f\"Empty zip archive created for empty source directory.\")\n",
    "        else:\n",
    "            print(f\"Failed to create zip archive (source directory might be truly empty or inaccessible).\")\n",
    "\n",
    "    else:\n",
    "        # Use -r for recursive, -q for quiet execution\n",
    "        # Zip the contents *inside* the source directory to avoid nested 'ensemble_fusion' folder in zip\n",
    "        # We cd into the parent dir, then zip the target dir\n",
    "        parent_dir = os.path.dirname(source_path.rstrip('/'))\n",
    "        target_dir_name = os.path.basename(source_path.rstrip('/'))\n",
    "        zip_command = f\"cd {parent_dir} && zip -r -q {zip_filename} {target_dir_name}\"\n",
    "        print(f\"Executing zip command: {zip_command}\")\n",
    "        !{zip_command}\n",
    "        # Check if zip was created\n",
    "        if os.path.exists(zip_filename):\n",
    "             print(f\"\\nZip process finished successfully for '{output_subdir}'.\")\n",
    "             print(f\"Archive created: {zip_filename}\")\n",
    "        else:\n",
    "             print(f\"\\nERROR: Zip command executed but archive not found at {zip_filename}. Check permissions or command output.\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nERROR: Source directory not found or is not a directory: {source_path}\")\n",
    "    print(\"Cannot create zip archive. Please ensure the pipeline ran correctly and produced output.\")\n",
    "\n",
    "# --- Verify by listing the working directory ---\n",
    "print(\"\\nContents of /kaggle/working/ (showing zip files and output directories):\")\n",
    "# Use ls with options: -l (long format), -h (human readable sizes), -t (sort by time, newest first)\n",
    "# Filter for zip files and the output directory for clarity\n",
    "!ls -lht /kaggle/working/ | grep -E '.zip$|anxiety$|depression$'\n",
    "print(\"\\nFull contents of /kaggle/working/:\")\n",
    "!ls -lht /kaggle/working/"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
