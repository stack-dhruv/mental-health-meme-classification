{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 1: Installs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T05:41:06.309298Z",
     "iopub.status.busy": "2025-04-13T05:41:06.308752Z",
     "iopub.status.idle": "2025-04-13T05:43:31.248561Z",
     "shell.execute_reply": "2025-04-13T05:43:31.247715Z",
     "shell.execute_reply.started": "2025-04-13T05:41:06.309270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upgrading pip...\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.0.1\n",
      "\n",
      "Clearing pip cache...\n",
      "Files removed: 6 (1.9 MB)\n",
      "\n",
      "Installing required packages...\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m150.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m339.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m271.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m393.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m323.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m367.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m449.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m313.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m343.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m290.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m314.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m257.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m253.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m278.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m344.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m264.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m247.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m440.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "datasets 3.5.0 requires tqdm>=4.66.3, but you have tqdm 4.66.2 which is incompatible.\n",
      "featuretools 1.31.0 requires tqdm>=4.66.3, but you have tqdm 4.66.2 which is incompatible.\n",
      "nilearn 0.11.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
      "nilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.2 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "ibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "ibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\n",
      "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
      "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.1.2 which is incompatible.\n",
      "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.1.2 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Package installation attempt finished.\n",
      "Successfully imported faiss after installation.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Installs (Revised to handle hash issues)\n",
    "\n",
    "# 1. Upgrade pip itself\n",
    "print(\"Upgrading pip...\")\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# 2. Clear pip's cache to remove potentially corrupted downloads\n",
    "print(\"\\nClearing pip cache...\")\n",
    "!pip cache purge\n",
    "\n",
    "# 3. Install necessary libraries without using the cache directory\n",
    "#    (--no-cache-dir forces fresh downloads and helps avoid hash mismatches)\n",
    "print(\"\\nInstalling required packages...\")\n",
    "!pip install -q --no-cache-dir \\\n",
    "    transformers==4.38.2 \\\n",
    "    sentence-transformers==2.7.0 \\\n",
    "    faiss-cpu==1.8.0 \\\n",
    "    torch==2.1.2 \\\n",
    "    accelerate==0.28.0 \\\n",
    "    scikit-learn==1.3.2 \\\n",
    "    pandas==2.1.4 \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    Pillow==10.2.0 \\\n",
    "    tqdm==4.66.2\n",
    "\n",
    "print(\"\\nPackage installation attempt finished.\")\n",
    "\n",
    "# Optional: Verify faiss installation by trying to import it here\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"Successfully imported faiss after installation.\")\n",
    "    # You might see the CUDA registration warnings here again, which is fine.\n",
    "except ImportError:\n",
    "    print(\"ERROR: Failed to import faiss even after installation attempt. Check install logs above.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during faiss import check: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 2: Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:43:31.250338Z",
     "iopub.status.busy": "2025-04-13T05:43:31.250098Z",
     "iopub.status.idle": "2025-04-13T05:43:38.126931Z",
     "shell.execute_reply": "2025-04-13T05:43:38.126400Z",
     "shell.execute_reply.started": "2025-04-13T05:43:31.250317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MultiheadAttention \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BartForSequenceClassification,\n",
    "    BartTokenizer,\n",
    "    BartModel, # Base BART model\n",
    "    CLIPProcessor, # For vision features\n",
    "    CLIPVisionModel, # For vision features\n",
    "    get_linear_schedule_with_warmup,\n",
    "    CLIPVisionConfig # To get visual feature dimension\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, hamming_loss, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pickle\n",
    "import gc # Garbage collector\n",
    "import torch.nn.functional as F # For sigmoid and softmax\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 3: Configuration and Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:43:38.127902Z",
     "iopub.status.busy": "2025-04-13T05:43:38.127569Z",
     "iopub.status.idle": "2025-04-13T05:43:38.521200Z",
     "shell.execute_reply": "2025-04-13T05:43:38.520667Z",
     "shell.execute_reply.started": "2025-04-13T05:43:38.127884Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f84baaf12be4dcc809c274926dabcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 3: Configuration and Seed (Updated)\n",
    "\n",
    "# --- Basic Hyperparameters ---\n",
    "MAX_LEN = 512          # Max length for BART tokenizer\n",
    "BATCH_SIZE = 8         # Adjust based on GPU memory (T4 likely needs 4 or 8)\n",
    "NUM_EPOCHS = 10        # Number of training epochs PER ensemble member\n",
    "LEARNING_RATE = 3e-5\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "WEIGHT_DECAY = 0.05\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# --- RAG Hyperparameters ---\n",
    "TEXT_EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
    "RETRIEVAL_K = 3          # Number of examples to retrieve for prompts\n",
    "\n",
    "# --- Vision Hyperparameters ---\n",
    "VISION_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "try:\n",
    "    vision_config = CLIPVisionConfig.from_pretrained(VISION_MODEL_NAME)\n",
    "    VISUAL_FEATURE_DIM = vision_config.projection_dim\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not load vision config for {VISION_MODEL_NAME}: {e}. Defaulting VISUAL_FEATURE_DIM to 768.\")\n",
    "    VISUAL_FEATURE_DIM = 768\n",
    "\n",
    "# --- Ensemble Hyperparameters ---\n",
    "NUM_ENSEMBLE_MODELS = 3 # Number of models to train in the ensemble\n",
    "BASE_SEED = 42           # Base seed for reproducibility\n",
    "\n",
    "# --- Classification Model ---\n",
    "# <<< CHANGE HERE: Use the MentalBART model identifier >>>\n",
    "BASE_TEXT_MODEL_NAME = \"facebook/bart-base\"\n",
    "# <<< END CHANGE >>>\n",
    "\n",
    "\n",
    "# --- Kaggle File Paths ---\n",
    "KAGGLE_INPUT_DIR = \"/kaggle/input/axiom-dataset\"\n",
    "KAGGLE_WORKING_DIR = \"/kaggle/working\"\n",
    "\n",
    "# --- Seed Function ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # torch.backends.cudnn.deterministic = True # Optional for reproducibility\n",
    "    # torch.backends.cudnn.benchmark = False   # Optional\n",
    "    logger.info(f\"Seed set to {seed}\")\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "logger.info(f\"Visual Feature Dimension set to: {VISUAL_FEATURE_DIM}\")\n",
    "# Log the base text model being used\n",
    "logger.info(f\"Base Text Model set to: {BASE_TEXT_MODEL_NAME}\")\n",
    "logger.info(f\"Text Embedding Model: {TEXT_EMBEDDING_MODEL}\")\n",
    "logger.info(f\"Vision Model: {VISION_MODEL_NAME}\")\n",
    "\n",
    "\n",
    "# Set initial seed\n",
    "set_seed(BASE_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 4: Visual Feature Extraction (Run Once)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:43:38.522802Z",
     "iopub.status.busy": "2025-04-13T05:43:38.522585Z",
     "iopub.status.idle": "2025-04-13T05:44:03.822430Z",
     "shell.execute_reply": "2025-04-13T05:44:03.821841Z",
     "shell.execute_reply.started": "2025-04-13T05:43:38.522784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 05:43:39.892125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744523020.081546      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744523020.133529      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c44be4450b4f5382b084a805b0df4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820205524b4c40f69a3b642cf75bd051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649dae459c8a4bc3847336da93ce6359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3d60e0cd124ebb8c7aa17a16fcf01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1af2bc582e478a801c4518adeb601f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25adbea0435d4398987c81d79f95f281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c9fa07ddeb4471ab79d66434520cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0717e62e738e4b7d95613109ffa36a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train images for anxiety:   0%|          | 0/2608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f0db5560d4451697b074eb56a903ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test images for anxiety:   0%|          | 0/652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5754114c21940db8eb51a55810c730e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train images for depression:   0%|          | 0/8814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ecd81e32864b67bfcf7febbf7b8af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test images for depression:   0%|          | 0/520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42312d860b2a470281a6aa3b64e248c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val images for depression:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === VISUAL FEATURE EXTRACTION ===\n",
    "# NOTE: This cell only needs to be run once if the features are saved.\n",
    "# If features (.pt files) already exist in /kaggle/working/visual_features,\n",
    "# you can skip running this cell on subsequent runs.\n",
    "\n",
    "def get_image_path(sample_id, dataset_base_path, json_path):\n",
    "    \"\"\"\n",
    "    Constructs the image path based on sample ID and base path.\n",
    "    Uses json_path to infer the split if prefix is ambiguous.\n",
    "    \"\"\"\n",
    "    split = None\n",
    "    # Try matching known prefixes first\n",
    "    if isinstance(sample_id, str):\n",
    "        if sample_id.startswith('TR-'): split = 'train'\n",
    "        elif sample_id.startswith('TE-'): split = 'test'\n",
    "        elif sample_id.startswith('VL-'): split = 'val' # Assuming VL- for validation if needed\n",
    "\n",
    "    # If split not determined by prefix, infer from the JSON file path\n",
    "    if not split:\n",
    "        if 'train.json' in json_path or 'anxiety_train.json' in json_path:\n",
    "            split = 'train'\n",
    "        elif 'test.json' in json_path or 'anxiety_test.json' in json_path:\n",
    "            split = 'test'\n",
    "        elif 'val.json' in json_path:\n",
    "             split = 'val'\n",
    "        else:\n",
    "            # Last resort - needs better logic if IDs aren't prefixed and file names don't help\n",
    "            logger.warning(f\"Cannot determine split (train/test/val) for sample_id: {sample_id} from path {json_path}. Assuming 'train'. Check logic.\")\n",
    "            split = 'train'\n",
    "\n",
    "    # Construct the image folder path based on dataset type and split\n",
    "    img_folder = None\n",
    "    parent_dir = os.path.dirname(dataset_base_path) # Go one level up from Anxiety_Data or Depressive_Data\n",
    "    if \"Anxiety_Data\" in dataset_base_path:\n",
    "        img_folder = os.path.join(parent_dir, f\"anxiety_{split}_image\")\n",
    "    elif \"Depressive_Data\" in dataset_base_path:\n",
    "        # Adjusted path structure for depression based on observed zip structure\n",
    "        # Check if Images/depressive_image/split exists\n",
    "        potential_path1 = os.path.join(parent_dir, \"Images\", \"depressive_image\", split)\n",
    "        if os.path.isdir(potential_path1):\n",
    "             img_folder = potential_path1\n",
    "        else:\n",
    "             # Fallback if the structure is different (e.g., directly under depressive_image)\n",
    "             potential_path2 = os.path.join(parent_dir, \"depressive_image\", split)\n",
    "             if os.path.isdir(potential_path2):\n",
    "                 img_folder = potential_path2\n",
    "             else:\n",
    "                 logger.error(f\"Cannot find depression image folder for split '{split}' near {parent_dir}\")\n",
    "                 return None\n",
    "    else:\n",
    "        logger.error(f\"Unknown dataset base path structure: {dataset_base_path}\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.isdir(img_folder):\n",
    "        logger.error(f\"Determined image folder does not exist: {img_folder}\")\n",
    "        return None\n",
    "\n",
    "    # Try common extensions\n",
    "    for ext in ['.jpeg', '.jpg', '.png']:\n",
    "        img_path = os.path.join(img_folder, f\"{sample_id}{ext}\")\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "\n",
    "    logger.debug(f\"Image file not found for {sample_id} in {img_folder} with common extensions (.jpeg, .jpg, .png).\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_and_save_features(dataset_name, split_name, json_path, dataset_base_path, processor, vision_model, device, output_file):\n",
    "    \"\"\"Extracts CLIP features for a given dataset split and saves them.\"\"\"\n",
    "    logger.info(f\"Extracting features for {dataset_name} - {split_name} from {json_path}...\")\n",
    "    features_map = {}\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"JSON file not found: {json_path}. Cannot extract features.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load JSON {json_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    missing_images = 0\n",
    "    vision_model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for sample in tqdm(data, desc=f\"Processing {split_name} images for {dataset_name}\"):\n",
    "            sample_id = sample.get('sample_id', sample.get('id')) # Handle both 'sample_id' and 'id' keys\n",
    "            if not sample_id:\n",
    "                logger.warning(\"Skipping sample with missing ID.\")\n",
    "                continue\n",
    "\n",
    "            # Pass the json_path to help get_image_path infer split if needed\n",
    "            image_path = get_image_path(sample_id, dataset_base_path, json_path)\n",
    "\n",
    "            if image_path and os.path.exists(image_path):\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "                    outputs = vision_model(**inputs)\n",
    "                    # Use pooler_output which is the CLS token's embedding after projection for CLIP\n",
    "                    # Squeeze to remove batch dimension (as we process one image at a time)\n",
    "                    features = outputs.pooler_output.squeeze().cpu() # (feature_dim,) -> Move to CPU before storing\n",
    "                    if features.shape[0] != VISUAL_FEATURE_DIM:\n",
    "                         logger.warning(f\"Extracted feature dim {features.shape[0]} != expected {VISUAL_FEATURE_DIM} for {sample_id}. Check CLIP model/config.\")\n",
    "                         # Handle dimension mismatch if necessary (e.g., skip, pad, project) - here we just warn\n",
    "                    features_map[sample_id] = features\n",
    "                except FileNotFoundError:\n",
    "                    # This case should be less likely now with the initial check\n",
    "                    logger.warning(f\"File not found during processing (should have been checked): {image_path}\")\n",
    "                    missing_images += 1\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to process image {image_path}: {e}\")\n",
    "                    missing_images += 1\n",
    "            else:\n",
    "                logger.debug(f\"Image path not found or invalid for sample_id: {sample_id}. Path sought: {image_path}\")\n",
    "                missing_images += 1\n",
    "\n",
    "    if missing_images > 0:\n",
    "        logger.warning(f\"Could not find or process {missing_images}/{len(data)} images for {dataset_name} - {split_name}.\")\n",
    "\n",
    "    if features_map:\n",
    "        logger.info(f\"Saving {len(features_map)} extracted features to {output_file}...\")\n",
    "        try:\n",
    "            torch.save(features_map, output_file)\n",
    "            logger.info(\"Features saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save features to {output_file}: {e}\")\n",
    "    else:\n",
    "        logger.warning(f\"No features were extracted for {dataset_name} - {split_name}. Output file not saved.\")\n",
    "\n",
    "# --- Load CLIP Model --- (Do this once)\n",
    "clip_processor = None\n",
    "clip_vision_model = None\n",
    "try:\n",
    "    logger.info(f\"Loading CLIP processor and vision model: {VISION_MODEL_NAME}\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(VISION_MODEL_NAME)\n",
    "    clip_vision_model = CLIPVisionModel.from_pretrained(VISION_MODEL_NAME).to(device)\n",
    "    clip_vision_model.eval() # Ensure model is in eval mode\n",
    "    logger.info(\"CLIP models loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load CLIP model or processor: {e}\", exc_info=True)\n",
    "    logger.error(\"Visual feature extraction cannot proceed.\")\n",
    "\n",
    "# --- Define Paths and Run Extraction (if models loaded) ---\n",
    "if clip_processor and clip_vision_model:\n",
    "    # Define base directories within the input dataset\n",
    "    anxiety_base_dir = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Anxiety_Data\")\n",
    "    depression_base_dir = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Depressive_Data\")\n",
    "\n",
    "    # Define the output directory for saving features\n",
    "    output_feature_dir = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\")\n",
    "    os.makedirs(output_feature_dir, exist_ok=True)\n",
    "    logger.info(f\"Visual features will be saved to: {output_feature_dir}\")\n",
    "\n",
    "    datasets_to_process = [\n",
    "        # Anxiety\n",
    "        (\"anxiety\", \"train\", os.path.join(anxiety_base_dir, \"anxiety_train.json\"), anxiety_base_dir, os.path.join(output_feature_dir, \"anxiety_train_features.pt\")),\n",
    "        (\"anxiety\", \"test\", os.path.join(anxiety_base_dir, \"anxiety_test.json\"), anxiety_base_dir, os.path.join(output_feature_dir, \"anxiety_test_features.pt\")),\n",
    "        # Depression\n",
    "        (\"depression\", \"train\", os.path.join(depression_base_dir, \"train.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_train_features.pt\")),\n",
    "        (\"depression\", \"test\", os.path.join(depression_base_dir, \"test.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_test_features.pt\")),\n",
    "        (\"depression\", \"val\", os.path.join(depression_base_dir, \"val.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_val_features.pt\")) # Add validation set\n",
    "    ]\n",
    "\n",
    "    for name, split, json_p, data_base_p, out_f in datasets_to_process:\n",
    "        # Only process if the corresponding JSON file exists\n",
    "        if os.path.exists(json_p):\n",
    "            # Check if features already exist before extracting\n",
    "            if not os.path.exists(out_f):\n",
    "                 logger.info(f\"Starting feature extraction for: {name} - {split}\")\n",
    "                 extract_and_save_features(name, split, json_p, data_base_p, clip_processor, clip_vision_model, device, out_f)\n",
    "                 gc.collect() # Collect garbage after processing each split\n",
    "                 if device == torch.device('cuda'):\n",
    "                     torch.cuda.empty_cache()\n",
    "            else:\n",
    "                 logger.info(f\"Visual features file already exists: {out_f}. Skipping extraction for {name} - {split}.\")\n",
    "        else:\n",
    "            logger.warning(f\"JSON file not found: {json_p}. Skipping feature extraction for {name} - {split}.\")\n",
    "\n",
    "    # Clean up vision model from memory after all extractions are done\n",
    "    logger.info(\"Visual feature extraction process finished. Cleaning up CLIP model...\")\n",
    "    del clip_vision_model\n",
    "    del clip_processor\n",
    "    gc.collect()\n",
    "    if device == torch.device('cuda'):\n",
    "        torch.cuda.empty_cache()\n",
    "    logger.info(\"CLIP model cleanup complete.\")\n",
    "else:\n",
    "    logger.error(\"CLIP model failed to load. Cannot extract visual features.\")\n",
    "    # Define dummy path so pipeline doesn't crash immediately, but loading will fail later\n",
    "    output_feature_dir = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\")\n",
    "    os.makedirs(output_feature_dir, exist_ok=True) # Still create the dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 5: Data Loading, Cleaning, Splitting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.823773Z",
     "iopub.status.busy": "2025-04-13T05:44:03.823321Z",
     "iopub.status.idle": "2025-04-13T05:44:03.849999Z",
     "shell.execute_reply": "2025-04-13T05:44:03.849397Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.823754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- load_data Function (handles image_id and label variations) ---\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads data from JSON, extracts relevant fields, handles label variations.\"\"\"\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "         logger.error(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "         # Optionally try to load line by line if it's jsonl\n",
    "         logger.info(f\"Attempting to load {file_path} as JSON Lines (.jsonl)\")\n",
    "         data = []\n",
    "         try:\n",
    "             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                 for line in f:\n",
    "                     try:\n",
    "                         data.append(json.loads(line))\n",
    "                     except json.JSONDecodeError:\n",
    "                         logger.warning(f\"Skipping invalid JSON line in {file_path}: {line.strip()}\")\n",
    "             if not data: return [] # If still empty after trying jsonl\n",
    "             logger.info(f\"Successfully loaded {len(data)} lines as JSON Lines.\")\n",
    "         except Exception as e_jsonl:\n",
    "             logger.error(f\"Failed to load as JSON Lines as well: {e_jsonl}\")\n",
    "             return []\n",
    "\n",
    "    is_anxiety = \"Anxiety_Data\" in file_path\n",
    "    filtered_data = []\n",
    "    processed_ids = set() # Keep track of processed IDs to avoid duplicates\n",
    "\n",
    "    for idx, sample in enumerate(data):\n",
    "        if not isinstance(sample, dict):\n",
    "            logger.warning(f\"Skipping non-dictionary item at index {idx} in {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Use 'sample_id' primarily, fallback to 'id', then None\n",
    "        sample_id = sample.get('sample_id', sample.get('id', None))\n",
    "\n",
    "        # Skip if ID is missing or already processed\n",
    "        if sample_id is None:\n",
    "            logger.warning(f\"Skipping sample at index {idx} due to missing ID.\")\n",
    "            continue\n",
    "        if sample_id in processed_ids:\n",
    "             logger.warning(f\"Skipping duplicate sample ID: {sample_id}\")\n",
    "             continue\n",
    "        processed_ids.add(sample_id)\n",
    "\n",
    "        # Ensure necessary fields exist\n",
    "        ocr_text = sample.get('ocr_text', None)\n",
    "        triples = sample.get('triples', \"\") # Default to empty string if missing\n",
    "\n",
    "        # Get image identifier (use sample_id if 'image_id' isn't explicitly present)\n",
    "        image_id = sample.get('image_id', sample_id)\n",
    "        if not image_id:\n",
    "             logger.warning(f\"Sample {sample_id} missing a valid image identifier. Using sample_id '{sample_id}'.\")\n",
    "             image_id = sample_id # Fallback for safety, though should have ID by now\n",
    "\n",
    "        # Add image_id to the sample dictionary\n",
    "        sample['image_id'] = image_id\n",
    "\n",
    "        # Process based on dataset type\n",
    "        if is_anxiety:\n",
    "            anxiety_label_key = 'meme_anxiety_category'\n",
    "            original_label = sample.get(anxiety_label_key, None)\n",
    "\n",
    "            if ocr_text is not None and original_label is not None:\n",
    "                 # Standardize known label variations\n",
    "                 if original_label == 'Irritatbily': original_label = 'Irritability'\n",
    "                 elif original_label == 'Unknown': original_label = 'Unknown Anxiety'\n",
    "\n",
    "                 sample['original_labels'] = original_label # Single label for anxiety\n",
    "                 sample['stratify_label'] = original_label # Use the single label for stratification\n",
    "                 sample['triples'] = triples # Ensure triples are included\n",
    "                 sample['ocr_text'] = ocr_text # Ensure ocr_text is included\n",
    "                 filtered_data.append(sample)\n",
    "            else:\n",
    "                 logger.warning(f\"Skipping anxiety sample {sample_id} due to missing 'ocr_text' or '{anxiety_label_key}'. OCR: {'Present' if ocr_text is not None else 'Missing'}, Label: {'Present' if original_label is not None else 'Missing'}\")\n",
    "\n",
    "        else: # Depression (multilabel)\n",
    "             depression_labels_key = 'meme_depressive_categories'\n",
    "             original_label_data = sample.get(depression_labels_key, None)\n",
    "             processed_labels = []\n",
    "\n",
    "             if ocr_text is not None and original_label_data is not None:\n",
    "                  # Handle different formats of labels (list, string, potentially comma-separated string)\n",
    "                  if isinstance(original_label_data, list):\n",
    "                      processed_labels = [str(lbl).strip() for lbl in original_label_data if str(lbl).strip()]\n",
    "                  elif isinstance(original_label_data, str):\n",
    "                      # Simple split by comma if it's a string, could be more robust\n",
    "                      processed_labels = [lbl.strip() for lbl in original_label_data.split(',') if lbl.strip()]\n",
    "                  else:\n",
    "                      logger.warning(f\"Unexpected label format for depression sample {sample_id}: {type(original_label_data)}. Treating as empty.\")\n",
    "                      processed_labels = []\n",
    "\n",
    "                  # Ensure 'Unknown Depression' is handled if needed, though seems less common\n",
    "                  processed_labels = [lbl if lbl != 'Unknown' else 'Unknown Depression' for lbl in processed_labels]\n",
    "\n",
    "                  if not processed_labels:\n",
    "                       logger.warning(f\"Depression sample {sample_id} resulted in empty label list after processing. Assigning 'Unknown Depression'. Original data: {original_label_data}\")\n",
    "                       processed_labels = [\"Unknown Depression\"] # Assign a default if empty\n",
    "\n",
    "                  sample['original_labels'] = processed_labels # List of labels for depression\n",
    "                  # Use the first label for stratification (or a default if list is somehow empty)\n",
    "                  sample['stratify_label'] = processed_labels[0] if processed_labels else \"Unknown Depression\"\n",
    "                  sample['triples'] = triples # Ensure triples are included\n",
    "                  sample['ocr_text'] = ocr_text # Ensure ocr_text is included\n",
    "                  filtered_data.append(sample)\n",
    "             else:\n",
    "                 logger.warning(f\"Skipping depression sample {sample_id} due to missing 'ocr_text' or '{depression_labels_key}'. OCR: {'Present' if ocr_text is not None else 'Missing'}, Label: {'Present' if original_label_data is not None else 'Missing'}\")\n",
    "\n",
    "    logger.info(f\"Loaded {len(filtered_data)} samples from {file_path} after filtering and processing.\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# --- clean_triples Function ---\n",
    "def clean_triples(triples_text):\n",
    "    \"\"\"Cleans the structured triples text, keeping section headers.\"\"\"\n",
    "    if pd.isna(triples_text) or not isinstance(triples_text, str) or not triples_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # Define the sections expected in the triples\n",
    "    sections = [\"Cause-Effect\", \"Figurative Understanding\", \"Mental State\"]\n",
    "    cleaned_parts = []\n",
    "\n",
    "    # Use regex to capture content for each section, handling potential missing sections\n",
    "    current_text = triples_text\n",
    "    found_any_section = False\n",
    "    for i, section in enumerate(sections):\n",
    "        # Regex to find section header and capture text until the next known header or end of string\n",
    "        # (?s) is equivalent to re.DOTALL flag\n",
    "        # Lazily match content with .*?\n",
    "        # Lookahead (?=...) ensures we stop before the next header or end of string ($)\n",
    "        next_headers_pattern = \"|\".join(f\"{re.escape(s)}\\\\s*:\" for s in sections[i+1:])\n",
    "        if next_headers_pattern:\n",
    "             pattern = rf\"(?s){re.escape(section)}\\s*:(.*?)(?=\\s*(?:{next_headers_pattern}|$))\"\n",
    "        else: # Last section\n",
    "             pattern = rf\"(?s){re.escape(section)}\\s*:(.*)\"\n",
    "\n",
    "        match = re.search(pattern, current_text, re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            content = match.group(1).strip()\n",
    "            # Further clean the content: remove excessive whitespace\n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            if content: # Only add if there's actual content\n",
    "                cleaned_parts.append(f\"{section}: {content}\")\n",
    "                found_any_section = True\n",
    "                # Reduce the search space for the next iteration (optional but can help)\n",
    "                # current_text = current_text[match.end():] # This might be too aggressive if order isn't guaranteed\n",
    "        else:\n",
    "             logger.debug(f\"Section '{section}' not found or empty in triples: {triples_text[:100]}...\") # Log if a section is missed\n",
    "\n",
    "\n",
    "    # If no standard sections were found, return the original text after basic whitespace cleaning\n",
    "    if not found_any_section:\n",
    "         logger.debug(f\"No standard sections found in triples. Returning cleaned original text: {triples_text[:100]}...\")\n",
    "         return re.sub(r'\\s+', ' ', triples_text).strip()\n",
    "\n",
    "    # Join the cleaned parts with newlines\n",
    "    return \"\\n\".join(cleaned_parts).strip()\n",
    "\n",
    "\n",
    "# --- split_data Function (Handles stratification carefully) ---\n",
    "def split_data(data: List[Dict], val_size: float = 0.1, test_size: float = 0.2, random_state: int = 42) -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "    \"\"\"Splits data into train, validation, and test sets with stratification.\"\"\"\n",
    "    if not data:\n",
    "        logger.error(\"Cannot split empty data list.\")\n",
    "        return [], [], []\n",
    "\n",
    "    n_samples = len(data)\n",
    "    logger.info(f\"Attempting to split {n_samples} samples. Val ratio: {val_size}, Test ratio: {test_size}\")\n",
    "\n",
    "    # Ensure ratios are valid\n",
    "    if val_size < 0 or val_size >= 1 or test_size < 0 or test_size >= 1 or (val_size + test_size >= 1):\n",
    "        logger.error(f\"Invalid split ratios: val={val_size}, test={test_size}. Ratios must be [0, 1) and sum < 1.\")\n",
    "        # Default behavior: return all data as train if ratios invalid\n",
    "        return data, [], []\n",
    "\n",
    "    try:\n",
    "        # Extract labels for stratification\n",
    "        stratify_labels = [d['stratify_label'] for d in data]\n",
    "    except KeyError:\n",
    "        logger.error(\"All samples must have a 'stratify_label' key for splitting.\")\n",
    "        return [], [], [] # Cannot proceed without labels\n",
    "\n",
    "    # Check for labels with only one sample, which breaks stratification\n",
    "    unique_labels, counts = np.unique(stratify_labels, return_counts=True)\n",
    "    labels_with_one_sample = unique_labels[counts == 1]\n",
    "\n",
    "    if len(labels_with_one_sample) > 0:\n",
    "        logger.warning(f\"Labels with only 1 sample found: {list(labels_with_one_sample)}. Stratification might be unstable or fail. Consider merging/removing.\")\n",
    "        # Proceeding anyway, but train_test_split might raise errors later if splits result in single-sample classes\n",
    "\n",
    "    train_data, val_data, test_data = [], [], []\n",
    "\n",
    "    # --- First Split: Separate Test Set (if test_size > 0) ---\n",
    "    if test_size > 0:\n",
    "        remaining_data = data\n",
    "        remaining_labels = stratify_labels\n",
    "        try:\n",
    "            logger.info(f\"Splitting off test set ({test_size * 100:.1f}%)...\")\n",
    "            train_val_indices, test_indices = train_test_split(\n",
    "                range(n_samples),\n",
    "                test_size=test_size,\n",
    "                random_state=random_state,\n",
    "                stratify=remaining_labels\n",
    "            )\n",
    "            train_val_data = [remaining_data[i] for i in train_val_indices]\n",
    "            test_data = [remaining_data[i] for i in test_indices]\n",
    "            train_val_labels = [remaining_labels[i] for i in train_val_indices]\n",
    "            logger.info(f\"Split complete: Train/Val pool = {len(train_val_data)}, Test = {len(test_data)}\")\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"Stratified split for test set failed: {e}. Falling back to non-stratified split for test set.\")\n",
    "            train_val_indices, test_indices = train_test_split(\n",
    "                range(n_samples),\n",
    "                test_size=test_size,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            train_val_data = [remaining_data[i] for i in train_val_indices]\n",
    "            test_data = [remaining_data[i] for i in test_indices]\n",
    "            # We don't need train_val_labels if the next split is non-stratified\n",
    "            train_val_labels = [stratify_labels[i] for i in train_val_indices] # Still get them in case next split works\n",
    "\n",
    "    else: # No test set needed, all data goes to train/val pool\n",
    "        logger.info(\"No test set requested (test_size=0). Using all data for train/val split.\")\n",
    "        train_val_data = data\n",
    "        train_val_labels = stratify_labels\n",
    "        test_data = []\n",
    "\n",
    "    # --- Second Split: Separate Validation Set from Train/Val Pool (if val_size > 0) ---\n",
    "    if val_size > 0 and len(train_val_data) > 0:\n",
    "        # Adjust val_size relative to the size of the train/val pool\n",
    "        if test_size > 0: # Need to adjust val_size because test set was removed\n",
    "             relative_val_size = val_size / (1.0 - test_size)\n",
    "        else: # No test set removed, val_size is already relative to the whole pool\n",
    "             relative_val_size = val_size\n",
    "\n",
    "        # Ensure relative_val_size is valid and meaningful\n",
    "        if relative_val_size <= 0 or relative_val_size >= 1:\n",
    "            logger.warning(f\"Calculated relative validation size ({relative_val_size:.3f}) is invalid or zero. Assigning all remaining data to train set.\")\n",
    "            train_data = train_val_data\n",
    "            val_data = []\n",
    "        elif len(train_val_data) < 2: # Cannot split if only 1 sample left\n",
    "             logger.warning(f\"Only {len(train_val_data)} sample(s) left for train/val split. Assigning all to train set.\")\n",
    "             train_data = train_val_data\n",
    "             val_data = []\n",
    "        else:\n",
    "            try:\n",
    "                logger.info(f\"Splitting off validation set ({relative_val_size * 100:.1f}% of remaining)...\")\n",
    "                train_indices, val_indices = train_test_split(\n",
    "                    range(len(train_val_data)),\n",
    "                    test_size=relative_val_size,\n",
    "                    random_state=random_state,\n",
    "                    stratify=train_val_labels\n",
    "                )\n",
    "                train_data = [train_val_data[i] for i in train_indices]\n",
    "                val_data = [train_val_data[i] for i in val_indices]\n",
    "                logger.info(f\"Split complete: Train = {len(train_data)}, Validation = {len(val_data)}\")\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"Stratified split for validation set failed: {e}. Falling back to non-stratified split for validation set.\")\n",
    "                # Check again if we have enough samples for non-stratified split\n",
    "                if len(train_val_data) >= 2:\n",
    "                    train_indices, val_indices = train_test_split(\n",
    "                        range(len(train_val_data)),\n",
    "                        test_size=relative_val_size,\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "                    train_data = [train_val_data[i] for i in train_indices]\n",
    "                    val_data = [train_val_data[i] for i in val_indices]\n",
    "                else: # Should not happen based on earlier check, but safeguard\n",
    "                    logger.warning(\"Cannot perform non-stratified split with < 2 samples. Assigning all to train.\")\n",
    "                    train_data = train_val_data\n",
    "                    val_data = []\n",
    "\n",
    "    else: # No validation set needed or train/val pool is empty\n",
    "        logger.info(\"No validation set requested (val_size=0) or train/val pool empty. Assigning remaining data to train set.\")\n",
    "        train_data = train_val_data # Assign whatever is left (could be empty)\n",
    "        val_data = []\n",
    "\n",
    "    # Final check on sizes\n",
    "    logger.info(f\"Final split sizes: Train={len(train_data)}, Validation={len(val_data)}, Test={len(test_data)}\")\n",
    "    if len(train_data) + len(val_data) + len(test_data) != n_samples:\n",
    "        logger.warning(\"Total samples after split do not match initial count. Check logic.\")\n",
    "\n",
    "    # Sanity check: Ensure no overlap between sets based on IDs\n",
    "    train_ids = {d['image_id'] for d in train_data}\n",
    "    val_ids = {d['image_id'] for d in val_data}\n",
    "    test_ids = {d['image_id'] for d in test_data}\n",
    "    if train_ids.intersection(val_ids): logger.error(\"Overlap detected between Train and Validation sets!\")\n",
    "    if train_ids.intersection(test_ids): logger.error(\"Overlap detected between Train and Test sets!\")\n",
    "    if val_ids.intersection(test_ids): logger.error(\"Overlap detected between Validation and Test sets!\")\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 6: RAG Components (Text-Based Embeddings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.851038Z",
     "iopub.status.busy": "2025-04-13T05:44:03.850749Z",
     "iopub.status.idle": "2025-04-13T05:44:03.875624Z",
     "shell.execute_reply": "2025-04-13T05:44:03.874901Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.851013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles generation of text embeddings using SentenceTransformer.\"\"\"\n",
    "    def __init__(self, model_name: str = TEXT_EMBEDDING_MODEL, device: Optional[str] = None):\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Initializing SentenceTransformer model: {model_name} on device: {self.device}\")\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name, device=self.device)\n",
    "            # Test encoding to get embedding dimension\n",
    "            test_emb = self.model.encode([\"test sentence\"])\n",
    "            self.embedding_dim = test_emb.shape[1]\n",
    "            logger.info(f\"SentenceTransformer model loaded successfully. Embedding dimension: {self.embedding_dim}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load SentenceTransformer model '{model_name}': {e}\", exc_info=True)\n",
    "            raise # Re-raise the exception to halt execution if model loading fails\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> Optional[np.ndarray]:\n",
    "        \"\"\"Generates embeddings for a list of texts.\"\"\"\n",
    "        if not texts:\n",
    "            logger.warning(\"Received empty list of texts for embedding generation.\")\n",
    "            return None\n",
    "        logger.info(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        try:\n",
    "            # Use tqdm for progress bar if list is large\n",
    "            show_progress = len(texts) > 1000\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=show_progress,\n",
    "                convert_to_numpy=True,\n",
    "                device=self.device\n",
    "            )\n",
    "            logger.info(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during embedding generation: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def generate_fused_embeddings(self, ocr_texts: List[str], triples_texts: List[str], batch_size: int = 32) -> Optional[np.ndarray]:\n",
    "        \"\"\"Generates embeddings for OCR and triples, then fuses them (e.g., concatenation).\"\"\"\n",
    "        logger.info(\"Generating fused text embeddings (OCR + Triples)...\")\n",
    "        if len(ocr_texts) != len(triples_texts):\n",
    "            logger.error(f\"Length mismatch between OCR texts ({len(ocr_texts)}) and Triples texts ({len(triples_texts)}). Cannot fuse.\")\n",
    "            return None\n",
    "\n",
    "        # Generate embeddings for OCR and Triples separately\n",
    "        # Handle cases where one or both might be empty lists correctly\n",
    "        ocr_embeddings = self.generate_embeddings(ocr_texts, batch_size=batch_size) if ocr_texts else None\n",
    "        triples_embeddings = self.generate_embeddings(triples_texts, batch_size=batch_size) if triples_texts else None\n",
    "\n",
    "        # Check if generation was successful\n",
    "        if ocr_embeddings is None and triples_embeddings is None:\n",
    "             logger.error(\"Both OCR and Triples embedding generation failed or produced None.\")\n",
    "             return None\n",
    "\n",
    "        # If one is missing, use zeros of the correct dimension\n",
    "        num_samples = len(ocr_texts) # Should be same as len(triples_texts)\n",
    "        if ocr_embeddings is None:\n",
    "             logger.warning(\"OCR embedding generation failed or list was empty. Using zero vectors.\")\n",
    "             ocr_embeddings = np.zeros((num_samples, self.embedding_dim), dtype=np.float32)\n",
    "        if triples_embeddings is None:\n",
    "             logger.warning(\"Triples embedding generation failed or list was empty. Using zero vectors.\")\n",
    "             triples_embeddings = np.zeros((num_samples, self.embedding_dim), dtype=np.float32)\n",
    "\n",
    "        # Normalize embeddings before concatenation (optional, but often good practice)\n",
    "        # Add small epsilon to avoid division by zero for zero vectors\n",
    "        epsilon = 1e-12\n",
    "        ocr_norm = np.linalg.norm(ocr_embeddings, axis=1, keepdims=True)\n",
    "        triples_norm = np.linalg.norm(triples_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "        ocr_normalized = np.divide(ocr_embeddings, ocr_norm + epsilon, out=np.zeros_like(ocr_embeddings), where=(ocr_norm + epsilon)!=0)\n",
    "        triples_normalized = np.divide(triples_embeddings, triples_norm + epsilon, out=np.zeros_like(triples_embeddings), where=(triples_norm + epsilon)!=0)\n",
    "\n",
    "\n",
    "        # Simple concatenation for fusion\n",
    "        fused_embeddings = np.concatenate([ocr_normalized, triples_normalized], axis=1)\n",
    "        logger.info(f\"Generated fused embeddings with shape: {fused_embeddings.shape}\")\n",
    "\n",
    "        # Clean up intermediate arrays\n",
    "        del ocr_embeddings, triples_embeddings, ocr_norm, triples_norm, ocr_normalized, triples_normalized\n",
    "        gc.collect()\n",
    "\n",
    "        return fused_embeddings\n",
    "\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles building and querying a FAISS index for text similarity retrieval.\"\"\"\n",
    "    def __init__(self, embeddings: Optional[np.ndarray], top_k: int = RETRIEVAL_K):\n",
    "        self.top_k = top_k\n",
    "        self.index = None\n",
    "        self.dimension = 0\n",
    "\n",
    "        if embeddings is not None and embeddings.size > 0:\n",
    "            # Ensure embeddings are float32 for FAISS\n",
    "            if embeddings.dtype != np.float32:\n",
    "                 logger.warning(f\"Embeddings dtype is {embeddings.dtype}, converting to float32 for FAISS.\")\n",
    "                 embeddings = embeddings.astype(np.float32)\n",
    "            self.build_index(embeddings)\n",
    "        else:\n",
    "            logger.warning(\"RAGRetriever initialized with no embeddings. Retrieval will not be possible.\")\n",
    "\n",
    "    def build_index(self, embeddings: np.ndarray):\n",
    "        \"\"\"Builds a FAISS index from the provided embeddings.\"\"\"\n",
    "        if embeddings is None or embeddings.shape[0] == 0:\n",
    "            logger.error(\"Cannot build FAISS index from empty or None embeddings.\")\n",
    "            return\n",
    "        if embeddings.ndim != 2:\n",
    "             logger.error(f\"Embeddings must be 2D (samples, dimension), but got shape {embeddings.shape}. Cannot build index.\")\n",
    "             return\n",
    "\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        n_samples = embeddings.shape[0]\n",
    "        logger.info(f\"Building FAISS IndexFlatL2 with dimension {self.dimension} for {n_samples} vectors.\")\n",
    "\n",
    "        try:\n",
    "            self.index = faiss.IndexFlatL2(self.dimension) # Using L2 distance (Euclidean)\n",
    "            self.index.add(embeddings)\n",
    "            logger.info(f\"FAISS index built successfully. Index size: {self.index.ntotal} vectors.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building FAISS index: {e}\", exc_info=True)\n",
    "            self.index = None # Ensure index is None if building failed\n",
    "\n",
    "    def retrieve_similar(self, query_embeddings: np.ndarray) -> Optional[np.ndarray]:\n",
    "        \"\"\"Retrieves indices of the top_k most similar items from the index.\"\"\"\n",
    "        if self.index is None:\n",
    "            logger.error(\"FAISS index is not built. Cannot retrieve similar items.\")\n",
    "            return None\n",
    "        if query_embeddings is None or query_embeddings.size == 0:\n",
    "            logger.warning(\"Received empty or None query embeddings for retrieval.\")\n",
    "            return None # Return None for empty query\n",
    "\n",
    "        # Ensure query is float32 and 2D\n",
    "        if query_embeddings.dtype != np.float32:\n",
    "             query_embeddings = query_embeddings.astype(np.float32)\n",
    "        if query_embeddings.ndim == 1:\n",
    "             query_embeddings = np.expand_dims(query_embeddings, axis=0) # Reshape (D,) to (1, D)\n",
    "\n",
    "        if query_embeddings.shape[1] != self.dimension:\n",
    "            logger.error(f\"Query embedding dimension ({query_embeddings.shape[1]}) does not match index dimension ({self.dimension}).\")\n",
    "            return None\n",
    "\n",
    "        # Determine number of neighbors to search for (k)\n",
    "        # We search for top_k + 1 because the query item itself might be in the index\n",
    "        # and we typically want to exclude self-retrieval for prompt construction.\n",
    "        k = min(self.top_k + 1, self.index.ntotal) # Cannot retrieve more neighbors than exist in index\n",
    "        if k == 0:\n",
    "            logger.warning(\"FAISS index is empty (ntotal=0). Cannot retrieve.\")\n",
    "            # Return an empty array structure consistent with multiple queries\n",
    "            return np.array([[] for _ in range(query_embeddings.shape[0])], dtype=int)\n",
    "\n",
    "\n",
    "        logger.info(f\"Searching for top {k} neighbors for {query_embeddings.shape[0]} query vectors...\")\n",
    "        try:\n",
    "            # search returns distances (D) and indices (I)\n",
    "            distances, indices = self.index.search(query_embeddings, k=k)\n",
    "            logger.info(f\"FAISS search completed. Found indices shape: {indices.shape}\")\n",
    "\n",
    "            # Optional: Exclude self-retrieval if necessary (depends on whether queries are from the indexed data)\n",
    "            # This requires knowing the original indices of the query embeddings if they are part of the training data.\n",
    "            # For simplicity here, we return the raw indices including potential self.\n",
    "            # The PromptConstructor might handle skipping the first result if needed.\n",
    "\n",
    "            return indices # Shape: (num_queries, k)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during FAISS search: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "\n",
    "class PromptConstructor:\n",
    "    \"\"\"Constructs prompts for the classification model, optionally including RAG examples.\"\"\"\n",
    "    def __init__(self, train_data: List[Dict], label_encoder: LabelEncoder):\n",
    "        self.train_data = train_data\n",
    "        self.label_encoder = label_encoder\n",
    "        try:\n",
    "             self.class_names = \", \".join(label_encoder.classes_)\n",
    "             logger.info(f\"PromptConstructor initialized with {len(train_data)} training examples. Target classes: {self.class_names}\")\n",
    "        except AttributeError:\n",
    "             logger.error(\"LabelEncoder does not seem to be fitted yet (no classes_ attribute).\")\n",
    "             self.class_names = \"ERROR_CLASSES_UNDEFINED\"\n",
    "\n",
    "\n",
    "    def construct_prompt(self, sample: Dict, retrieved_indices: Optional[List[int]] = None) -> str:\n",
    "        \"\"\"Constructs a classification prompt for a given sample, optionally adding similar examples.\"\"\"\n",
    "\n",
    "        # --- System/Task Instruction ---\n",
    "        system_instruction = (\n",
    "            f\"Perform { 'multilabel' if isinstance(sample.get('original_labels', None), list) else 'multiclass' } \"\n",
    "            f\"classification for the given meme's text and knowledge graph context.\\n\"\n",
    "            f\"Choose the most relevant category/categories from the following list: {self.class_names}.\\n\"\n",
    "            f\"{ 'Output all applicable labels separated by commas if multiple apply.' if isinstance(sample.get('original_labels', None), list) else 'Output only the single most likely label.' }\\n\\n\"\n",
    "        )\n",
    "        prompt = system_instruction\n",
    "\n",
    "        # --- Few-Shot Examples (RAG) ---\n",
    "        if retrieved_indices:\n",
    "            prompt += \"Here are some potentially similar examples:\\n\\n\"\n",
    "            num_added = 0\n",
    "            for idx in retrieved_indices:\n",
    "                # Ensure index is valid and avoid retrieving the sample itself (if applicable - assumes indices match train_data)\n",
    "                # Simple check: if the ID matches, skip. Assumes 'sample_id' or 'id' exists.\n",
    "                current_id = sample.get('sample_id', sample.get('id'))\n",
    "                try:\n",
    "                    retrieved_sample = self.train_data[idx]\n",
    "                    retrieved_id = retrieved_sample.get('sample_id', retrieved_sample.get('id'))\n",
    "\n",
    "                    if current_id is not None and current_id == retrieved_id:\n",
    "                        logger.debug(f\"Skipping self-retrieval for index {idx} (ID: {current_id})\")\n",
    "                        continue # Skip self\n",
    "\n",
    "                    ex_text = retrieved_sample.get(\"ocr_text\", \"N/A\")\n",
    "                    ex_triples = retrieved_sample.get(\"triples\", \"\") # Cleaned triples should be here\n",
    "                    ex_labels = retrieved_sample.get(\"original_labels\", \"N/A\")\n",
    "\n",
    "                    # Format labels nicely\n",
    "                    if isinstance(ex_labels, list):\n",
    "                        ex_label_str = \", \".join(ex_labels) if ex_labels else \"None\"\n",
    "                    else: # Should be single string for anxiety\n",
    "                        ex_label_str = str(ex_labels) if ex_labels is not None else \"N/A\"\n",
    "\n",
    "                    prompt += f\"--- Example {num_added + 1} ---\\n\"\n",
    "                    prompt += f\"Example Text: {ex_text}\\n\"\n",
    "                    if ex_triples:\n",
    "                        prompt += f\"Example Knowledge:\\n{ex_triples}\\n\" # Keep newline for readability\n",
    "                    prompt += f\"Example Category: {ex_label_str}\\n\\n\"\n",
    "                    num_added += 1\n",
    "                    if num_added >= RETRIEVAL_K: # Limit to K examples even if more retrieved\n",
    "                         break\n",
    "\n",
    "                except IndexError:\n",
    "                    logger.warning(f\"Retrieved index {idx} is out of bounds for train_data (size {len(self.train_data)}).\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing retrieved example at index {idx}: {e}\")\n",
    "\n",
    "            if num_added > 0:\n",
    "                 prompt += \"--- End of Examples ---\\n\\n\"\n",
    "            else:\n",
    "                 prompt += \"No similar examples found or provided.\\n\\n\"\n",
    "\n",
    "\n",
    "        # --- Current Sample to Classify ---\n",
    "        prompt += \"Now, classify the following meme:\\n\\n\"\n",
    "        current_text = sample.get('ocr_text', 'N/A')\n",
    "        current_triples = sample.get('triples', '') # Assumes triples are already cleaned\n",
    "\n",
    "        prompt += f\"Text: {current_text}\\n\"\n",
    "        if current_triples:\n",
    "            prompt += f\"Knowledge:\\n{current_triples}\\n\" # Keep newline\n",
    "        prompt += \"Category:\" # Model should predict what comes after this\n",
    "\n",
    "        return prompt.strip() # Remove any trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 7: Dataset Classes (Handles Image Features)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.876572Z",
     "iopub.status.busy": "2025-04-13T05:44:03.876385Z",
     "iopub.status.idle": "2025-04-13T05:44:03.896708Z",
     "shell.execute_reply": "2025-04-13T05:44:03.896016Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.876557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AnxietyDataset(Dataset):\n",
    "    \"\"\"Dataset for single-label Anxiety classification (multimodal).\"\"\"\n",
    "    def __init__(self,\n",
    "                 samples: List[Dict],\n",
    "                 prompts: List[str],\n",
    "                 tokenizer: BartTokenizer,\n",
    "                 max_len: int,\n",
    "                 label_encoder: LabelEncoder,\n",
    "                 image_features_map: Dict[str, torch.Tensor],\n",
    "                 visual_feature_dim: int = VISUAL_FEATURE_DIM):\n",
    "        self.samples = samples\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_encoder = label_encoder\n",
    "        self.image_features_map = image_features_map\n",
    "        self.visual_feature_dim = visual_feature_dim # Store expected dim\n",
    "        self.default_image_feature = torch.zeros(self.visual_feature_dim, dtype=torch.float) # Pre-create zero tensor\n",
    "\n",
    "        logger.info(f\"AnxietyDataset created with {len(samples)} samples.\")\n",
    "        if len(samples) != len(prompts):\n",
    "            logger.warning(f\"Mismatch between number of samples ({len(samples)}) and prompts ({len(prompts)}).\")\n",
    "\n",
    "        # Pre-verify image features for a few samples (optional)\n",
    "        missing_count = 0\n",
    "        for i in range(min(5, len(samples))):\n",
    "            sample_id = samples[i].get('image_id')\n",
    "            if sample_id not in image_features_map:\n",
    "                 missing_count += 1\n",
    "        if missing_count > 0:\n",
    "             logger.warning(f\"In first 5 samples, {missing_count} are missing pre-computed image features.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        prompt = self.prompts[idx]\n",
    "        label_str = sample.get(\"original_labels\", None) # Expecting single string label\n",
    "        image_id = sample.get(\"image_id\", None) # Get the image identifier\n",
    "\n",
    "        # Encode label\n",
    "        label_idx = -1 # Use -1 as placeholder for missing/unknown\n",
    "        if label_str is not None:\n",
    "            try:\n",
    "                label_idx = self.label_encoder.transform([label_str])[0]\n",
    "            except ValueError:\n",
    "                logger.error(f\"Label '{label_str}' in sample {idx} (ID: {sample.get('id', 'N/A')}) not found in LabelEncoder classes: {self.label_encoder.classes_}. Assigning index 0 (or handle differently).\")\n",
    "                # Decide how to handle unknown labels during training/eval\n",
    "                # Option 1: Assign a default index (e.g., 0)\n",
    "                label_idx = 0 # Or find an 'unknown' class index if available\n",
    "                # Option 2: Raise an error\n",
    "                # raise ValueError(f\"Unknown label encountered: {label_str}\")\n",
    "                # Option 3: Skip the sample (would require changes in DataLoader/training loop)\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Error encoding label '{label_str}' for sample {idx}: {e}\")\n",
    "                 label_idx = 0 # Fallback to 0\n",
    "\n",
    "        # Get image features\n",
    "        if image_id and image_id in self.image_features_map:\n",
    "            img_features = self.image_features_map[image_id]\n",
    "            # Ensure the loaded feature has the correct dimension\n",
    "            if img_features.shape[0] != self.visual_feature_dim:\n",
    "                 logger.warning(f\"Image feature for ID {image_id} has incorrect dimension {img_features.shape[0]}, expected {self.visual_feature_dim}. Using zeros.\")\n",
    "                 img_features = self.default_image_feature\n",
    "            # Ensure dtype is float\n",
    "            if img_features.dtype != torch.float:\n",
    "                 img_features = img_features.float()\n",
    "        else:\n",
    "            logger.debug(f\"Image features missing for image_id '{image_id}' in sample {idx}. Using default zero vector.\")\n",
    "            img_features = self.default_image_feature\n",
    "\n",
    "\n",
    "        # Tokenize prompt\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                prompt,\n",
    "                max_length=self.max_len,\n",
    "                padding=\"max_length\", # Pad to max_len\n",
    "                truncation=True,      # Truncate if longer\n",
    "                return_tensors=\"pt\"   # Return PyTorch tensors\n",
    "            )\n",
    "            # Squeeze to remove the batch dimension added by tokenizer\n",
    "            input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error tokenizing prompt for sample {idx} (ID: {sample.get('id', 'N/A')}): {e}\", exc_info=True)\n",
    "             # Return dummy data or raise error\n",
    "             input_ids = torch.zeros(self.max_len, dtype=torch.long)\n",
    "             attention_mask = torch.zeros(self.max_len, dtype=torch.long)\n",
    "             # Keep label_idx and img_features as potentially valid\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"image_features\": img_features, # Return the (potentially zero) image features\n",
    "            \"label\": torch.tensor(label_idx, dtype=torch.long) # CrossEntropyLoss expects long indices\n",
    "        }\n",
    "\n",
    "\n",
    "class DepressionDataset(Dataset):\n",
    "    \"\"\"Dataset for multi-label Depression classification (multimodal).\"\"\"\n",
    "    def __init__(self,\n",
    "                 samples: List[Dict],\n",
    "                 prompts: List[str],\n",
    "                 tokenizer: BartTokenizer,\n",
    "                 max_len: int,\n",
    "                 label_encoder: LabelEncoder,\n",
    "                 image_features_map: Dict[str, torch.Tensor],\n",
    "                 visual_feature_dim: int = VISUAL_FEATURE_DIM):\n",
    "        self.samples = samples\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_encoder = label_encoder\n",
    "        self.num_labels = len(label_encoder.classes_)\n",
    "        self.image_features_map = image_features_map\n",
    "        self.visual_feature_dim = visual_feature_dim\n",
    "        self.default_image_feature = torch.zeros(self.visual_feature_dim, dtype=torch.float)\n",
    "\n",
    "        logger.info(f\"DepressionDataset created with {len(samples)} samples and {self.num_labels} labels.\")\n",
    "        if len(samples) != len(prompts):\n",
    "            logger.warning(f\"Mismatch between number of samples ({len(samples)}) and prompts ({len(prompts)}).\")\n",
    "\n",
    "        # Pre-verify image features for a few samples (optional)\n",
    "        missing_count = 0\n",
    "        for i in range(min(5, len(samples))):\n",
    "            sample_id = samples[i].get('image_id')\n",
    "            if sample_id not in image_features_map:\n",
    "                 missing_count += 1\n",
    "        if missing_count > 0:\n",
    "             logger.warning(f\"In first 5 samples, {missing_count} are missing pre-computed image features.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        prompt = self.prompts[idx]\n",
    "        label_str_list = sample.get(\"original_labels\", []) # Expecting list of strings\n",
    "        image_id = sample.get(\"image_id\", None)\n",
    "\n",
    "        # Create multi-hot encoded label vector\n",
    "        multi_hot_label = torch.zeros(self.num_labels, dtype=torch.float) # Use float for BCEWithLogitsLoss\n",
    "        if not isinstance(label_str_list, list):\n",
    "             logger.warning(f\"Expected label_str_list to be a list for sample {idx}, but got {type(label_str_list)}. Treating as empty.\")\n",
    "             label_str_list = []\n",
    "\n",
    "        if not label_str_list:\n",
    "            logger.warning(f\"Sample {idx} (ID: {sample.get('id', 'N/A')}) has an empty label list.\")\n",
    "            # Decide how to handle samples with no labels (e.g., skip, assign 'Unknown')\n",
    "            # Here, we just leave the multi_hot_label as all zeros.\n",
    "        else:\n",
    "            for label_str in label_str_list:\n",
    "                try:\n",
    "                    label_idx = self.label_encoder.transform([label_str])[0]\n",
    "                    if 0 <= label_idx < self.num_labels:\n",
    "                        multi_hot_label[label_idx] = 1.0\n",
    "                    else:\n",
    "                         logger.error(f\"Label index {label_idx} out of bounds for label '{label_str}' in sample {idx}.\")\n",
    "                except ValueError:\n",
    "                    logger.error(f\"Label '{label_str}' in sample {idx} (ID: {sample.get('id', 'N/A')}) not found in LabelEncoder classes: {self.label_encoder.classes_}. Skipping this label.\")\n",
    "                except Exception as e:\n",
    "                     logger.error(f\"Error encoding label '{label_str}' for sample {idx}: {e}\")\n",
    "\n",
    "        # Get image features\n",
    "        if image_id and image_id in self.image_features_map:\n",
    "            img_features = self.image_features_map[image_id]\n",
    "            if img_features.shape[0] != self.visual_feature_dim:\n",
    "                 logger.warning(f\"Image feature for ID {image_id} has incorrect dimension {img_features.shape[0]}, expected {self.visual_feature_dim}. Using zeros.\")\n",
    "                 img_features = self.default_image_feature\n",
    "            if img_features.dtype != torch.float:\n",
    "                 img_features = img_features.float()\n",
    "        else:\n",
    "            logger.debug(f\"Image features missing for image_id '{image_id}' in sample {idx}. Using default zero vector.\")\n",
    "            img_features = self.default_image_feature\n",
    "\n",
    "        # Tokenize prompt\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                prompt,\n",
    "                max_length=self.max_len,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error tokenizing prompt for sample {idx} (ID: {sample.get('id', 'N/A')}): {e}\", exc_info=True)\n",
    "             input_ids = torch.zeros(self.max_len, dtype=torch.long)\n",
    "             attention_mask = torch.zeros(self.max_len, dtype=torch.long)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"image_features\": img_features, # Return the image features\n",
    "            \"label\": multi_hot_label # Return the multi-hot vector\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 8: Model Definition (Fusion Classifier)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.897566Z",
     "iopub.status.busy": "2025-04-13T05:44:03.897390Z",
     "iopub.status.idle": "2025-04-13T05:44:03.912137Z",
     "shell.execute_reply": "2025-04-13T05:44:03.911471Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.897545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 8: Model Definition (NEW Fusion Classifier with Cross-Attention Simulation - Refined Init)\n",
    "from torch.nn import MultiheadAttention # Ensure import\n",
    "\n",
    "class CrossAttentionModule(nn.Module):\n",
    "    \"\"\"A simplified module for bidirectional cross-attention.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.txt_q_vis_kv_attn = MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.vis_q_txt_kv_attn = MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        # Optional: Add FFN layers if needed\n",
    "        # self.ffn = nn.Sequential(...)\n",
    "\n",
    "    def forward(self, text_features, visual_feature):\n",
    "        # Text attends to Vision\n",
    "        txt_attn_output, _ = self.txt_q_vis_kv_attn(query=text_features, key=visual_feature, value=visual_feature)\n",
    "        attended_text = self.norm1(text_features + txt_attn_output)\n",
    "\n",
    "        # Vision attends to Text\n",
    "        vis_attn_output, _ = self.vis_q_txt_kv_attn(query=visual_feature, key=attended_text, value=attended_text)\n",
    "        attended_visual = self.norm2(visual_feature + vis_attn_output)\n",
    "\n",
    "        return attended_text, attended_visual\n",
    "\n",
    "\n",
    "class M3H_CrossAttentionClassifier(nn.Module):\n",
    "    \"\"\"Classifier using BART encoder, visual features, and cross-attention.\"\"\"\n",
    "    def __init__(self,\n",
    "                 num_labels: int,\n",
    "                 pretrained_model_name: str, # Removed default\n",
    "                 is_multilabel: bool = False,\n",
    "                 visual_feature_dim: int = 768, # Use default from config\n",
    "                 num_attention_heads: int = 8,\n",
    "                 dropout_prob: float = 0.1): # Use default from config\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.visual_feature_dim = visual_feature_dim\n",
    "\n",
    "        logger.info(f\"--- Initializing M3H_CrossAttentionClassifier ---\")\n",
    "        logger.info(f\"  Base Text Model: {pretrained_model_name}\")\n",
    "        logger.info(f\"  Task Type: {'Multi-Label' if is_multilabel else 'Single-Label'}\")\n",
    "        logger.info(f\"  Num Labels: {num_labels}\")\n",
    "        logger.info(f\"  Visual Dim (Input): {visual_feature_dim}\")\n",
    "        logger.info(f\"  Attention Heads: {num_attention_heads}\")\n",
    "        logger.info(f\"  Dropout: {dropout_prob}\")\n",
    "\n",
    "        try:\n",
    "            # Load BART base model FIRST to get hidden size\n",
    "            logger.info(\"  Loading BART base model...\")\n",
    "            self.bart = BartModel.from_pretrained(pretrained_model_name)\n",
    "            self.text_feature_dim = self.bart.config.hidden_size\n",
    "            logger.info(f\"  BART Loaded. Hidden Dim: {self.text_feature_dim}\")\n",
    "\n",
    "            # Define Visual projection\n",
    "            if self.visual_feature_dim != self.text_feature_dim:\n",
    "                logger.info(f\"  Adding visual projection layer: {visual_feature_dim} -> {self.text_feature_dim}\")\n",
    "                self.visual_projection = nn.Linear(self.visual_feature_dim, self.text_feature_dim)\n",
    "            else:\n",
    "                logger.info(\"  Visual dim matches BART. Using Identity projection.\")\n",
    "                self.visual_projection = nn.Identity()\n",
    "\n",
    "            # Define Cross-Attention Module\n",
    "            logger.info(\"  Defining CrossAttentionModule...\")\n",
    "            self.cross_attention = CrossAttentionModule(\n",
    "                embed_dim=self.text_feature_dim,\n",
    "                num_heads=num_attention_heads,\n",
    "                dropout=dropout_prob\n",
    "            )\n",
    "\n",
    "            # Define Classifier Head (Using only attended text CLS for simplicity first)\n",
    "            self.classifier_input_dim = self.text_feature_dim\n",
    "            logger.info(f\"  Defining Classifier Head (Input Dim: {self.classifier_input_dim})...\")\n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.classifier = nn.Linear(self.classifier_input_dim, num_labels)\n",
    "\n",
    "            # Define Loss Function\n",
    "            if self.is_multilabel:\n",
    "                self.loss_fct = nn.BCEWithLogitsLoss()\n",
    "                logger.info(\"  Using BCEWithLogitsLoss.\")\n",
    "            else:\n",
    "                self.loss_fct = nn.CrossEntropyLoss()\n",
    "                logger.info(\"  Using CrossEntropyLoss.\")\n",
    "\n",
    "            logger.info(\"--- M3H_CrossAttentionClassifier Initialized Successfully ---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing M3H_CrossAttentionClassifier: {e}\", exc_info=True)\n",
    "            raise # Re-raise to prevent proceeding with a broken model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image_features, labels=None):\n",
    "        # ... [Forward pass logic remains the same as previous Cell 8] ...\n",
    "        # 1. BART Encoder\n",
    "        encoder_outputs = self.bart.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        # 2. Project Visual & Reshape\n",
    "        projected_visual_features = self.visual_projection(image_features)\n",
    "        visual_features_seq = projected_visual_features.unsqueeze(1) # (B, 1, Dim)\n",
    "\n",
    "        # 3. Cross-Attention\n",
    "        attended_text, attended_visual = self.cross_attention(\n",
    "            text_features=encoder_outputs,\n",
    "            visual_feature=visual_features_seq\n",
    "        )\n",
    "\n",
    "        # 4. Pool & Dropout\n",
    "        pooled_output = attended_text[:, 0, :] # Use CLS token representation\n",
    "        final_representation = self.dropout(pooled_output)\n",
    "\n",
    "        # 5. Classify\n",
    "        logits = self.classifier(final_representation)\n",
    "\n",
    "        # 6. Loss Calculation\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                if self.is_multilabel:\n",
    "                    loss = self.loss_fct(logits, labels.float())\n",
    "                else:\n",
    "                    loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            except Exception as loss_e:\n",
    "                 logger.error(f\"Error calculating loss: {loss_e}. Logits shape: {logits.shape}, Labels shape: {labels.shape}, Is MultiLabel: {self.is_multilabel}\", exc_info=True)\n",
    "                 # Return logits but indicate loss calculation failed\n",
    "                 loss = None # Set loss to None if calculation fails\n",
    "\n",
    "\n",
    "        # Return simple output object\n",
    "        class SimpleOutput: pass\n",
    "        output = SimpleOutput()\n",
    "        output.loss = loss\n",
    "        output.logits = logits\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 9: Training Function (Trains ONE Model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.913268Z",
     "iopub.status.busy": "2025-04-13T05:44:03.913005Z",
     "iopub.status.idle": "2025-04-13T05:44:03.938820Z",
     "shell.execute_reply": "2025-04-13T05:44:03.938153Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.913252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 9: Training Function (Trains ONE Model)\n",
    "\n",
    "def train_and_evaluate(\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    output_dir: str, # Directory for saving models and logs for this specific run\n",
    "    label_encoder: LabelEncoder,\n",
    "    is_multilabel: bool\n",
    ") -> Tuple[Dict[str, List], str]:\n",
    "    \"\"\"Trains and evaluates a single model instance.\"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Starting training run. Output directory: {output_dir}\")\n",
    "    logger.info(f\"Task Type: {'Multi-Label' if is_multilabel else 'Single-Label (Multiclass)'}\")\n",
    "\n",
    "    best_val_metric = 0.0 # Use Macro F1 for single-label, Micro F1 for multi-label? Or choose one consistently.\n",
    "    best_val_f1_macro = 0.0 # Track best macro F1 specifically for model saving\n",
    "    best_model_path = os.path.join(output_dir, \"best_model_macro_f1.pt\")\n",
    "    last_model_path = os.path.join(output_dir, \"last_model.pt\")\n",
    "    log_file = os.path.join(output_dir, \"training_log.csv\")\n",
    "    training_logs = [] # Store logs for saving to CSV\n",
    "    history = defaultdict(list) # Store metrics history\n",
    "\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    logger.info(f\"Total training steps: {total_steps} ({len(train_dataloader)} steps/epoch * {num_epochs} epochs)\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_num = epoch + 1\n",
    "            logger.info(f\"--- Epoch {epoch_num}/{num_epochs} ---\")\n",
    "\n",
    "            # --- Training Phase ---\n",
    "            model.train() # Set model to training mode\n",
    "            total_train_loss = 0.0\n",
    "            all_train_logits = []\n",
    "            all_train_labels_raw = [] # Store raw labels from dataloader\n",
    "\n",
    "            train_progress = tqdm(train_dataloader, desc=f\"Train Epoch {epoch_num}\", leave=False)\n",
    "            for batch_idx, batch in enumerate(train_progress):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Move batch to device\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                image_features = batch[\"image_features\"].to(device) # Get image features\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                try:\n",
    "                    # Forward pass\n",
    "                    outputs = model(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    image_features=image_features, # Pass image features\n",
    "                                    labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    if loss is None:\n",
    "                        logger.error(f\"Epoch {epoch_num}, Batch {batch_idx}: Loss is None. Check model forward pass and loss calculation.\")\n",
    "                        continue # Skip batch if loss is None\n",
    "\n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    # Gradient clipping (optional but recommended)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step() # Update learning rate\n",
    "\n",
    "                    total_train_loss += loss.item()\n",
    "\n",
    "                    # Store logits and labels for epoch metrics calculation\n",
    "                    all_train_logits.append(logits.detach().cpu())\n",
    "                    all_train_labels_raw.append(labels.cpu())\n",
    "\n",
    "                    # Update progress bar\n",
    "                    train_progress.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during training batch {batch_idx} in epoch {epoch_num}: {e}\", exc_info=True)\n",
    "                    # Decide whether to continue or stop training\n",
    "                    # For now, just log and continue to the next batch\n",
    "                    continue\n",
    "\n",
    "            # Calculate average training loss for the epoch\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0.0\n",
    "\n",
    "            # Calculate training metrics for the epoch\n",
    "            if not all_train_logits or not all_train_labels_raw:\n",
    "                 logger.warning(f\"Epoch {epoch_num}: No training logits or labels collected. Skipping training metrics calculation.\")\n",
    "                 train_f1_micro, train_f1_macro, train_f1_weighted = 0.0, 0.0, 0.0\n",
    "            else:\n",
    "                all_train_logits_cat = torch.cat(all_train_logits, dim=0)\n",
    "                all_train_labels_raw_cat = torch.cat(all_train_labels_raw, dim=0)\n",
    "\n",
    "                if is_multilabel:\n",
    "                    train_probs = torch.sigmoid(all_train_logits_cat).numpy()\n",
    "                    train_preds = (train_probs > 0.5).astype(int)\n",
    "                    train_labels = all_train_labels_raw_cat.numpy().astype(int)\n",
    "                    train_f1_micro = f1_score(train_labels, train_preds, average=\"micro\", zero_division=0)\n",
    "                    train_f1_macro = f1_score(train_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "                    train_f1_weighted = f1_score(train_labels, train_preds, average=\"weighted\", zero_division=0)\n",
    "                    train_accuracy = accuracy_score(train_labels, train_preds) # Subset accuracy\n",
    "                    train_hamming = hamming_loss(train_labels, train_preds)\n",
    "                    logger.info(f\"Epoch {epoch_num} Train - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}, Hamming: {train_hamming:.4f}, MicroF1: {train_f1_micro:.4f}, MacroF1: {train_f1_macro:.4f}\")\n",
    "                else: # Single-label\n",
    "                    train_preds = torch.argmax(all_train_logits_cat, dim=1).numpy()\n",
    "                    train_labels = all_train_labels_raw_cat.numpy()\n",
    "                    train_f1_micro = f1_score(train_labels, train_preds, average=\"micro\", zero_division=0)\n",
    "                    train_f1_macro = f1_score(train_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "                    train_f1_weighted = f1_score(train_labels, train_preds, average=\"weighted\", zero_division=0)\n",
    "                    train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "                    train_hamming = 0.0 # Not typically used for single-label multiclass\n",
    "                    logger.info(f\"Epoch {epoch_num} Train - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}, MacroF1: {train_f1_macro:.4f}, WeightedF1: {train_f1_weighted:.4f}\")\n",
    "\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            total_val_loss = 0.0\n",
    "            all_val_logits = []\n",
    "            all_val_labels_raw = []\n",
    "\n",
    "            val_progress = tqdm(val_dataloader, desc=f\"Val Epoch {epoch_num}\", leave=False)\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for batch in val_progress:\n",
    "                    # Move batch to device\n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                    image_features = batch[\"image_features\"].to(device) # Get image features\n",
    "                    labels = batch[\"label\"].to(device)\n",
    "\n",
    "                    try:\n",
    "                        # Forward pass\n",
    "                        outputs = model(input_ids=input_ids,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        image_features=image_features, # Pass image features\n",
    "                                        labels=labels)\n",
    "                        loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "\n",
    "                        if loss is not None:\n",
    "                            total_val_loss += loss.item()\n",
    "\n",
    "                        # Store logits and labels for epoch metrics calculation\n",
    "                        all_val_logits.append(logits.cpu())\n",
    "                        all_val_labels_raw.append(labels.cpu())\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error during validation batch in epoch {epoch_num}: {e}\", exc_info=True)\n",
    "                        continue\n",
    "\n",
    "            # Calculate average validation loss for the epoch\n",
    "            avg_val_loss = total_val_loss / len(val_dataloader) if len(val_dataloader) > 0 else 0.0\n",
    "\n",
    "            # Calculate validation metrics for the epoch\n",
    "            if not all_val_logits or not all_val_labels_raw:\n",
    "                 logger.warning(f\"Epoch {epoch_num}: No validation logits or labels collected. Skipping validation metrics calculation.\")\n",
    "                 val_f1_micro, val_f1_macro, val_f1_weighted, val_accuracy, val_hamming = 0.0, 0.0, 0.0, 0.0, 1.0\n",
    "            else:\n",
    "                all_val_logits_cat = torch.cat(all_val_logits, dim=0)\n",
    "                all_val_labels_raw_cat = torch.cat(all_val_labels_raw, dim=0)\n",
    "\n",
    "                if is_multilabel:\n",
    "                    val_probs = torch.sigmoid(all_val_logits_cat).numpy()\n",
    "                    val_preds = (val_probs > 0.5).astype(int)\n",
    "                    val_labels = all_val_labels_raw_cat.numpy().astype(int)\n",
    "                    val_f1_micro = f1_score(val_labels, val_preds, average=\"micro\", zero_division=0)\n",
    "                    val_f1_macro = f1_score(val_labels, val_preds, average=\"macro\", zero_division=0)\n",
    "                    val_f1_weighted = f1_score(val_labels, val_preds, average=\"weighted\", zero_division=0)\n",
    "                    val_accuracy = accuracy_score(val_labels, val_preds) # Subset accuracy\n",
    "                    val_hamming = hamming_loss(val_labels, val_preds)\n",
    "                    logger.info(f\"Epoch {epoch_num} Val   - Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.4f}, Hamming: {val_hamming:.4f}, MicroF1: {val_f1_micro:.4f}, MacroF1: {val_f1_macro:.4f}\")\n",
    "                else: # Single-label\n",
    "                    val_preds = torch.argmax(all_val_logits_cat, dim=1).numpy()\n",
    "                    val_labels = all_val_labels_raw_cat.numpy()\n",
    "                    val_f1_micro = f1_score(val_labels, val_preds, average=\"micro\", zero_division=0)\n",
    "                    val_f1_macro = f1_score(val_labels, val_preds, average=\"macro\", zero_division=0)\n",
    "                    val_f1_weighted = f1_score(val_labels, val_preds, average=\"weighted\", zero_division=0)\n",
    "                    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "                    val_hamming = 0.0 # Not applicable\n",
    "                    logger.info(f\"Epoch {epoch_num} Val   - Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.4f}, MacroF1: {val_f1_macro:.4f}, WeightedF1: {val_f1_weighted:.4f}\")\n",
    "\n",
    "\n",
    "            # --- Logging and Saving ---\n",
    "            # Append metrics to history\n",
    "            history['epoch'].append(epoch_num)\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['train_f1_macro'].append(train_f1_macro)\n",
    "            history['val_f1_macro'].append(val_f1_macro)\n",
    "            history['train_f1_micro'].append(train_f1_micro)\n",
    "            history['val_f1_micro'].append(val_f1_micro)\n",
    "            history['train_f1_weighted'].append(train_f1_weighted)\n",
    "            history['val_f1_weighted'].append(val_f1_weighted)\n",
    "            history['train_accuracy'].append(train_accuracy)\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            history['train_hamming'].append(train_hamming)\n",
    "            history['val_hamming'].append(val_hamming)\n",
    "            history['learning_rate'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "            # Store log entry for CSV\n",
    "            current_log = {\n",
    "                'epoch': epoch_num,\n",
    "                'train_loss': avg_train_loss, 'val_loss': avg_val_loss,\n",
    "                'train_f1_macro': train_f1_macro, 'val_f1_macro': val_f1_macro,\n",
    "                'train_f1_micro': train_f1_micro, 'val_f1_micro': val_f1_micro,\n",
    "                'train_f1_weighted': train_f1_weighted, 'val_f1_weighted': val_f1_weighted,\n",
    "                'train_accuracy': train_accuracy, 'val_accuracy': val_accuracy,\n",
    "                'train_hamming': train_hamming, 'val_hamming': val_hamming,\n",
    "                'learning_rate': scheduler.get_last_lr()[0]\n",
    "            }\n",
    "            training_logs.append(current_log)\n",
    "\n",
    "            # Save the best model based on validation macro F1\n",
    "            if val_f1_macro > best_val_f1_macro:\n",
    "                best_val_f1_macro = val_f1_macro\n",
    "                logger.info(f\"Epoch {epoch_num}: New best validation Macro F1: {best_val_f1_macro:.4f}. Saving model to {best_model_path}\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                 logger.info(f\"Epoch {epoch_num}: Validation Macro F1 ({val_f1_macro:.4f}) did not improve from best ({best_val_f1_macro:.4f}).\")\n",
    "\n",
    "\n",
    "            # Save the model from the last epoch\n",
    "            torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "            # Save logs to CSV\n",
    "            pd.DataFrame(training_logs).to_csv(log_file, index=False)\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del all_train_logits, all_train_labels_raw, all_val_logits, all_val_labels_raw\n",
    "            del all_train_logits_cat, all_train_labels_raw_cat # Explicitly delete concatenated tensors\n",
    "            if 'all_val_logits_cat' in locals(): del all_val_logits_cat\n",
    "            if 'all_val_labels_raw_cat' in locals(): del all_val_labels_raw_cat\n",
    "            gc.collect()\n",
    "            if device == torch.device('cuda'):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        logger.info(f\"Training finished after {num_epochs} epochs.\")\n",
    "        logger.info(f\"Best Validation Macro F1 achieved: {best_val_f1_macro:.4f}\")\n",
    "        logger.info(f\"Best model saved to: {best_model_path}\")\n",
    "        logger.info(f\"Last model saved to: {last_model_path}\")\n",
    "        logger.info(f\"Training logs saved to: {log_file}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "         logger.warning(\"Training interrupted by user (KeyboardInterrupt).\")\n",
    "         # Save current state if interrupted\n",
    "         torch.save(model.state_dict(), last_model_path)\n",
    "         pd.DataFrame(training_logs).to_csv(log_file, index=False)\n",
    "         logger.info(f\"Saved last model state to {last_model_path} and logs to {log_file}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during the training loop: {e}\", exc_info=True)\n",
    "        # Save logs even if error occurs\n",
    "        pd.DataFrame(training_logs).to_csv(log_file, index=False)\n",
    "        raise # Re-raise the exception after logging\n",
    "\n",
    "    return history, best_model_path # Return history and path to the best model found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 10: Plotting Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.941225Z",
     "iopub.status.busy": "2025-04-13T05:44:03.940968Z",
     "iopub.status.idle": "2025-04-13T05:44:03.955565Z",
     "shell.execute_reply": "2025-04-13T05:44:03.954852Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.941210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history: Dict[str, List], output_dir: str, suffix: str = \"\"):\n",
    "    \"\"\"Plots training and validation metrics stored in the history dictionary.\"\"\"\n",
    "    if not history or not isinstance(history, dict):\n",
    "        logger.warning(\"Cannot plot history: Invalid or empty history dictionary provided.\")\n",
    "        return\n",
    "\n",
    "    # Check if essential keys exist and have data\n",
    "    required_keys = ['train_loss', 'val_loss', 'train_f1_macro', 'val_f1_macro']\n",
    "    if not all(key in history and history[key] for key in required_keys):\n",
    "         logger.warning(f\"History dict missing essential data ({required_keys}). Cannot generate plots.\")\n",
    "         # Log available keys for debugging:\n",
    "         logger.debug(f\"Available keys in history: {list(history.keys())}\")\n",
    "         # Try plotting available metrics anyway\n",
    "         # return # Or uncomment to stop if essential plots can't be made\n",
    "\n",
    "    num_epochs = len(history.get('epoch', history.get('train_loss', []))) # Use epoch key if available, else infer from loss\n",
    "    if num_epochs == 0:\n",
    "        logger.warning(\"History contains no epochs to plot.\")\n",
    "        return\n",
    "\n",
    "    epochs_range = history.get('epoch', range(1, num_epochs + 1)) # Use actual epoch numbers if available\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid') # Use a clean style\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12)) # Create 2x2 grid of subplots\n",
    "    fig.suptitle(f'Training History {suffix}'.strip(), fontsize=16)\n",
    "\n",
    "    # --- Plot 1: Loss ---\n",
    "    ax = axs[0, 0]\n",
    "    if 'train_loss' in history and history['train_loss']:\n",
    "        ax.plot(epochs_range, history['train_loss'], 'o-', label='Train Loss', color='royalblue')\n",
    "    if 'val_loss' in history and history['val_loss']:\n",
    "        ax.plot(epochs_range, history['val_loss'], 'o-', label='Validation Loss', color='darkorange')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    if ax.has_data(): ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Plot 2: Macro F1 Score ---\n",
    "    ax = axs[0, 1]\n",
    "    if 'train_f1_macro' in history and history['train_f1_macro']:\n",
    "        ax.plot(epochs_range, history['train_f1_macro'], 'o-', label='Train Macro F1', color='royalblue')\n",
    "    if 'val_f1_macro' in history and history['val_f1_macro']:\n",
    "        ax.plot(epochs_range, history['val_f1_macro'], 'o-', label='Validation Macro F1', color='darkorange')\n",
    "    ax.set_title('Macro F1 Score')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_ylim(bottom=0, top=1.05) # F1 score between 0 and 1\n",
    "    if ax.has_data(): ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Plot 3: Weighted F1 Score ---\n",
    "    ax = axs[1, 0]\n",
    "    if 'train_f1_weighted' in history and history['train_f1_weighted']:\n",
    "        ax.plot(epochs_range, history['train_f1_weighted'], 'o-', label='Train Weighted F1', color='royalblue')\n",
    "    if 'val_f1_weighted' in history and history['val_f1_weighted']:\n",
    "        ax.plot(epochs_range, history['val_f1_weighted'], 'o-', label='Validation Weighted F1', color='darkorange')\n",
    "    ax.set_title('Weighted F1 Score')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_ylim(bottom=0, top=1.05)\n",
    "    if ax.has_data(): ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Plot 4: Validation Accuracy & Hamming Loss ---\n",
    "    ax = axs[1, 1]\n",
    "    plot4_has_data = False\n",
    "    if 'val_accuracy' in history and history['val_accuracy']:\n",
    "         ax.plot(epochs_range, history['val_accuracy'], 'o-', label='Validation Accuracy', color='forestgreen')\n",
    "         plot4_has_data = True\n",
    "    if 'val_hamming' in history and history['val_hamming']:\n",
    "         # Only plot Hamming if it's meaningful (e.g., > 0 for multilabel)\n",
    "         if any(h > 0 for h in history['val_hamming']):\n",
    "             ax.plot(epochs_range, history['val_hamming'], 'o-', label='Validation Hamming Loss', color='crimson')\n",
    "             plot4_has_data = True\n",
    "         else:\n",
    "              logger.info(\"Skipping Hamming Loss plot as values seem to be zero (likely single-label).\")\n",
    "\n",
    "    ax.set_title('Validation Accuracy / Hamming Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_ylim(bottom=-0.05, top=1.05) # Metrics typically between 0 and 1\n",
    "    if plot4_has_data: ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "    # --- Save and Show Plot ---\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust layout to prevent title overlap\n",
    "    plot_filename = f'training_history{suffix}.png'\n",
    "    plot_path = os.path.join(output_dir, plot_filename)\n",
    "    try:\n",
    "        plt.savefig(plot_path, dpi=300) # Save with higher resolution\n",
    "        logger.info(f\"Training history plot saved to: {plot_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save training plot to {plot_path}: {e}\")\n",
    "\n",
    "    plt.show() # Display the plot in the notebook\n",
    "    plt.close(fig) # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 11: Evaluation Function (Ensemble)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.957131Z",
     "iopub.status.busy": "2025-04-13T05:44:03.956415Z",
     "iopub.status.idle": "2025-04-13T05:44:03.980456Z",
     "shell.execute_reply": "2025-04-13T05:44:03.979784Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.957105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 11: Evaluation Function (Ensemble - Corrected Syntax)\n",
    "\n",
    "def evaluate_ensemble(\n",
    "    model_paths: List[str],\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    label_encoder: LabelEncoder,\n",
    "    is_multilabel: bool,\n",
    "    num_labels: int,\n",
    "    output_dir: str, # Base output directory for saving reports\n",
    "    report_suffix: str = \"eval_ensemble\", # e.g., \"validation_ensemble\", \"test_ensemble\"\n",
    "    base_model_name: str = BASE_TEXT_MODEL_NAME,\n",
    "    visual_feature_dim: int = VISUAL_FEATURE_DIM\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluates an ENSEMBLE of models on a given dataloader.\"\"\"\n",
    "    if not dataloader:\n",
    "        logger.warning(f\"Dataloader for '{report_suffix}' is None or empty. Skipping evaluation.\")\n",
    "        return {'error': 'No data provided'}\n",
    "    if not model_paths:\n",
    "        logger.error(\"No model paths provided for ensemble evaluation.\")\n",
    "        return {'error': 'No models provided'}\n",
    "\n",
    "    logger.info(f\"--- Starting ENSEMBLE Evaluation ({report_suffix}) ---\\n  Models: {len(model_paths)}, Samples: {len(dataloader.dataset)}\")\n",
    "    logger.info(f\"  Task Type: {'Multi-Label' if is_multilabel else 'Single-Label (Multiclass)'}\")\n",
    "\n",
    "    all_individual_model_logits = [] # Store logits from each model\n",
    "    all_labels_raw = None # Store true labels once\n",
    "\n",
    "    # --- Get predictions from each model ---\n",
    "    for model_idx, model_path in enumerate(model_paths):\n",
    "        logger.info(f\"Loading and evaluating model {model_idx + 1}/{len(model_paths)}: {os.path.basename(model_path)}\")\n",
    "\n",
    "        model = None # Ensure model is reset\n",
    "        try:\n",
    "            model = M3H_CrossAttentionClassifier(\n",
    "                num_labels=num_labels,\n",
    "                pretrained_model_name=base_model_name,\n",
    "                is_multilabel=is_multilabel,\n",
    "                visual_feature_dim=visual_feature_dim\n",
    "            )\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=True)\n",
    "                logger.debug(\"Model state loaded successfully (strict=True).\")\n",
    "            except RuntimeError as e:\n",
    "                logger.warning(f\"Strict state dict loading failed for model {model_idx+1}: {e}. Retrying with strict=False.\")\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=False)\n",
    "                logger.debug(\"Model state loaded with strict=False.\")\n",
    "\n",
    "            model.to(device)\n",
    "            model.eval() # Set to evaluation mode\n",
    "        except FileNotFoundError:\n",
    "             logger.error(f\"Model file not found: {model_path}. Skipping this model.\")\n",
    "             continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model {model_idx + 1} from {model_path}: {e}\", exc_info=True)\n",
    "            continue # Skip this model\n",
    "\n",
    "        # --- Collect logits and labels for this model ---\n",
    "        current_model_logits = []\n",
    "        batch_labels_list = [] # Only populated for model_idx == 0\n",
    "\n",
    "        eval_progress = tqdm(dataloader, desc=f\"Eval Model {model_idx + 1}\", leave=False, ncols=100)\n",
    "        with torch.no_grad(): # Disable gradients for evaluation\n",
    "            for batch_idx, batch in enumerate(eval_progress):\n",
    "                try:\n",
    "                    input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                    attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                    image_features = batch['image_features'].to(device, non_blocking=True)\n",
    "                    labels = batch['label'] # Keep labels on CPU initially for the first pass storage\n",
    "\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        image_features=image_features,\n",
    "                        labels=None # Don't need labels for forward pass here\n",
    "                    )\n",
    "                    current_model_logits.append(outputs.logits.cpu()) # Move logits to CPU\n",
    "\n",
    "                    # Store labels only during the first model's pass\n",
    "                    if model_idx == 0:\n",
    "                        batch_labels_list.append(labels) # Append CPU tensor\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during evaluation batch {batch_idx} for model {model_idx + 1}: {e}\", exc_info=True)\n",
    "                    # Continue to next batch if one fails\n",
    "\n",
    "        # Concatenate logits for the current model\n",
    "        if current_model_logits:\n",
    "            all_individual_model_logits.append(torch.cat(current_model_logits, dim=0))\n",
    "            logger.debug(f\"Logits collected for model {model_idx+1}. Shape: {all_individual_model_logits[-1].shape}\")\n",
    "        else:\n",
    "            logger.warning(f\"No logits collected for model {model_idx + 1}. It will be skipped in the ensemble average.\")\n",
    "\n",
    "        # Concatenate all labels after the first model's pass\n",
    "        if model_idx == 0 and batch_labels_list:\n",
    "            all_labels_raw = torch.cat(batch_labels_list, dim=0)\n",
    "            logger.debug(f\"Labels collected. Shape: {all_labels_raw.shape}\")\n",
    "        elif model_idx == 0 and not batch_labels_list:\n",
    "             logger.error(\"Failed to collect any labels during evaluation. Cannot calculate metrics.\")\n",
    "             return {'error': 'Label collection failed'}\n",
    "\n",
    "        # Clean up model memory\n",
    "        del model, current_model_logits, batch_labels_list, outputs\n",
    "        gc.collect()\n",
    "        if device == torch.device('cuda'):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # --- Aggregate predictions and calculate metrics ---\n",
    "    if not all_individual_model_logits or all_labels_raw is None:\n",
    "        logger.error(\"Evaluation failed: No valid logits collected from any model or labels are missing.\")\n",
    "        return {'error': 'Logit/Label collection failed'}\n",
    "\n",
    "    # Stack logits from all models: shape (num_valid_models, num_samples, num_labels)\n",
    "    try:\n",
    "        stacked_logits = torch.stack(all_individual_model_logits, dim=0)\n",
    "        logger.info(f\"Logits collected from {stacked_logits.shape[0]} successfully evaluated models. Stacked shape: {stacked_logits.shape}\")\n",
    "    except Exception as stack_e:\n",
    "        logger.error(f\"Error stacking logits (check if shapes are consistent): {stack_e}\", exc_info=True)\n",
    "        return {'error': 'Logit stacking failed'}\n",
    "\n",
    "\n",
    "    # Average logits across models: shape (num_samples, num_labels)\n",
    "    avg_logits = torch.mean(stacked_logits, dim=0)\n",
    "\n",
    "    # --- Calculate Metrics based on Task Type ---\n",
    "    metrics_dict = {}\n",
    "    classification_rep = \"Classification report generation failed.\"\n",
    "    confusion_matrix_report = \"Confusion matrix generation failed.\"\n",
    "\n",
    "    try:\n",
    "        if is_multilabel:\n",
    "            logger.info(f\"Calculating multi-label metrics for {report_suffix} (Ensemble)...\")\n",
    "            probs = torch.sigmoid(avg_logits).numpy()\n",
    "            preds = (probs > 0.5).astype(int) # Apply threshold\n",
    "            labels = all_labels_raw.numpy().astype(int) # Ground truth multi-hot\n",
    "\n",
    "            accuracy = accuracy_score(labels, preds) # Subset accuracy\n",
    "            hamming = hamming_loss(labels, preds)\n",
    "            f1_micro = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "            f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "            f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "            f1_samples = f1_score(labels, preds, average='samples', zero_division=0)\n",
    "\n",
    "            metrics_dict = {\n",
    "                'accuracy_subset': accuracy, 'hamming_loss': hamming,\n",
    "                'micro_f1': f1_micro, 'macro_f1': f1_macro,\n",
    "                'weighted_f1': f1_weighted, 'samples_f1': f1_samples\n",
    "            }\n",
    "\n",
    "            # Generate classification report\n",
    "            try:\n",
    "                 classification_rep = classification_report(\n",
    "                     labels, preds,\n",
    "                     target_names=label_encoder.classes_,\n",
    "                     digits=4, zero_division=0\n",
    "                 )\n",
    "            except Exception as cr_e:\n",
    "                 logger.error(f\"Error generating classification report: {cr_e}\")\n",
    "                 classification_rep = f\"Error generating classification report: {cr_e}\"\n",
    "\n",
    "            # Generate multi-label confusion matrix report\n",
    "            try:\n",
    "                 cm = multilabel_confusion_matrix(labels, preds)\n",
    "                 # <<< FIXED LINE CONTINUATION ERROR HERE >>>\n",
    "                 cm_report_path = os.path.join(output_dir, f\"confusion_matrix_{report_suffix}.txt\")\n",
    "                 # <<< END FIX >>>\n",
    "                 cm_lines = [f\"--- Multilabel Confusion Matrices ({report_suffix}) ---\"]\n",
    "                 for i, label_name in enumerate(label_encoder.classes_):\n",
    "                     cm_lines.append(f\"\\nLabel: {label_name} (Index {i})\")\n",
    "                     cm_lines.append(f\"{cm[i]}\") # TN, FP / FN, TP\n",
    "                     cm_lines.append(f\"  TN={cm[i][0,0]}, FP={cm[i][0,1]}, FN={cm[i][1,0]}, TP={cm[i][1,1]}\")\n",
    "                 confusion_matrix_report = \"\\n\".join(cm_lines)\n",
    "                 with open(cm_report_path, \"w\", encoding='utf-8') as f:\n",
    "                     f.write(confusion_matrix_report)\n",
    "                 logger.info(f\"Multi-label confusion matrix report saved to: {cm_report_path}\")\n",
    "            except Exception as cm_e:\n",
    "                 logger.error(f\"Error generating multilabel confusion matrix: {cm_e}\")\n",
    "                 confusion_matrix_report = f\"Error generating multilabel confusion matrix: {cm_e}\"\n",
    "\n",
    "\n",
    "        else: # Single-label (Multiclass)\n",
    "            logger.info(f\"Calculating single-label metrics for {report_suffix} (Ensemble)...\")\n",
    "            preds = torch.argmax(avg_logits, dim=1).numpy()\n",
    "            labels = all_labels_raw.numpy()\n",
    "\n",
    "            accuracy = accuracy_score(labels, preds)\n",
    "            hamming = hamming_loss(labels, preds)\n",
    "            f1_micro = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "            f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "            f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "            f1_samples = f1_score(labels, preds, average='samples', zero_division=0) # Less meaningful\n",
    "\n",
    "            metrics_dict = {\n",
    "                'accuracy': accuracy, 'hamming_loss': hamming,\n",
    "                'micro_f1': f1_micro, 'macro_f1': f1_macro,\n",
    "                'weighted_f1': f1_weighted, 'samples_f1': f1_samples\n",
    "            }\n",
    "\n",
    "            # Generate classification report\n",
    "            try:\n",
    "                present_labels = sorted(list(set(labels) | set(preds)))\n",
    "                target_names = [label_encoder.classes_[i] for i in present_labels if 0 <= i < len(label_encoder.classes_)]\n",
    "                valid_present_labels = [i for i in present_labels if 0 <= i < len(label_encoder.classes_)]\n",
    "                if valid_present_labels:\n",
    "                    classification_rep = classification_report(\n",
    "                        labels, preds,\n",
    "                        labels=valid_present_labels, target_names=target_names,\n",
    "                        digits=4, zero_division=0\n",
    "                    )\n",
    "                else:\n",
    "                    classification_rep = \"No valid labels found in predictions or ground truth.\"\n",
    "            except Exception as cr_e:\n",
    "                 logger.error(f\"Error generating classification report: {cr_e}\")\n",
    "                 classification_rep = f\"Error generating classification report: {cr_e}\"\n",
    "\n",
    "            confusion_matrix_report = \"Standard confusion matrix can be generated separately if needed.\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating metrics for {report_suffix}: {e}\", exc_info=True)\n",
    "        metrics_dict = {'error': f\"Metric calculation failed: {e}\"}\n",
    "        classification_rep = f\"Report generation failed due to metric error: {e}\"\n",
    "\n",
    "    # --- Save Report ---\n",
    "    metrics_dict['report'] = classification_rep # Add report string to dictionary\n",
    "    report_path = os.path.join(output_dir, f\"classification_report_{report_suffix}.txt\")\n",
    "    try:\n",
    "        with open(report_path, \"w\", encoding='utf-8') as f:\n",
    "            f.write(f\"--- Ensemble Evaluation Metrics ({report_suffix}) ---\\n\\n\")\n",
    "            for key, value in metrics_dict.items():\n",
    "                if key != 'report':\n",
    "                    f.write(f\"{key.replace('_', ' ').title()}: {value:.4f if isinstance(value, float) else value}\\n\")\n",
    "            f.write(\"\\n--- Classification Report ---\\n\\n\")\n",
    "            f.write(metrics_dict.get('report', 'Report generation failed.'))\n",
    "            f.write(\"\\n\\n\")\n",
    "            # Optionally add CM report for multi-label\n",
    "            if is_multilabel:\n",
    "                f.write(confusion_matrix_report)\n",
    "        logger.info(f\"Evaluation report saved to: {report_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save evaluation report to {report_path}: {e}\")\n",
    "\n",
    "    logger.info(f\"--- Ensemble Evaluation ({report_suffix}) Complete ---\\n\")\n",
    "    log_key = 'accuracy_subset' if is_multilabel else 'accuracy'\n",
    "    logger.info(f\"Final {report_suffix} {log_key}: {metrics_dict.get(log_key, 'N/A'):.4f}, Macro F1: {metrics_dict.get('macro_f1', 'N/A'):.4f}\")\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 12: Prediction Function (Ensemble)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.981529Z",
     "iopub.status.busy": "2025-04-13T05:44:03.981227Z",
     "iopub.status.idle": "2025-04-13T05:44:03.997424Z",
     "shell.execute_reply": "2025-04-13T05:44:03.996855Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.981512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_ensemble(\n",
    "    model_paths: List[str],\n",
    "    tokenizer: BartTokenizer,\n",
    "    label_encoder: LabelEncoder,\n",
    "    text: str,\n",
    "    triples: str = \"\",\n",
    "    image_features: Optional[torch.Tensor] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    "    max_length: int = MAX_LEN,\n",
    "    is_multilabel: bool = False,\n",
    "    threshold: float = 0.5,\n",
    "    base_model_name: str = BASE_TEXT_MODEL_NAME,\n",
    "    visual_feature_dim: int = VISUAL_FEATURE_DIM,\n",
    "    num_labels: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"Make ENSEMBLE prediction for a single example.\"\"\"\n",
    "\n",
    "    # Device configuration\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if not model_paths:\n",
    "        return {'error': 'No models provided'}\n",
    "\n",
    "    if image_features is None:\n",
    "        print(\"Warning: No image features provided. Using zero tensor.\")\n",
    "        image_features = torch.zeros(1, visual_feature_dim)\n",
    "    elif image_features.shape == (visual_feature_dim,):\n",
    "        image_features = image_features.unsqueeze(0)\n",
    "    elif image_features.shape != (1, visual_feature_dim):\n",
    "        return {'error': 'Incorrect image feature dimensions'}\n",
    "\n",
    "    if num_labels is None:\n",
    "        num_labels = len(label_encoder.classes_)\n",
    "\n",
    "    # Prepare prompt\n",
    "    class_names = \", \".join(label_encoder.classes_)\n",
    "    prompt = f\"Classify... Choose from: {class_names}.\\n\\n\"\n",
    "    prompt += f\"Text: {text}\\n\"\n",
    "    cleaned_triples = clean_triples(triples)\n",
    "    if cleaned_triples:\n",
    "        prompt += f\"Knowledge: {cleaned_triples}\\n\"\n",
    "    prompt += \"Category:\"\n",
    "\n",
    "    # Tokenize\n",
    "    try:\n",
    "        encoding = tokenizer(prompt, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return {'error': f'Tokenization fail: {e}'}\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    image_features = image_features.float().to(device)\n",
    "\n",
    "    # Collect predictions from each model\n",
    "    all_logits = []\n",
    "    for model_idx, model_path in enumerate(model_paths):\n",
    "        try:\n",
    "            model = M3H_CrossAttentionClassifier(\n",
    "                num_labels=num_labels,\n",
    "                pretrained_model_name=base_model_name,\n",
    "                is_multilabel=is_multilabel,\n",
    "                visual_feature_dim=visual_feature_dim\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device), strict=True)\n",
    "            except RuntimeError:\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask, image_features, labels=None)\n",
    "                all_logits.append(outputs.logits.cpu())\n",
    "\n",
    "            del model\n",
    "            gc.collect()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_idx + 1}: {e}\")\n",
    "\n",
    "    if not all_logits:\n",
    "        return {'error': 'Prediction failed'}\n",
    "\n",
    "    # Aggregate predictions\n",
    "    stacked_logits = torch.stack(all_logits, dim=0)\n",
    "    avg_logits = torch.mean(stacked_logits, dim=0).squeeze()\n",
    "\n",
    "    prediction_result = {'probabilities': {}}\n",
    "\n",
    "    try:\n",
    "        if is_multilabel:\n",
    "            probs = torch.sigmoid(avg_logits).numpy()\n",
    "            predicted_indices = np.where(probs > threshold)[0]\n",
    "            predicted_labels = label_encoder.inverse_transform(predicted_indices).tolist() if len(predicted_indices) > 0 else [\"None\"]\n",
    "            prediction_result['predicted_labels'] = predicted_labels\n",
    "            prediction_result['probabilities'] = {label_encoder.classes_[i]: float(probs[i]) for i in range(len(probs))}\n",
    "        else:\n",
    "            probs = torch.softmax(avg_logits, dim=0)\n",
    "            pred_class_idx = torch.argmax(avg_logits).item()\n",
    "            predicted_label = \"Error: Index out of bounds\"\n",
    "            try:\n",
    "                if 0 <= pred_class_idx < len(label_encoder.classes_):\n",
    "                    predicted_label = label_encoder.inverse_transform([pred_class_idx])[0]\n",
    "                else:\n",
    "                    print(f\"Prediction index {pred_class_idx} out of bounds.\")\n",
    "            except Exception as le_error:\n",
    "                print(f\"Label encoder error: {le_error}\")\n",
    "\n",
    "            prediction_result['predicted_class'] = predicted_label\n",
    "            class_probs = probs.cpu().numpy()\n",
    "            prediction_result['probabilities'] = {label_encoder.classes_[i]: float(class_probs[i]) for i in range(len(class_probs))}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Aggregation error: {e}\")\n",
    "        prediction_result = {'error': f'Aggregation fail: {e}'}\n",
    "\n",
    "    return prediction_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 13: Main Pipeline Function (Orchestrates Ensemble Training)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T05:44:03.998447Z",
     "iopub.status.busy": "2025-04-13T05:44:03.998213Z",
     "iopub.status.idle": "2025-04-13T05:44:04.040638Z",
     "shell.execute_reply": "2025-04-13T05:44:04.039920Z",
     "shell.execute_reply.started": "2025-04-13T05:44:03.998427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 13: Main Pipeline Function (Orchestrates Ensemble Training - WITH RAG FOR TRAINING)\n",
    "import time\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "\n",
    "# <<< Add this helper function at the top of the cell or in another preceding cell >>>\n",
    "def check_cuda_memory(step_name=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        logger.debug(f\"CUDA Memory ({step_name}): Allocated={allocated:.3f} GB, Reserved={reserved:.3f} GB\")\n",
    "    else:\n",
    "        logger.debug(f\"CUDA not available ({step_name}). Skipping memory check.\")\n",
    "# <<< End helper function >>>\n",
    "\n",
    "\n",
    "def run_ensemble_pipeline(\n",
    "    dataset_type: str = 'anxiety',\n",
    "    num_ensemble_models: int = NUM_ENSEMBLE_MODELS,\n",
    "    base_seed: int = BASE_SEED,\n",
    "    use_test_set: bool = True,\n",
    "    val_split_ratio: float = 0.1,\n",
    "    test_split_ratio: float = 0.2,\n",
    "    batch_size_override: int = BATCH_SIZE,\n",
    "    num_epochs_per_model: int = NUM_EPOCHS,\n",
    "    embedding_model_name: str = TEXT_EMBEDDING_MODEL, # For RAG text embeddings\n",
    "    base_text_model_name: str = BASE_TEXT_MODEL_NAME, # For classifier\n",
    "    visual_feature_dim: int = VISUAL_FEATURE_DIM,\n",
    "    visual_feature_dir: str = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\") # Dir where features are saved/loaded\n",
    ") -> Tuple[Optional[Dict], Optional[BartTokenizer], Optional[LabelEncoder], List[str]]:\n",
    "    \"\"\"\n",
    "    Runs the full ensemble training and evaluation pipeline.\n",
    "    Includes RAG examples in prompts for ALL splits if retriever is available.\n",
    "    \"\"\"\n",
    "    pipeline_start_time = time.time()\n",
    "    print(\">>> DEBUG: ENTERING run_ensemble_pipeline <<<\")\n",
    "    check_cuda_memory(\"Start of Pipeline\")\n",
    "\n",
    "    is_multilabel = (dataset_type == 'depression')\n",
    "    logger.info(f\"--- Starting ENSEMBLE Pipeline ({num_ensemble_models} models) for Dataset: {dataset_type} --- \")\n",
    "    logger.info(f\"Task Type: {'Multi-Label' if is_multilabel else 'Single-Label (Multiclass)'}\")\n",
    "    # ... [rest of logging for models/dims] ...\n",
    "    logger.info(f\"Using Base Text Model: {base_text_model_name}\")\n",
    "    logger.info(f\"Using Text Embedding Model: {embedding_model_name}\")\n",
    "    logger.info(f\"Using Vision Model for Features: {VISION_MODEL_NAME} (Dim: {visual_feature_dim})\")\n",
    "\n",
    "    # --- 1. Define Paths ---\n",
    "    # ... [Path definition logic remains the same] ...\n",
    "    output_basedir_name = dataset_type;\n",
    "    pipeline_base_output_dir = os.path.join(KAGGLE_WORKING_DIR, output_basedir_name, \"output\", \"ensemble_fusion\"); os.makedirs(pipeline_base_output_dir, exist_ok=True); logger.info(f\"Pipeline base output: {pipeline_base_output_dir}\")\n",
    "    if dataset_type == \"anxiety\":\n",
    "        anxiety_base_dir = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Anxiety_Data\")\n",
    "        train_file_path = os.path.join(anxiety_base_dir, \"anxiety_train.json\")\n",
    "        test_file_path = os.path.join(anxiety_base_dir, \"anxiety_test.json\")\n",
    "        img_feature_train_path = os.path.join(visual_feature_dir, \"anxiety_train_features.pt\")\n",
    "        img_feature_test_path = os.path.join(visual_feature_dir, \"anxiety_test_features.pt\")\n",
    "        img_feature_val_path=None;\n",
    "        val_file_path=None;\n",
    "    elif dataset_type == \"depression\":\n",
    "        depression_base_dir = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Depressive_Data\")\n",
    "        train_file_path = os.path.join(depression_base_dir, \"train.json\")\n",
    "        test_file_path = os.path.join(depression_base_dir, \"test.json\")\n",
    "        val_file_path = os.path.join(depression_base_dir, \"val.json\")\n",
    "        img_feature_train_path = os.path.join(visual_feature_dir, \"depression_train_features.pt\")\n",
    "        img_feature_test_path = os.path.join(visual_feature_dir, \"depression_test_features.pt\")\n",
    "        img_feature_val_path = os.path.join(visual_feature_dir, \"depression_val_features.pt\")\n",
    "    else:\n",
    "        logger.error(f\"Invalid dataset_type specified: {dataset_type}. Choose 'anxiety' or 'depression'.\")\n",
    "        return None, None, None, []\n",
    "    print(f\">>> DEBUG: Paths Defined <<<\")\n",
    "\n",
    "    # --- 2. Load Data (Text) & Split ---\n",
    "    logger.info(\"Step 1a: Loading text data...\")\n",
    "    # ... [load_data and split_data calls - same as before] ...\n",
    "    full_train_data = load_data(train_file_path);\n",
    "    if not full_train_data:\n",
    "        logger.error(f\"Failed to load training data from {train_file_path}. Aborting.\")\n",
    "        return None, None, None, []\n",
    "    train_data, val_data, test_data = [], [], []; separate_val_file_loaded = False\n",
    "    if val_file_path and os.path.exists(val_file_path): val_data = load_data(val_file_path); # Load separate val if exists\n",
    "    if val_data: separate_val_file_loaded = True; logger.info(f\"Loaded {len(val_data)} val samples.\")\n",
    "    else: logger.warning(\"Separate val file empty or not specified.\")\n",
    "    if use_test_set and test_file_path and os.path.exists(test_file_path): test_data = load_data(test_file_path); # Load separate test if exists\n",
    "    if test_data: logger.info(f\"Loaded {len(test_data)} test samples.\")\n",
    "    elif use_test_set: logger.warning(f\"Test file not found: {test_file_path}\")\n",
    "    # Determine train/val/test splits\n",
    "    if separate_val_file_loaded: train_data = full_train_data # All original train is training data\n",
    "    elif use_test_set and test_data: train_data, val_data, _ = split_data(full_train_data, val_split_ratio, 0, base_seed) # Split train -> train/val\n",
    "    elif not use_test_set: train_data, val_data, test_data_split = split_data(full_train_data, val_split_ratio, test_split_ratio, base_seed); test_data = test_data_split # Split train -> train/val/test\n",
    "    else: train_data, val_data, _ = split_data(full_train_data, val_split_ratio, 0, base_seed) # Default: split train -> train/val, no test\n",
    "    logger.info(f\"Final Data split sizes: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
    "    if not train_data or not val_data:\n",
    "        logger.error(\"Empty train/val splits.\")\n",
    "        return None, None, None, []\n",
    "    print(\">>> DEBUG: Split data complete <<<\")\n",
    "\n",
    "    # --- 3. Load Visual Features ---\n",
    "    logger.info(\"Step 1b: Loading visual features...\")\n",
    "    # ... (Keep feature loading logic using load_feature_file) ...\n",
    "    def load_feature_file(path, description):\n",
    "        features = {}\n",
    "        if path and os.path.exists(path):\n",
    "            try:\n",
    "                features = torch.load(path, map_location='cpu')\n",
    "                logger.info(f\"Loaded {len(features)} {description} features from {os.path.basename(path)}.\")\n",
    "            except Exception as e: logger.error(f\"Error loading {description} features from {path}: {e}\")\n",
    "        elif path: logger.warning(f\"{description.capitalize()} feature file not found: {path}\")\n",
    "        return features\n",
    "    image_features_train = load_feature_file(img_feature_train_path, \"train\")\n",
    "    image_features_val = load_feature_file(img_feature_val_path, \"validation\") if img_feature_val_path else {}\n",
    "    image_features_test = load_feature_file(img_feature_test_path, \"test\")\n",
    "    image_features_map = {**image_features_test, **image_features_val, **image_features_train}\n",
    "    logger.info(f\"Combined visual feature map size: {len(image_features_map)}\")\n",
    "    # ... (Feature coverage logging) ...\n",
    "    train_ids_with_features = sum(1 for s in train_data if s.get('image_id') in image_features_map)\n",
    "    val_ids_with_features = sum(1 for s in val_data if s.get('image_id') in image_features_map)\n",
    "    test_ids_with_features = sum(1 for s in test_data if s.get('image_id') in image_features_map)\n",
    "    logger.info(f\"Feature coverage: Train={train_ids_with_features}/{len(train_data)}, Val={val_ids_with_features}/{len(val_data)}, Test={test_ids_with_features}/{len(test_data)}\")\n",
    "    print(\">>> DEBUG: Loaded visual features <<<\")\n",
    "\n",
    "    # --- 4. Clean Triples ---\n",
    "    logger.info(\"Step 1c: Cleaning triples text...\")\n",
    "    # ... (Keep cleaning loop) ...\n",
    "    [sample.update({'triples': clean_triples(sample.get('triples', ''))}) for dataset in [train_data, val_data, test_data] if dataset for sample in dataset]\n",
    "    print(\">>> DEBUG: Cleaned triples <<<\")\n",
    "\n",
    "    # --- 5. Encode Labels ---\n",
    "    logger.info(\"Step 1d: Encoding labels...\")\n",
    "    # ... (Keep label encoding logic) ...\n",
    "    all_possible_labels = set();\n",
    "    for ds in [train_data, val_data, test_data]:\n",
    "        if ds:\n",
    "            for s in ds:\n",
    "                 labels_in_sample = s.get('original_labels');\n",
    "                 if isinstance(labels_in_sample, list): all_possible_labels.update(lbl for lbl in labels_in_sample if lbl)\n",
    "                 elif isinstance(labels_in_sample, str) and labels_in_sample: all_possible_labels.add(labels_in_sample)\n",
    "    if not all_possible_labels:\n",
    "        logger.error(\"No labels found.\")\n",
    "        return None, None, None, []\n",
    "    sorted_labels = sorted(list(all_possible_labels)); label_encoder = LabelEncoder(); label_encoder.fit(sorted_labels); num_labels = len(label_encoder.classes_); logger.info(f\"Labels ({num_labels}): {label_encoder.classes_.tolist()}\")\n",
    "    print(\">>> DEBUG: Encoded labels <<<\")\n",
    "\n",
    "    # --- 6. RAG Setup ---\n",
    "    logger.info(\"Steps 2 & 3: Generating Text Embeddings for RAG DB and Building Index...\")\n",
    "    # Generate embeddings needed for RAG DB (train) and potentially for querying (val, test)\n",
    "    train_fused_embeddings = None\n",
    "    val_fused_embeddings = None\n",
    "    test_fused_embeddings = None\n",
    "    retriever = None\n",
    "    embedding_generator = None\n",
    "\n",
    "    try:\n",
    "        embedding_generator = EmbeddingGenerator(model_name=embedding_model_name, device=device)\n",
    "        train_ocr = [s.get('ocr_text', '') for s in train_data]; train_triples = [s.get('triples', '') for s in train_data]\n",
    "        train_fused_embeddings = embedding_generator.generate_fused_embeddings(train_ocr, train_triples)\n",
    "\n",
    "        # Generate embeddings for val/test if needed for prompt generation RAG lookup\n",
    "        # If you only do RAG based on train DB, you only need train embeddings for indexing\n",
    "        # The query embedding is generated on-the-fly by get_prompts_with_rag\n",
    "        # Let's keep it simple: only generate train embeddings here for the DB\n",
    "        # val_ocr = ... ; val_triples = ... ; val_fused_embeddings = embedding_generator.generate_fused_embeddings(...)\n",
    "        # test_ocr = ... ; test_triples = ... ; test_fused_embeddings = embedding_generator.generate_fused_embeddings(...)\n",
    "\n",
    "        if train_fused_embeddings is not None:\n",
    "            retriever = RAGRetriever(train_fused_embeddings, top_k=RETRIEVAL_K)\n",
    "            if retriever.index is None:\n",
    "                logger.warning(\"FAISS index building failed. RAG disabled.\")\n",
    "                retriever = None\n",
    "        else:\n",
    "            logger.warning(\"No train text embeddings generated. RAG disabled.\")\n",
    "            retriever = None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"RAG embedding/index error: {e}\", exc_info=True)\n",
    "        retriever = None # Ensure retriever is None on error\n",
    "    finally:\n",
    "        # Cleanup embedding generator AFTER using it for all necessary splits\n",
    "        if embedding_generator: del embedding_generator\n",
    "        gc.collect();\n",
    "        if device == torch.device('cuda'): torch.cuda.empty_cache()\n",
    "    print(\">>> DEBUG: RAG setup complete <<<\")\n",
    "    check_cuda_memory(\"After RAG setup\")\n",
    "\n",
    "\n",
    "    # --- 7. Prompt Construction ---\n",
    "    logger.info(\"Step 4: Preparing prompts (RAG examples included if retriever exists)...\")\n",
    "    prompt_constructor = PromptConstructor(train_data, label_encoder)\n",
    "\n",
    "    # >>> MODIFIED get_prompts_with_rag function definition <<<\n",
    "    def get_prompts_with_rag(data_split: List[Dict],\n",
    "                              embeddings_for_query: Optional[np.ndarray], # Embeddings of the data_split itself\n",
    "                              split_name: str,\n",
    "                              is_training_split: bool = False) -> List[str]:\n",
    "        \"\"\"Generates prompts, including RAG examples using the global retriever.\"\"\"\n",
    "        logger.info(f\"Generating prompts for {split_name} ({len(data_split)} samples). RAG enabled: {retriever is not None}\")\n",
    "        prompts = []\n",
    "        # Check if RAG is possible *at all* (retriever must exist)\n",
    "        rag_possible_globally = retriever is not None\n",
    "\n",
    "        # Check if embeddings are provided *for this split* to perform the query\n",
    "        can_query_rag = rag_possible_globally and (embeddings_for_query is not None)\n",
    "\n",
    "        if not can_query_rag and rag_possible_globally:\n",
    "             logger.warning(f\"RAG retriever exists, but no query embeddings provided for {split_name}. Generating basic prompts.\")\n",
    "        elif not rag_possible_globally:\n",
    "             logger.info(f\"RAG retriever not available. Generating basic prompts for {split_name}.\")\n",
    "\n",
    "\n",
    "        for i, sample in enumerate(tqdm(data_split, desc=f\"Generating {split_name} Prompts\")):\n",
    "            retrieved_indices_for_prompt = [] # Default: no examples\n",
    "            if can_query_rag:\n",
    "                try:\n",
    "                    query_embedding = embeddings_for_query[i : i + 1]\n",
    "                    # Retrieve k+1 neighbors (raw_indices shape: (1, k+1))\n",
    "                    raw_indices = retriever.retrieve_similar(query_embedding)\n",
    "\n",
    "                    if raw_indices is not None and len(raw_indices[0]) > 0:\n",
    "                        # Exclude self (index i if is_training_split, otherwise keep top K)\n",
    "                        potential_indices = raw_indices[0]\n",
    "                        if is_training_split:\n",
    "                             # Filter out the current sample's own index (i) and take top K remaining\n",
    "                             final_indices = [idx for idx in potential_indices if idx != i][:RETRIEVAL_K]\n",
    "                             retrieved_indices_for_prompt = final_indices\n",
    "                        else:\n",
    "                             # For val/test, just take the top K (might include self if it was somehow indexed)\n",
    "                             retrieved_indices_for_prompt = potential_indices[:RETRIEVAL_K]\n",
    "                    else:\n",
    "                        logger.debug(f\"Retrieval returned None or empty for sample {i} in {split_name}.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during RAG retrieval for sample {i} in {split_name}: {e}\", exc_info=True)\n",
    "                    # Fallback to no examples for this sample on error\n",
    "\n",
    "            # Construct the prompt with the retrieved indices (or empty list)\n",
    "            prompts.append(prompt_constructor.construct_prompt(sample, retrieved_indices_for_prompt))\n",
    "\n",
    "        return prompts\n",
    "    # >>> END MODIFIED FUNCTION <<<\n",
    "\n",
    "    # Generate prompts - Pass the correct embeddings for querying\n",
    "    # We generated train_fused_embeddings earlier for the index AND for querying train data\n",
    "    train_prompts = get_prompts_with_rag(train_data, train_fused_embeddings, \"Train\", is_training_split=True)\n",
    "    # For Val/Test, we currently pass None for embeddings, disabling RAG for them.\n",
    "    # If you want RAG for Val/Test, generate val_fused_embeddings/test_fused_embeddings in Step 6\n",
    "    # and pass them here instead of None.\n",
    "    val_prompts   = get_prompts_with_rag(val_data, None, \"Validation\", is_training_split=False)\n",
    "    test_prompts  = get_prompts_with_rag(test_data, None, \"Test\", is_training_split=False) if test_data else []\n",
    "    print(\">>> DEBUG: Prompts prepared <<<\")\n",
    "\n",
    "\n",
    "    # --- 8. Tokenizer, Datasets, DataLoaders ---\n",
    "    logger.info(\"Step 5: Loading tokenizer...\")\n",
    "    # ... [Load tokenizer - same as before] ...\n",
    "    try:\n",
    "        tokenizer = BartTokenizer.from_pretrained(base_text_model_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Tokenizer load fail: {e}\")\n",
    "        return None,None,None,[]\n",
    "    print(\">>> DEBUG: Tokenizer loaded <<<\")\n",
    "\n",
    "    logger.info(\"Step 6: Creating Datasets...\")\n",
    "    # ... [Create Datasets - same as before] ...\n",
    "    try:\n",
    "        DatasetClass = DepressionDataset if is_multilabel else AnxietyDataset\n",
    "        train_dataset=DatasetClass(train_data, train_prompts, tokenizer, MAX_LEN, label_encoder, image_features_map, visual_feature_dim)\n",
    "        val_dataset=DatasetClass(val_data, val_prompts, tokenizer, MAX_LEN, label_encoder, image_features_map, visual_feature_dim)\n",
    "        test_dataset=DatasetClass(test_data, test_prompts, tokenizer, MAX_LEN, label_encoder, image_features_map, visual_feature_dim) if test_data else None\n",
    "        print(f\">>> DEBUG: Datasets created (Train: {len(train_dataset)}, Val: {len(val_dataset)}) <<<\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Dataset fail: {e}\")\n",
    "        return None, tokenizer, label_encoder, []\n",
    "\n",
    "    logger.info(\"Step 7: Creating DataLoaders...\")\n",
    "    # ... [Create DataLoaders - same as before] ...\n",
    "    try:\n",
    "        num_workers = 2 if torch.cuda.is_available() else 0; pin_memory = bool(device == torch.device('cuda'))\n",
    "        # Add drop_last=True if batch size doesn't divide dataset size perfectly, especially for training\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size_override, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=(len(train_dataset) % batch_size_override == 1))\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size_override, shuffle=False, num_workers=num_workers, pin_memory=pin_memory);\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size_override, shuffle=False, num_workers=num_workers, pin_memory=pin_memory) if test_dataset else None\n",
    "        print(f\">>> DEBUG: DataLoaders created (Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}) <<<\")\n",
    "        if len(train_dataloader)==0 or len(val_dataloader)==0:\n",
    "            logger.error(\"Train/Val Dataloader empty!\")\n",
    "            return None,tokenizer,label_encoder,[]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Dataloader fail: {e}\")\n",
    "        return None, tokenizer, label_encoder, []\n",
    "\n",
    "\n",
    "    print(\">>> DEBUG: Reached Point 3 - BEFORE ENSEMBLE LOOP <<<\")\n",
    "    check_cuda_memory(\"Before Ensemble Loop\")\n",
    "\n",
    "    # --- 9. Ensemble Training Loop ---\n",
    "    logger.info(f\"Step 8: Starting Ensemble Training ({num_ensemble_models} models)...\")\n",
    "    trained_model_paths = []\n",
    "    all_histories = []\n",
    "\n",
    "    print(\">>> DEBUG: Reached Point 4 - ENTERING ENSEMBLE LOOP <<<\")\n",
    "    for i in range(num_ensemble_models):\n",
    "        # ... [Rest of the ensemble loop remains the same] ...\n",
    "        # It instantiates M3H_CrossAttentionClassifier, sets up optimizer/scheduler,\n",
    "        # calls train_and_evaluate, saves results, and cleans up.\n",
    "        print(f\">>> DEBUG: Starting ensemble run {i+1}/{num_ensemble_models} <<<\"); run_seed = base_seed + i; set_seed(run_seed); model_run_output_dir = os.path.join(pipeline_base_output_dir, f\"run_{run_seed}\"); os.makedirs(model_run_output_dir, exist_ok=True); logger.info(f\"--- Training Model {i + 1}/{num_ensemble_models} (Seed: {run_seed}) --- Output: {model_run_output_dir}\")\n",
    "        model = None\n",
    "        try:\n",
    "            print(f\">>> DEBUG: Instantiating model run {i+1} <<<\")\n",
    "            model = M3H_CrossAttentionClassifier(\n",
    "                num_labels=num_labels,\n",
    "                pretrained_model_name=base_text_model_name,\n",
    "                is_multilabel=is_multilabel,\n",
    "                visual_feature_dim=visual_feature_dim,\n",
    "                dropout_prob=DROPOUT\n",
    "            ).to(device)\n",
    "            print(f\">>> DEBUG: Model instantiated run {i+1} <<<\")\n",
    "            check_cuda_memory(f\"After model init run {i+1}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model init fail run {i + 1}: {e}\", exc_info=True);\n",
    "            continue\n",
    "        optimizer = None; scheduler = None\n",
    "        try:\n",
    "            print(f\">>> DEBUG: Creating opt/sched run {i+1} <<<\")\n",
    "            optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, betas=(ADAM_BETA1, ADAM_BETA2), eps=ADAM_EPSILON, weight_decay=WEIGHT_DECAY)\n",
    "            total_training_steps = len(train_dataloader) * num_epochs_per_model;\n",
    "            num_warmup_steps = int(0.1 * total_training_steps)\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_training_steps)\n",
    "            print(f\">>> DEBUG: Opt/sched created run {i+1} <<<\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Opt/Sched setup fail run {i + 1}: {e}\");\n",
    "            del model;\n",
    "            gc.collect();\n",
    "            torch.cuda.empty_cache();\n",
    "            continue\n",
    "        try:\n",
    "            print(f\">>> DEBUG: Calling train_and_evaluate run {i+1} <<<\")\n",
    "            history, best_model_path_run = train_and_evaluate(\n",
    "                train_dataloader, val_dataloader, model, optimizer, scheduler, device,\n",
    "                num_epochs_per_model, model_run_output_dir, label_encoder, is_multilabel\n",
    "            )\n",
    "            all_histories.append(history);\n",
    "            trained_model_paths.append(best_model_path_run)\n",
    "            logger.info(f\"Run {i+1} training complete. Best model: {best_model_path_run}\")\n",
    "            plot_training_history(history, model_run_output_dir, suffix=f\"_run_{run_seed}\")\n",
    "        except Exception as train_e:\n",
    "            logger.error(f\"Training failed run {i + 1}: {train_e}\", exc_info=True)\n",
    "            tb_path = os.path.join(model_run_output_dir, \"error_traceback.txt\");\n",
    "            with open(tb_path, \"w\") as f: traceback.print_exc(file=f)\n",
    "        finally:\n",
    "            print(f\">>> DEBUG: Cleaning up run {i+1} <<<\")\n",
    "            del model, optimizer, scheduler\n",
    "            if 'history' in locals(): del history\n",
    "            if 'best_model_path_run' in locals(): del best_model_path_run\n",
    "            gc.collect();\n",
    "            if device == torch.device('cuda'): torch.cuda.empty_cache()\n",
    "            check_cuda_memory(f\"End of run {i+1}\")\n",
    "\n",
    "    print(\">>> DEBUG: Reached Point 12 - EXITED ensemble training loop <<<\")\n",
    "    logger.info(f\"--- Ensemble Training Finished ({len(trained_model_paths)} models trained) ---\")\n",
    "\n",
    "    if not trained_model_paths:\n",
    "        logger.error(\"No models trained.\")\n",
    "        return None, tokenizer, label_encoder, []\n",
    "\n",
    "    # --- 10. Final Ensemble Evaluation ---\n",
    "    # ... [Evaluation logic remains the same] ...\n",
    "    logger.info(\"Step 9: Evaluating Ensemble...\"); final_metrics = {}; logger.info(\"--- Final Val Eval (Ensemble) ---\"); val_metrics = evaluate_ensemble(\n",
    "        model_paths=trained_model_paths,\n",
    "        dataloader=val_dataloader,\n",
    "        device=device,\n",
    "        label_encoder=label_encoder,\n",
    "        is_multilabel=is_multilabel,\n",
    "        num_labels=num_labels,\n",
    "        output_dir=pipeline_base_output_dir,\n",
    "        report_suffix=\"validation_ensemble\",\n",
    "        base_model_name=base_text_model_name,\n",
    "        visual_feature_dim=visual_feature_dim\n",
    "    ); final_metrics['validation'] = val_metrics; logger.info(f\"Ens Val Acc: {val_metrics.get('accuracy_subset', val_metrics.get('accuracy', 'N/A')):.4f}, MacroF1: {val_metrics.get('macro_f1', 'N/A'):.4f}\")\n",
    "    if test_dataloader:\n",
    "        logger.info(\"--- Final Test Eval (Ensemble) ---\"); test_metrics = evaluate_ensemble(\n",
    "            model_paths=trained_model_paths,\n",
    "            dataloader=test_dataloader,\n",
    "            device=device,\n",
    "            label_encoder=label_encoder,\n",
    "            is_multilabel=is_multilabel,\n",
    "            num_labels=num_labels,\n",
    "            output_dir=pipeline_base_output_dir,\n",
    "            report_suffix=\"test_ensemble\",\n",
    "            base_model_name=base_text_model_name,\n",
    "            visual_feature_dim=visual_feature_dim\n",
    "        ); final_metrics['test'] = test_metrics; logger.info(f\"Ens Test Acc: {test_metrics.get('accuracy_subset', test_metrics.get('accuracy', 'N/A')):.4f}, MacroF1: {test_metrics.get('macro_f1', 'N/A'):.4f}\")\n",
    "    else:\n",
    "        logger.info(\"No test data.\"); final_metrics['test'] = \"Skipped\"\n",
    "\n",
    "    # --- 11. Save Label Encoder ---\n",
    "    # ... [Saving logic remains the same] ...\n",
    "    logger.info(\"Step 10: Saving Label Encoder...\"); label_encoder_path = os.path.join(pipeline_base_output_dir, \"label_encoder.pkl\"); try: with open(label_encoder_path, 'wb') as f: pickle.dump(label_encoder, f); logger.info(f\"LE saved: {label_encoder_path}\") except Exception as e: logger.error(f\"LE save fail: {e}\")\n",
    "\n",
    "    pipeline_end_time = time.time()\n",
    "    logger.info(f\"--- ENSEMBLE Pipeline finished for {dataset_type} in {(pipeline_end_time - pipeline_start_time)/60:.2f} minutes ---\")\n",
    "    return final_metrics, tokenizer, label_encoder, trained_model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 14: Execution Cell (Runs Ensemble Pipeline)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for the Run ---\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#       <<<<< CHOOSE DATASET TYPE HERE >>>>>\n",
    "DATASET_CHOICE = 'anxiety'   # Options: 'anxiety' or 'depression'\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# --- Other Ensemble & Training Settings ---\n",
    "NUM_ENSEMBLE_RUNS = 3       # How many models in the ensemble (e.g., 3 or 5)\n",
    "EPOCHS_PER_MODEL = 10       # Epochs for EACH model in the ensemble\n",
    "BATCH_SIZE_TO_USE = 8       # Adjust based on GPU memory (T4 usually handles 8 well)\n",
    "\n",
    "# --- Data Splitting Strategy ---\n",
    "# If True, expects 'anxiety_test.json' or 'test.json'/'val.json' in dataset dirs\n",
    "# If False, splits the main training file into train/val/test sets\n",
    "USE_SEPARATE_TEST_FILE = True\n",
    "# Ratios used ONLY if USE_SEPARATE_TEST_FILE = False (or if separate files are missing)\n",
    "# These ratios are relative to the *original* training data size.\n",
    "VALIDATION_SPLIT = 0.1      # e.g., 10% of original train data for validation\n",
    "TEST_SPLIT = 0.2            # e.g., 20% of original train data for test\n",
    "\n",
    "# Override hyperparameters from Cell 3 if needed for this specific run\n",
    "# LEARNING_RATE_OVERRIDE = 5e-5\n",
    "# DROPOUT_OVERRIDE = 0.15\n",
    "\n",
    "\n",
    "# --- Run the Ensemble Pipeline ---\n",
    "logger.info(f\"===== Starting Ensemble Pipeline Execution for: {DATASET_CHOICE} =====\")\n",
    "logger.info(f\"Number of ensemble models: {NUM_ENSEMBLE_RUNS}\")\n",
    "logger.info(f\"Epochs per model: {EPOCHS_PER_MODEL}\")\n",
    "logger.info(f\"Batch size: {BATCH_SIZE_TO_USE}\")\n",
    "logger.info(f\"Using separate test file: {USE_SEPARATE_TEST_FILE}\")\n",
    "\n",
    "# Clean up memory before starting\n",
    "gc.collect()\n",
    "if device == torch.device('cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(f\"CUDA Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    logger.info(f\"CUDA Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "# Initialize variables to store results\n",
    "final_eval_metrics = None\n",
    "trained_tokenizer = None\n",
    "trained_label_encoder = None\n",
    "trained_model_paths = []\n",
    "\n",
    "try:\n",
    "    final_eval_metrics, trained_tokenizer, trained_label_encoder, trained_model_paths = run_ensemble_pipeline(\n",
    "        dataset_type=DATASET_CHOICE,\n",
    "        num_ensemble_models=NUM_ENSEMBLE_RUNS,\n",
    "        base_seed=BASE_SEED, # Use the global base seed\n",
    "        use_test_set=USE_SEPARATE_TEST_FILE,\n",
    "        val_split_ratio=VALIDATION_SPLIT,\n",
    "        test_split_ratio=TEST_SPLIT,\n",
    "        batch_size_override=BATCH_SIZE_TO_USE,\n",
    "        num_epochs_per_model=EPOCHS_PER_MODEL,\n",
    "        embedding_model_name=TEXT_EMBEDDING_MODEL, # From config\n",
    "        base_text_model_name=BASE_TEXT_MODEL_NAME, # From config\n",
    "        visual_feature_dim=VISUAL_FEATURE_DIM,     # From config\n",
    "        visual_feature_dir = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\") # Pass dir where features are stored\n",
    "        # Pass overrides if defined:\n",
    "        # learning_rate_override = LEARNING_RATE_OVERRIDE,\n",
    "        # dropout_override = DROPOUT_OVERRIDE,\n",
    "    )\n",
    "    logger.info(\"<<<<< run_ensemble_pipeline function returned >>>>>\")\n",
    "\n",
    "    # --- Post-Training Analysis ---\n",
    "    if trained_model_paths and trained_tokenizer and trained_label_encoder:\n",
    "        logger.info(f\"Pipeline completed successfully for '{DATASET_CHOICE}'.\")\n",
    "        logger.info(f\"Trained {len(trained_model_paths)} models.\")\n",
    "        logger.info(f\"Best model paths: {trained_model_paths}\")\n",
    "        logger.info(\"Final Evaluation Metrics:\")\n",
    "        print(json.dumps(final_eval_metrics, indent=2)) # Pretty print the metrics dict\n",
    "\n",
    "        # --- Example Prediction Call (Requires loading features for the sample) ---\n",
    "        # You would need to get the 'image_features' tensor for a specific sample ID\n",
    "        # example_sample_id = \"some_image_id_from_your_data\"\n",
    "        # try:\n",
    "        #     # Load the combined feature map again (or pass it)\n",
    "        #     combined_features_path = os.path.join(KAGGLE_WORKING_DIR, \"visual_features\", f\"{DATASET_CHOICE}_combined_features.pt\") # Assuming you saved a combined one\n",
    "        #     if os.path.exists(combined_features_path):\n",
    "        #          all_features = torch.load(combined_features_path, map_location='cpu')\n",
    "        #          example_image_feature = all_features.get(example_sample_id)\n",
    "        #          if example_image_feature is not None:\n",
    "        #              logger.info(f\"\\n--- Running Example Prediction for ID: {example_sample_id} ---\")\n",
    "        #              prediction = predict_ensemble(\n",
    "        #                  model_paths=trained_model_paths,\n",
    "        #                  tokenizer=trained_tokenizer,\n",
    "        #                  label_encoder=trained_label_encoder,\n",
    "        #                  text=\"This is some example meme text.\",\n",
    "        #                  triples=\"Cause-Effect: text causes laughter\", # Optional example triples\n",
    "        #                  image_features=example_image_feature, # Provide the loaded features\n",
    "        #                  device=device,\n",
    "        #                  is_multilabel=(DATASET_CHOICE == 'depression'),\n",
    "        #                  base_model_name=BASE_TEXT_MODEL_NAME,\n",
    "        #                  visual_feature_dim=VISUAL_FEATURE_DIM,\n",
    "        #                  num_labels=len(trained_label_encoder.classes_)\n",
    "        #              )\n",
    "        #              logger.info(\"Example Prediction Result:\")\n",
    "        #              print(json.dumps(prediction, indent=2))\n",
    "        #          else:\n",
    "        #              logger.warning(f\"Could not find image features for example ID: {example_sample_id}\")\n",
    "        #     else:\n",
    "        #         logger.warning(\"Could not load combined features map for example prediction.\")\n",
    "\n",
    "        # except Exception as pred_e:\n",
    "        #     logger.error(f\"Error running example prediction: {pred_e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        logger.error(f\"Pipeline for '{DATASET_CHOICE}' did not complete successfully. Check logs for errors.\")\n",
    "        if not trained_model_paths: logger.error(\"  - No models were successfully trained.\")\n",
    "        if not trained_tokenizer: logger.error(\"  - Tokenizer was not loaded/returned.\")\n",
    "        if not trained_label_encoder: logger.error(\"  - Label encoder was not fitted/returned.\")\n",
    "\n",
    "\n",
    "except Exception as pipeline_e:\n",
    "    logger.error(f\"!!!!!! CATASTROPHIC ERROR in pipeline execution cell !!!!!!\")\n",
    "    logger.error(f\"Error Type: {type(pipeline_e).__name__}\")\n",
    "    logger.error(f\"Error Message: {pipeline_e}\")\n",
    "    logger.error(\"Traceback:\", exc_info=True)\n",
    "    # Save traceback to file\n",
    "    tb_main_path = os.path.join(KAGGLE_WORKING_DIR, f\"{DATASET_CHOICE}_pipeline_crash_traceback.txt\")\n",
    "    with open(tb_main_path, \"w\") as f:\n",
    "        traceback.print_exc(file=f)\n",
    "    logger.info(f\"Crash traceback saved to: {tb_main_path}\")\n",
    "\n",
    "\n",
    "# --- Final Cleanup ---\n",
    "logger.info(\"Performing final cleanup...\")\n",
    "# Explicitly delete large objects if they exist\n",
    "if 'final_eval_metrics' in locals(): del final_eval_metrics\n",
    "if 'trained_tokenizer' in locals(): del trained_tokenizer\n",
    "if 'trained_label_encoder' in locals(): del trained_label_encoder\n",
    "if 'trained_model_paths' in locals(): del trained_model_paths\n",
    "# Delete potentially large data splits if no longer needed\n",
    "if 'train_data' in locals(): del train_data\n",
    "if 'val_data' in locals(): del val_data\n",
    "if 'test_data' in locals(): del test_data\n",
    "if 'full_train_data' in locals(): del full_train_data\n",
    "if 'image_features_map' in locals(): del image_features_map\n",
    "\n",
    "gc.collect() # Run garbage collection\n",
    "if device == torch.device('cuda'):\n",
    "    torch.cuda.empty_cache() # Clear PyTorch CUDA cache\n",
    "    logger.info(\"CUDA cache cleared.\")\n",
    "    logger.info(f\"Final CUDA Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    logger.info(f\"Final CUDA Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "logger.info(f\"===== Execution cell finished for: {DATASET_CHOICE} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 15: Zip Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Ensure os is imported if running this cell independently\n",
    "\n",
    "# --- Determine paths based on the DATASET_CHOICE from the previous execution cell ---\n",
    "# This relies on DATASET_CHOICE being set correctly in Cell 14\n",
    "try:\n",
    "    # Check if DATASET_CHOICE exists and is one of the expected values\n",
    "    if 'DATASET_CHOICE' in locals() and DATASET_CHOICE in ['anxiety', 'depression']:\n",
    "        output_subdir = DATASET_CHOICE\n",
    "        logger.info(f\"Determined output subdirectory for zipping: '{output_subdir}'\")\n",
    "    else:\n",
    "        # Attempt to infer from directory structure if variable is missing\n",
    "        anxiety_path = \"/kaggle/working/anxiety/output/ensemble_fusion/\"\n",
    "        depression_path = \"/kaggle/working/depression/output/ensemble_fusion/\"\n",
    "        if os.path.exists(anxiety_path):\n",
    "             output_subdir = \"anxiety\"\n",
    "             logger.warning(\"DATASET_CHOICE variable not found or invalid. Inferred '{output_subdir}' from directory structure.\")\n",
    "        elif os.path.exists(depression_path):\n",
    "             output_subdir = \"depression\"\n",
    "             logger.warning(\"DATASET_CHOICE variable not found or invalid. Inferred '{output_subdir}' from directory structure.\")\n",
    "        else:\n",
    "             output_subdir = \"anxiety\" # Default if inference fails\n",
    "             logger.error(\"DATASET_CHOICE variable not found and cannot infer directory. Defaulting to '{output_subdir}'. Check Cell 14 execution and output paths.\")\n",
    "\n",
    "except NameError:\n",
    "    output_subdir = \"anxiety\" # Fallback if DATASET_CHOICE is not defined at all\n",
    "    logger.error(\"NameError: DATASET_CHOICE variable not defined. Defaulting to '{output_subdir}'. Please run Cell 14 first.\")\n",
    "\n",
    "\n",
    "# Define the source directory to zip (the base ensemble output dir)\n",
    "# This directory contains the run_* subfolders, reports, and label encoder\n",
    "source_path = f\"/kaggle/working/{output_subdir}/output/ensemble_fusion/\"\n",
    "zip_filename = f\"/kaggle/working/{output_subdir}_ensemble_fusion_output.zip\" # Changed filename for clarity\n",
    "\n",
    "print(f\"\\nAttempting to zip contents of: {source_path}\")\n",
    "print(f\"Creating archive named: {zip_filename}\")\n",
    "\n",
    "# --- Create the zip archive ---\n",
    "if os.path.exists(source_path) and os.path.isdir(source_path):\n",
    "    # Check if the directory is empty\n",
    "    if not os.listdir(source_path):\n",
    "        print(f\"\\nWARNING: Source directory '{source_path}' is empty. Zip file will be created but empty.\")\n",
    "        # Still create an empty zip or skip? Let's create it for consistency.\n",
    "        # The command might fail on an empty dir depending on zip version, let's try anyway.\n",
    "        !zip -r -q {zip_filename} {source_path}* # Using * inside might handle empty dir better sometimes\n",
    "        # Check if zip was created\n",
    "        if os.path.exists(zip_filename):\n",
    "            print(f\"Empty zip archive created for empty source directory.\")\n",
    "        else:\n",
    "            print(f\"Failed to create zip archive (source directory might be truly empty or inaccessible).\")\n",
    "\n",
    "    else:\n",
    "        # Use -r for recursive, -q for quiet execution\n",
    "        # Zip the contents *inside* the source directory to avoid nested 'ensemble_fusion' folder in zip\n",
    "        # We cd into the parent dir, then zip the target dir\n",
    "        parent_dir = os.path.dirname(source_path.rstrip('/'))\n",
    "        target_dir_name = os.path.basename(source_path.rstrip('/'))\n",
    "        zip_command = f\"cd {parent_dir} && zip -r -q {zip_filename} {target_dir_name}\"\n",
    "        print(f\"Executing zip command: {zip_command}\")\n",
    "        !{zip_command}\n",
    "        # Check if zip was created\n",
    "        if os.path.exists(zip_filename):\n",
    "             print(f\"\\nZip process finished successfully for '{output_subdir}'.\")\n",
    "             print(f\"Archive created: {zip_filename}\")\n",
    "        else:\n",
    "             print(f\"\\nERROR: Zip command executed but archive not found at {zip_filename}. Check permissions or command output.\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nERROR: Source directory not found or is not a directory: {source_path}\")\n",
    "    print(\"Cannot create zip archive. Please ensure the pipeline ran correctly and produced output.\")\n",
    "\n",
    "# --- Verify by listing the working directory ---\n",
    "print(\"\\nContents of /kaggle/working/ (showing zip files and output directories):\")\n",
    "# Use ls with options: -l (long format), -h (human readable sizes), -t (sort by time, newest first)\n",
    "# Filter for zip files and the output directory for clarity\n",
    "!ls -lht /kaggle/working/ | grep -E '.zip$|anxiety$|depression$'\n",
    "print(\"\\nFull contents of /kaggle/working/:\")\n",
    "!ls -lht /kaggle/working/"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7105680,
     "sourceId": 11354594,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
