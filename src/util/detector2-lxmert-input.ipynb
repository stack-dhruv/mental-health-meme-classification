{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[],"toc_visible":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11354594,"sourceType":"datasetVersion","datasetId":7105680}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install detectron2","metadata":{"id":"vM54r6jlKTII"}},{"cell_type":"code","source":"!pip install 'git+https://github.com/facebookresearch/detectron2.git'","metadata":{"id":"FsePPpwZSmqt","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:58:20.629707Z","iopub.execute_input":"2025-04-14T13:58:20.630248Z","iopub.status.idle":"2025-04-14T14:00:22.550221Z","shell.execute_reply.started":"2025-04-14T13:58:20.630221Z","shell.execute_reply":"2025-04-14T14:00:22.549523Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/facebookresearch/detectron2.git\n  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-aeyqlyer\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-aeyqlyer\n  Resolved https://github.com/facebookresearch/detectron2.git to commit 400a49c1ec11a18dd25aea3910507bc3bcd15794\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.1.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.7.5)\nRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.8)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.5.0)\nCollecting yacs>=0.1.8 (from detectron2==0.6)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\nRequirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\nCollecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\nRequirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.3.0)\nCollecting hydra-core>=1.1 (from detectron2==0.6)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting black (from detectron2==0.6)\n  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (24.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\nCollecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.1.8)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (1.0.0)\nCollecting pathspec>=0.9.0 (from black->detectron2==0.6)\n  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.7)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.70.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.1.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2024.2.0)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: detectron2, fvcore\n  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for detectron2: filename=detectron2-0.6-cp311-cp311-linux_x86_64.whl size=6384299 sha256=d868580cfe9677d4689c8791ee68a309cbc86eed69006a82890a078da8f57c60\n  Stored in directory: /tmp/pip-ephem-wheel-cache-71qxpfs6/wheels/17/d9/40/60db98e485aa9455d653e29d1046601ce96fe23647f60c1c5a\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=35ec3099734c30034e9b3a1a5f2ca391933b6239502cc66b1b6203d06769ab91\n  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\nSuccessfully built detectron2 fvcore\nInstalling collected packages: yacs, portalocker, pathspec, iopath, hydra-core, black, fvcore, detectron2\nSuccessfully installed black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 pathspec-0.12.1 portalocker-3.1.1 yacs-0.1.8\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch, detectron2\n!nvcc --version\nTORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\nCUDA_VERSION = torch.__version__.split(\"+\")[-1]\nprint(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\nimport pkg_resources\nprint(\"detectron2:\", pkg_resources.get_distribution(\"detectron2\").version)","metadata":{"id":"0d288Z2mF5dC","outputId":"c47c5426-64d6-4632-f868-e2f14dfe39be","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T14:02:42.248324Z","iopub.execute_input":"2025-04-14T14:02:42.249138Z","iopub.status.idle":"2025-04-14T14:02:43.311869Z","shell.execute_reply.started":"2025-04-14T14:02:42.249108Z","shell.execute_reply":"2025-04-14T14:02:43.310922Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Thu_Jun__6_02:18:23_PDT_2024\nCuda compilation tools, release 12.5, V12.5.82\nBuild cuda_12.5.r12.5/compiler.34385749_0\ntorch:  2.5 ; cuda:  cu124\ndetectron2: 0.6\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4: Imports & Config (Updated for FPN Model)\n\nimport torch\nimport torch.nn as nn # Added for potential projection layer\nimport detectron2\nimport os\nimport numpy as np\nfrom PIL import Image\nimport gc\nfrom tqdm.notebook import tqdm\nimport logging\nimport json # Added for loading JSON data\n\n# Detectron2 specific imports\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.model_zoo import model_zoo, get_config_file, get_checkpoint_url\nfrom detectron2.modeling import build_model\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.structures import Boxes, ImageList\n\n# --- Configuration ---\\n# Ensure these match the main notebook's Cell 3 values if running separately\nKAGGLE_INPUT_DIR = \"/kaggle/input\"\nKAGGLE_WORKING_DIR = \"/kaggle/working\"\n# Where to save the final .pt files containing feature dictionaries\nVISUAL_FEATURE_DIR = os.path.join(KAGGLE_WORKING_DIR, \"region_visual_features\")\n\nMAX_REGIONS = 36         # Max regions per image (for LXMERT)\n# *** UPDATED FOR FPN MODEL ***\n# Feature dim from R-101-FPN box_head output (before predictor) is usually 1024\nDETECTOR_OUTPUT_DIM = 1024\n# Target dim should match what LXMERT expects (e.g., 2048 for base)\n# We might need projection if TARGET_FEATURE_DIM remains 2048\nTARGET_FEATURE_DIM = 2048\n# --- End Configuration ---\n\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"Using device: {device}\")\n\n# Create output directory\nos.makedirs(VISUAL_FEATURE_DIR, exist_ok=True)\nlogger.info(f\"Output directory for features: {VISUAL_FEATURE_DIR}\")\n\n# --- Helper Function for Image Paths ---\n# (Keep the get_image_path function from the previous version here)\ndef get_image_path(sample_id, dataset_base_path, split_name):\n    \"\"\"\n    Constructs the full image path based on dataset conventions.\n    Args:\n        sample_id (str): The ID of the sample (e.g., 'TR-1', 'TE-10', 'D-TR-5').\n        dataset_base_path (str): Base path for the dataset split\n                                 (e.g., \"/kaggle/input/axiom-dataset/dataset/Anxiety_Data\").\n        split_name (str): The name of the split ('train', 'test', 'val').\n    Returns:\n        str or None: The full path to the image file, or None if not found.\n    \"\"\"\n    dataset_name = os.path.basename(dataset_base_path) # \"Anxiety_Data\" or \"Depressive_Data\"\n\n    if \"Anxiety\" in dataset_name:\n        # Path: .../Anxiety_Data/anxiety_{split_name}_image/{sample_id}.jpg\n        image_folder = os.path.join(dataset_base_path, f\"anxiety_{split_name}_image\")\n        # Assume .jpg extension based on screenshots\n        image_filename = f\"{sample_id}.jpg\"\n        full_path = os.path.join(image_folder, image_filename)\n        if os.path.exists(full_path):\n            return full_path\n        else:\n            # You could try other extensions like .jpeg here if needed\n            logger.debug(f\"Anxiety image not found at expected path: {full_path}\")\n            return None\n\n    elif \"Depressive\" in dataset_name:\n        # Path: .../Depressive_Data/Images/depressive_image/{split_name}/{sample_id}.jpeg (or .jpg)\n        image_folder = os.path.join(dataset_base_path, \"Images\", \"depressive_image\", split_name)\n        # Try common extensions based on screenshots and common usage\n        for ext in [\".jpeg\", \".jpg\", \".png\"]:\n             image_filename = f\"{sample_id}{ext}\"\n             full_path = os.path.join(image_folder, image_filename)\n             if os.path.exists(full_path):\n                  return full_path\n        logger.debug(f\"Depressive image not found for {sample_id} in {image_folder} with extensions .jpeg, .jpg, .png.\")\n        return None\n\n    else:\n        logger.warning(f\"Unrecognized dataset base path structure: {dataset_base_path}\")\n        return None","metadata":{"id":"ZyAvNCJMmvFF","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T14:02:46.343389Z","iopub.execute_input":"2025-04-14T14:02:46.344140Z","iopub.status.idle":"2025-04-14T14:02:49.865368Z","shell.execute_reply.started":"2025-04-14T14:02:46.344114Z","shell.execute_reply":"2025-04-14T14:02:49.864841Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Train on a custom dataset\n\n","metadata":{"id":"b2bjrfb2LDeo"}},{"cell_type":"code","source":"# Cell 6: Load Detectron2 Model (Using R-101-FPN COCO Model - Cleaned)\n\nimport torch.nn as nn # Ensure nn is imported if running cell independently\nfrom detectron2.config import get_cfg\nfrom detectron2.model_zoo import model_zoo, get_config_file, get_checkpoint_url\nfrom detectron2.modeling import build_model\nfrom detectron2.checkpoint import DetectionCheckpointer\nimport logging # Ensure logger is available\n\nlogger = logging.getLogger(__name__) # Ensure logger is defined\n\ndef load_coco_detector(): # Renamed function for clarity\n    \"\"\"Loads the Faster R-CNN R-101-FPN model trained on COCO.\"\"\"\n    cfg = get_cfg()\n    try:\n        # USE COCO R-101-FPN MODEL CONFIG\n        config_file = \"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"\n        full_config_path = model_zoo.get_config_file(config_file)\n        cfg.merge_from_file(full_config_path)\n        logger.info(f\"Loaded config file: {full_config_path}\")\n\n        # Set thresholds\n        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1 # Set threshold to 0.1\n        logger.info(f\"Set cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST to {cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST}\")\n        # cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 1000 # Default is 1000 for FPN\n        # cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5 # Default is 0.5 for FPN\n\n        # Load weights for the COCO model\n        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)\n        logger.info(f\"Attempting to load weights from: {cfg.MODEL.WEIGHTS}\")\n\n        # Set device\n        cfg.MODEL.DEVICE = device.type # Assumes 'device' is defined globally\n        cfg.freeze()\n\n        # Build model\n        logger.info(f\"Building model...\")\n        model = build_model(cfg)\n        model.eval()\n        logger.info(f\"Model built successfully.\")\n\n        # Load weights into model\n        checkpointer = DetectionCheckpointer(model)\n        logger.info(f\"Attempting checkpointer.load()...\")\n        checkpointer.load(cfg.MODEL.WEIGHTS) # Load COCO weights\n        logger.info(\"Weights loaded successfully.\")\n\n        # Move model to device\n        model.to(device)\n        logger.info(\"Detectron2 Faster R-CNN R-101-FPN (COCO) model loaded successfully to device.\")\n        return model, cfg\n\n    # Keep error handling as before\n    except RuntimeError as e:\n         if \"No file found\" in str(e) or \"matching config file\" in str(e):\n              logger.error(f\"Could not find config file or weights for {config_file} at expected paths.\")\n              logger.error(\"Please ensure Detectron2 is installed correctly and the model zoo path is valid.\")\n         elif \"CUDA out of memory\" in str(e):\n             logger.error(\"CUDA out of memory. Try reducing batch size (if applicable) or using a smaller model.\")\n         else:\n              logger.error(f\"Runtime error loading Detectron2 model: {e}\", exc_info=True)\n         return None, None\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred loading the detector: {e}\", exc_info=True)\n        return None, None\n\n# --- Calling the Updated Function ---\nlogger.info(\"Calling load_coco_detector()...\")\nobject_detector_model, detector_cfg = load_coco_detector()\nif object_detector_model is not None:\n     logger.info(\"Detector model loaded successfully.\")\nelse:\n     logger.error(\"Detector model failed to load.\")\n\n\n# Define projection layer here. Needed because 1024 (detector) != 2048 (target)\nprojection_layer = None\nif object_detector_model is not None and DETECTOR_OUTPUT_DIM != TARGET_FEATURE_DIM:\n    logger.info(f\"Defining projection layer {DETECTOR_OUTPUT_DIM} -> {TARGET_FEATURE_DIM}\")\n    projection_layer = nn.Linear(DETECTOR_OUTPUT_DIM, TARGET_FEATURE_DIM).to(device).eval()\nelif object_detector_model is not None:\n     logger.info(f\"Projection layer not needed (Detector output dim {DETECTOR_OUTPUT_DIM} == Target dim {TARGET_FEATURE_DIM}).\")\nelse:\n     logger.info(f\"Skipping projection layer definition as model is None.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T14:03:09.097013Z","iopub.execute_input":"2025-04-14T14:03:09.097646Z","iopub.status.idle":"2025-04-14T14:03:11.497122Z","shell.execute_reply.started":"2025-04-14T14:03:09.097621Z","shell.execute_reply":"2025-04-14T14:03:11.496554Z"}},"outputs":[{"name":"stderr","text":"model_final_f6e8b1.pkl: 243MB [00:00, 290MB/s]                             \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 7: Feature Extraction Function (Cleaned)\n\nimport torch\nfrom detectron2.structures import Boxes, Instances\nfrom PIL import Image\nimport numpy as np\nimport logging\nimport traceback # Keep for unexpected errors\n\n# Make sure logger is available\nlogger = logging.getLogger(__name__)\n\ndef extract_features_from_image_vg(img_path, model, cfg, device, projection_layer,\n                                   max_regions=MAX_REGIONS,\n                                   target_dim=TARGET_FEATURE_DIM,\n                                   detector_output_dim=DETECTOR_OUTPUT_DIM):\n    \"\"\"\n    Extracts features and normalized boxes using the loaded model.\n    Applies projection if needed, pads/truncates, and creates attention mask.\n    \"\"\"\n    try:\n        with Image.open(img_path) as img_pil:\n            img_pil = img_pil.convert(\"RGB\")\n            img_cv2 = np.array(img_pil)[:, :, ::-1].copy()\n\n        height, width = img_cv2.shape[:2]\n        if height == 0 or width == 0:\n            logger.warning(f\"Image has zero dimension: {img_path}. Skipping.\")\n            return None\n\n        with torch.no_grad():\n            image_tensor = torch.as_tensor(img_cv2.astype(\"float32\").transpose(2, 0, 1))\n            inputs = [{\"image\": image_tensor, \"height\": height, \"width\": width}]\n            images = model.preprocess_image(inputs)\n            features = model.backbone(images.tensor)\n            proposals, _ = model.proposal_generator(images, features)\n            proposals = proposals[0]\n            features_list = [features[f] for f in model.roi_heads.box_in_features]\n\n            # pred_outputs is likely a list containing ONE Instances object\n            pred_outputs, _ = model.roi_heads(images, features, [proposals])\n\n            # Validation and Access\n            if not pred_outputs or not isinstance(pred_outputs, list) or len(pred_outputs) == 0:\n                logger.warning(f\"pred_outputs list is empty or not a list for {img_path}. Skipping.\")\n                return None\n            if not isinstance(pred_outputs[0], Instances):\n                 logger.warning(f\"First element of pred_outputs is not an Instances object (Type: {type(pred_outputs[0])}) for {img_path}. Skipping.\")\n                 return None\n\n            instances_gpu = pred_outputs[0] # Access the Instances object directly\n\n            # Check length BEFORE further processing or .to(cpu)\n            if len(instances_gpu) == 0:\n                logger.debug(f\"Instances object has length 0 (no detections after NMS) for {img_path}. Skipping.\")\n                return None # Exit cleanly if no instances remain\n\n            # If len > 0, proceed to move to CPU and extract data\n            try:\n                instances = instances_gpu.to(\"cpu\")\n            except Exception as cpu_err:\n                logger.error(f\"Error during .to('cpu'): {cpu_err} for {img_path}\", exc_info=True)\n                return None\n\n            # Filter and Select Top Regions\n            if not instances.has(\"scores\") or not instances.has(\"pred_boxes\"):\n                 logger.warning(f\"Instances missing 'scores' or 'pred_boxes' after to(cpu) for {img_path}. Skipping.\")\n                 return None\n\n            scores = instances.scores\n            pred_boxes = instances.pred_boxes.tensor\n\n            keep_indices = torch.where(scores >= cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)[0]\n            num_kept_after_thresh = len(keep_indices)\n\n            if num_kept_after_thresh == 0:\n                 logger.debug(f\"No instances passed score threshold {cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST}. Max score: {scores.max():.3f}. Image: {img_path}. Skipping.\")\n                 return None\n\n            num_to_select = min(num_kept_after_thresh, max_regions)\n            scores_above_thresh = scores[keep_indices]\n            _, top_k_relative_indices = torch.topk(scores_above_thresh, num_to_select, sorted=False)\n            final_indices = keep_indices[top_k_relative_indices]\n\n            # logger.info(f\"Kept {num_to_select} instances for {img_path} (Max score: {scores[final_indices].max():.3f})\") # Optional: log success details\n            final_boxes_topk = pred_boxes[final_indices]\n\n            # --- Re-pooling, Projection, Normalization, Padding ---\n            final_box_list_gpu = [Boxes(final_boxes_topk).to(device)]\n            final_features_pooled = model.roi_heads.box_pooler(features_list, final_box_list_gpu)\n            final_features_unprojected = model.roi_heads.box_head(final_features_pooled)\n\n            if projection_layer is not None:\n                projected_features = projection_layer(final_features_unprojected)\n            else:\n                projected_features = final_features_unprojected\n\n            whwh = torch.tensor([width, height, width, height], device=device, dtype=torch.float32)\n            normalized_boxes = final_boxes_topk.to(device) / whwh\n            normalized_boxes = torch.clamp(normalized_boxes, 0.0, 1.0)\n\n            num_final_regions = projected_features.shape[0]\n            padded_features = torch.zeros((max_regions, target_dim), dtype=projected_features.dtype, device=device)\n            padded_boxes = torch.zeros((max_regions, 4), dtype=normalized_boxes.dtype, device=device)\n            vis_mask = torch.zeros(max_regions, dtype=torch.long, device=device)\n\n            padded_features[:num_final_regions] = projected_features\n            padded_boxes[:num_final_regions] = normalized_boxes\n            vis_mask[:num_final_regions] = 1\n\n            # logger.debug(f\"Successfully processed {img_path}\") # Optional: Log success\n            return {\n                \"features\": padded_features.cpu(),\n                \"boxes\": padded_boxes.cpu(),\n                \"visual_attention_mask\": vis_mask.cpu()\n            }\n\n    except FileNotFoundError:\n        logger.error(f\"Image file not found: {img_path}\")\n        return None\n    except Exception as e:\n        # Log the full traceback for unexpected errors\n        logger.error(f\"Unexpected error processing image {img_path}: {e}\", exc_info=True)\n        # traceback.print_exc() # Optionally print traceback directly if logger isn't detailed enough\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T14:03:18.646341Z","iopub.execute_input":"2025-04-14T14:03:18.646641Z","iopub.status.idle":"2025-04-14T14:03:18.660736Z","shell.execute_reply.started":"2025-04-14T14:03:18.646619Z","shell.execute_reply":"2025-04-14T14:03:18.659929Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Cell 8: Main Feature Extraction Loop (Cleaned)\n\nimport os\nimport json\nimport torch\nimport gc\nfrom tqdm.notebook import tqdm\nimport logging\n\n# Make sure logger is available\nlogger = logging.getLogger(__name__)\n\n# Make sure helper function is defined (or imported if moved)\n# def get_image_path(...): ...\n\ndef process_dataset_split(dataset_name, split_name, json_path, dataset_base_path, model, cfg, device, projection_layer, output_file, max_regions=MAX_REGIONS, target_dim=TARGET_FEATURE_DIM):\n    \"\"\"Processes all images for a given JSON split and saves features.\"\"\"\n    logger.info(f\"Starting processing for: {dataset_name} - {split_name}\")\n    logger.info(f\"JSON source: {json_path}\")\n    logger.info(f\"Output file: {output_file}\")\n\n    if model is None:\n        logger.error(\"Object detector model not loaded. Cannot process.\")\n        return\n\n    all_features_map = {}  # Dictionary to store features: {sample_id: feature_dict}\n\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        logger.info(f\"Loaded {len(data)} samples from {json_path}\")\n    except FileNotFoundError:\n        logger.error(f\"JSON file not found: {json_path}. Skipping this split.\")\n        return\n    except json.JSONDecodeError:\n        logger.error(f\"Error decoding JSON file: {json_path}. Skipping this split.\")\n        return\n    except Exception as e:\n        logger.error(f\"Failed to load or parse JSON {json_path}: {e}. Skipping this split.\", exc_info=True)\n        return\n\n    processed_count = 0\n    skipped_count = 0\n    error_count = 0\n\n    # Iterate through samples listed in the JSON\n    for sample in tqdm(data, desc=f\"Extracting {split_name} features ({dataset_name})\"):\n        sample_id = sample.get('sample_id', sample.get('id', sample.get('image_id')))\n        if not sample_id:\n            logger.warning(f\"Sample ID not found in record: {sample}. Skipping.\")\n            skipped_count += 1\n            continue\n        sample_id = str(sample_id)\n\n        # Construct image path using the helper function\n        image_path = get_image_path(sample_id, dataset_base_path, split_name) # Assumes get_image_path is defined\n\n        if not image_path:\n            logger.warning(f\"Image path not found for sample_id: {sample_id} (split: {split_name}). Skipping.\")\n            skipped_count += 1\n            continue\n\n        # Extract features for this image\n        visual_data = extract_features_from_image_vg(\n            img_path=image_path,\n            model=object_detector_model, # Use globally defined model\n            cfg=detector_cfg,           # Use globally defined cfg\n            device=device,              # Use globally defined device\n            projection_layer=projection_layer, # Use globally defined projection_layer\n            max_regions=max_regions,\n            target_dim=target_dim,\n            detector_output_dim=DETECTOR_OUTPUT_DIM # Use global DETECTOR_OUTPUT_DIM\n        )\n\n        if visual_data:\n            all_features_map[sample_id] = visual_data\n            processed_count += 1\n        else:\n            # Feature extraction returned None (error/skip already logged inside the function)\n            error_count += 1\n\n    logger.info(f\"Finished processing {split_name} for {dataset_name}. \"\n                f\"Successfully processed: {processed_count}, Skipped (no ID/image): {skipped_count}, Errors/No Detections: {error_count}\")\n\n    # Save the collected features\n    if all_features_map:\n        logger.info(f\"Saving {len(all_features_map)} extracted region features to {output_file}...\")\n        try:\n            torch.save(all_features_map, output_file)\n            logger.info(f\"Region features saved successfully to {output_file}\")\n        except Exception as e:\n            logger.error(f\"Failed to save features to {output_file}: {e}\", exc_info=True)\n    else:\n        logger.warning(f\"No features were successfully extracted for {dataset_name} - {split_name} (or all resulted in errors/no detections). Output file not saved.\")\n\n\n# --- Define Paths and Run Extraction Loop ---\n\n# Ensure global variables like object_detector_model, detector_cfg, device,\n# KAGGLE_INPUT_DIR, VISUAL_FEATURE_DIR, etc., are defined from previous cells.\nif object_detector_model is not None:\n    logger.info(\"Model loaded. Proceeding with feature extraction loop.\")\n\n    # Use KAGGLE_INPUT_DIR variable defined in Cell 4 (should be /kaggle/input)\n    anxiety_base_dir = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Anxiety_Data\")\n    depression_base_dir = os.path.join(KAGGLE_INPUT_DIR, \"dataset\", \"Depressive_Data\")\n    output_feature_dir = VISUAL_FEATURE_DIR\n\n    # Verify paths exist (optional check)\n    if not os.path.exists(anxiety_base_dir): logger.warning(f\"Anxiety base directory not found: {anxiety_base_dir}\")\n    if not os.path.exists(depression_base_dir): logger.warning(f\"Depression base directory not found: {depression_base_dir}\")\n    if not os.path.exists(output_feature_dir): logger.info(f\"Creating output directory: {output_feature_dir}\"); os.makedirs(output_feature_dir)\n\n\n    datasets_to_process = [\n        (\"anxiety\", \"train\", os.path.join(anxiety_base_dir, \"anxiety_train.json\"), anxiety_base_dir, os.path.join(output_feature_dir, \"anxiety_train_region_features.pt\")),\n        (\"anxiety\", \"test\", os.path.join(anxiety_base_dir, \"anxiety_test.json\"), anxiety_base_dir, os.path.join(output_feature_dir, \"anxiety_test_region_features.pt\")),\n        (\"depression\", \"train\", os.path.join(depression_base_dir, \"train.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_train_region_features.pt\")),\n        (\"depression\", \"test\", os.path.join(depression_base_dir, \"test.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_test_region_features.pt\")),\n        (\"depression\", \"val\", os.path.join(depression_base_dir, \"val.json\"), depression_base_dir, os.path.join(output_feature_dir, \"depression_val_region_features.pt\"))\n    ]\n    logger.info(f\"List of datasets/splits to process created (length {len(datasets_to_process)}).\")\n\n\n    # Process each defined dataset split\n    logger.info(\"Starting the main loop over datasets_to_process...\")\n    for name, split, json_p, data_base_p, out_f in datasets_to_process:\n        logger.info(f\"--- Checking prerequisites for loop iteration: {name} - {split} ---\")\n        logger.info(f\"JSON path: {json_p} | Output file path: {out_f}\")\n\n        # Check if the corresponding JSON exists before processing\n        if os.path.exists(json_p):\n            # Check if the output file already exists\n            if not os.path.exists(out_f):\n                logger.info(f\"Output file not found. Calling process_dataset_split for {name} - {split}.\")\n                process_dataset_split(\n                    dataset_name=name,\n                    split_name=split,\n                    json_path=json_p,\n                    dataset_base_path=data_base_p,\n                    model=object_detector_model,\n                    cfg=detector_cfg,\n                    device=device,\n                    projection_layer=projection_layer,\n                    output_file=out_f,\n                    max_regions=MAX_REGIONS,\n                    target_dim=TARGET_FEATURE_DIM\n                )\n                # Clean up GPU memory after processing each split\n                logger.info(f\"Cleaning up memory after {name} - {split}...\")\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n            else:\n                logger.info(f\"Output file exists: {out_f}. Skipping extraction for {name} - {split}.\")\n        else:\n            logger.warning(f\"JSON file not found: {json_p}. Skipping iteration for {name} - {split}.\")\n        logger.info(f\"--- Finished checks/processing for loop iteration: {name} - {split} ---\\n\")\n\n    logger.info(\"Region feature extraction process finished for all defined splits.\")\n    # Optional cleanup\n    # del object_detector_model, detector_cfg, projection_layer\n    # gc.collect()\n    # if torch.cuda.is_available(): torch.cuda.empty_cache()\n    # logger.info(\"Detector model and config cleaned up from memory.\")\n\nelse:\n    logger.error(\"Object detection model was not loaded successfully in prior cells. CANNOT extract region features.\")\n\nlogger.info(\"Cell 8 Execution Reached End.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T14:03:22.292106Z","iopub.execute_input":"2025-04-14T14:03:22.292807Z","iopub.status.idle":"2025-04-14T14:26:15.403433Z","shell.execute_reply.started":"2025-04-14T14:03:22.292780Z","shell.execute_reply":"2025-04-14T14:26:15.402896Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting train features (anxiety):   0%|          | 0/2608 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8ec0d205bfb4489bc52582a59f2171d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting test features (anxiety):   0%|          | 0/652 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb68d1f450334ddda83f02958e3507c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting train features (depression):   0%|          | 0/8814 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c5377f216e34c6f98a9928d4937d971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting test features (depression):   0%|          | 0/520 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729742a1e0ef4f8eb8c115eb6100a116"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting val features (depression):   0%|          | 0/361 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8dcb68396e4a3487399513651ffad3"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Cell 9: Create Zip Archive of Features\n\nimport os\n\noutput_dir = \"/kaggle/working/region_visual_features\"\nzip_file_path = \"/kaggle/working/axiom_region_features.zip\" # Output zip file name\n\n# Check if the directory exists and has files\nif os.path.exists(output_dir) and len(os.listdir(output_dir)) > 0:\n    print(f\"Found output directory: {output_dir}\")\n    print(\"Files inside:\")\n    !ls -lh {output_dir}\n\n    print(f\"\\nZipping directory {output_dir} into {zip_file_path}...\")\n    # Use shell zip command: -r for recursive, -q for quiet (less verbose output)\n    !zip -r -q {zip_file_path} {output_dir}\n\n    # Verify zip file creation\n    if os.path.exists(zip_file_path):\n        print(\"\\nZip file created successfully:\")\n        !ls -lh {zip_file_path}\n        print(f\"\\n--> You can now download '{os.path.basename(zip_file_path)}' from the 'Output' section in the Kaggle UI panel.\")\n    else:\n        print(\"\\nError: Zip file creation failed.\")\nelse:\n    print(f\"Error: Output directory {output_dir} not found or is empty. Cannot create zip file.\")\n    print(\"Please ensure the feature extraction in Cell 8 completed and generated .pt files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T14:26:30.633127Z","iopub.execute_input":"2025-04-14T14:26:30.633740Z","iopub.status.idle":"2025-04-14T14:27:17.919670Z","shell.execute_reply.started":"2025-04-14T14:26:30.633708Z","shell.execute_reply":"2025-04-14T14:27:17.918747Z"}},"outputs":[{"name":"stdout","text":"Found output directory: /kaggle/working/region_visual_features\nFiles inside:\ntotal 3.2G\n-rw-r--r-- 1 root root 175M Apr 14 14:10 anxiety_test_region_features.pt\n-rw-r--r-- 1 root root 701M Apr 14 14:08 anxiety_train_region_features.pt\n-rw-r--r-- 1 root root 137M Apr 14 14:25 depression_test_region_features.pt\n-rw-r--r-- 1 root root 2.1G Apr 14 14:24 depression_train_region_features.pt\n-rw-r--r-- 1 root root  86M Apr 14 14:26 depression_val_region_features.pt\n\nZipping directory /kaggle/working/region_visual_features into /kaggle/working/axiom_region_features.zip...\n\nZip file created successfully:\n-rw-r--r-- 1 root root 546M Apr 14 14:27 /kaggle/working/axiom_region_features.zip\n\n--> You can now download 'axiom_region_features.zip' from the 'Output' section in the Kaggle UI panel.\n","output_type":"stream"}],"execution_count":8}]}